#!/usr/bin/env python3
"""
v0.3.5æˆåŠŸç”»åƒãƒãƒƒãƒè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2
äººé–“è©•ä¾¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆA-Fåˆ¤å®šï¼‰ã«çµ±ä¸€ã—ãŸGPT-4Oè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

27æšã®æˆåŠŸç”»åƒã‚’æ–°ã—ã„è©•ä¾¡åŸºæº–ã§è‡ªå‹•è©•ä¾¡
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# Add project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

from features.evaluation.image_evaluation_mcp import ImageEvaluationMCP


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ v0.3.5æˆåŠŸç”»åƒãƒãƒƒãƒè©•ä¾¡é–‹å§‹ï¼ˆæ–°è©•ä¾¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰")
    
    # è¨­å®š
    results_dir = Path("/mnt/c/AItools/lora/train/yadokugaeru/clipped_boundingbox/kaname09_0_3_5")
    output_file = Path("batch_evaluation_results_v2.json")
    
    # batch_results_v035.jsonã‹ã‚‰æˆåŠŸç”»åƒãƒªã‚¹ãƒˆã‚’å–å¾—
    batch_results_path = results_dir / "batch_results_v035.json"
    
    if not batch_results_path.exists():
        print(f"âŒ ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {batch_results_path}")
        return
    
    # æˆåŠŸç”»åƒãƒªã‚¹ãƒˆã‚’æŠ½å‡º
    with open(batch_results_path, 'r', encoding='utf-8') as f:
        batch_results = json.load(f)
    
    success_images = []
    for result in batch_results['results']:
        if result['success']:
            image_path = results_dir / result['filename']
            if image_path.exists():
                success_images.append(str(image_path))
    
    print(f"ğŸ“Š è©•ä¾¡å¯¾è±¡: {len(success_images)}æšã®æˆåŠŸç”»åƒ")
    print(f"ğŸ¯ è©•ä¾¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: A-Fåˆ¤å®šï¼ˆäººé–“è©•ä¾¡æº–æ‹ ï¼‰")
    
    if not success_images:
        print("âŒ è©•ä¾¡å¯¾è±¡ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    evaluator = ImageEvaluationMCP()
    
    try:
        # ãƒãƒƒãƒè©•ä¾¡å®Ÿè¡Œ
        summary = evaluator.batch_evaluate(success_images, str(output_file))
        
        # è¿½åŠ åˆ†æï¼ˆæ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œï¼‰
        print(f"\nğŸ“ˆ è©•ä¾¡åˆ†æ:")
        
        successful_evaluations = [r for r in summary['results'] if r['success'] and not r.get('parse_error')]
        
        if successful_evaluations:
            # è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
            grade_distribution = {}
            issues_count = {}
            
            for result in successful_evaluations:
                grade = result.get('grade', 'Unknown')
                grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
                
                # å•é¡Œåˆ†é¡ã®é›†è¨ˆ
                issues = result.get('issues', [])
                for issue in issues:
                    issues_count[issue] = issues_count.get(issue, 0) + 1
            
            print(f"\nğŸ“Š è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ:")
            for grade, count in sorted(grade_distribution.items()):
                percentage = (count / len(successful_evaluations)) * 100
                print(f"  {grade}è©•ä¾¡: {count}æš ({percentage:.1f}%)")
            
            if issues_count:
                print(f"\nâš ï¸ å•é¡Œåˆ†é¡é »åº¦:")
                for issue, count in sorted(issues_count.items(), key=lambda x: x[1], reverse=True):
                    print(f"  {issue}: {count}æš")
            
            # APIä½¿ç”¨çµ±è¨ˆ
            api_usage = {}
            for result in successful_evaluations:
                api_used = result.get('api_used', 'Unknown')
                api_usage[api_used] = api_usage.get(api_used, 0) + 1
            
            print(f"\nğŸ¤– APIä½¿ç”¨çµ±è¨ˆ:")
            for api, count in api_usage.items():
                print(f"  {api}: {count}æš")
        
        print(f"\nâœ… è©•ä¾¡å®Œäº†! æ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµæœ: {output_file}")
        
    except Exception as e:
        print(f"âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return


if __name__ == "__main__":
    main()