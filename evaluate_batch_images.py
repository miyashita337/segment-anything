#!/usr/bin/env python3
"""
v0.3.5æˆåŠŸç”»åƒãƒãƒƒãƒè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GPT-4O + Gemini ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨

27æšã®æˆåŠŸç”»åƒã‚’è‡ªå‹•è©•ä¾¡ã—ã€å“è³ªåˆ†æã‚’è¡Œã†
"""

import json
import sys
from datetime import datetime
from pathlib import Path

# Add project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

from features.evaluation.image_evaluation_mcp import ImageEvaluationMCP


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ v0.3.5æˆåŠŸç”»åƒãƒãƒƒãƒè©•ä¾¡é–‹å§‹")
    
    # è¨­å®š
    results_dir = Path("/mnt/c/AItools/lora/train/yadokugaeru/clipped_boundingbox/kaname09_0_3_5")
    output_file = Path("batch_evaluation_results.json")
    
    # batch_results_v035.jsonã‹ã‚‰æˆåŠŸç”»åƒãƒªã‚¹ãƒˆã‚’å–å¾—
    batch_results_path = results_dir / "batch_results_v035.json"
    
    if not batch_results_path.exists():
        print(f"âŒ ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {batch_results_path}")
        return
    
    # æˆåŠŸç”»åƒãƒªã‚¹ãƒˆã‚’æŠ½å‡º
    with open(batch_results_path, 'r', encoding='utf-8') as f:
        batch_results = json.load(f)
    
    success_images = []
    for result in batch_results['results']:
        if result['success']:
            image_path = results_dir / result['filename']
            if image_path.exists():
                success_images.append(str(image_path))
    
    print(f"ğŸ“Š è©•ä¾¡å¯¾è±¡: {len(success_images)}æšã®æˆåŠŸç”»åƒ")
    
    if not success_images:
        print("âŒ è©•ä¾¡å¯¾è±¡ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    evaluator = ImageEvaluationMCP()
    
    try:
        # ãƒãƒƒãƒè©•ä¾¡å®Ÿè¡Œ
        summary = evaluator.batch_evaluate(success_images, str(output_file))
        
        # è¿½åŠ åˆ†æ
        print(f"\nğŸ“ˆ å“è³ªåˆ†æ:")
        
        successful_evaluations = [r for r in summary['results'] if r['success'] and not r.get('parse_error')]
        
        if successful_evaluations:
            # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
            completeness_scores = [r.get('completeness', 0) for r in successful_evaluations]
            boundary_scores = [r.get('boundary_quality', 0) for r in successful_evaluations]
            background_scores = [r.get('background_removal', 0) for r in successful_evaluations]
            overall_scores = [r.get('overall_quality', 0) for r in successful_evaluations]
            
            if completeness_scores:
                print(f"  å®Œå…¨æ€§å¹³å‡: {sum(completeness_scores) / len(completeness_scores):.2f}")
            if boundary_scores:
                print(f"  å¢ƒç•Œå“è³ªå¹³å‡: {sum(boundary_scores) / len(boundary_scores):.2f}")
            if background_scores:
                print(f"  èƒŒæ™¯é™¤å»å¹³å‡: {sum(background_scores) / len(background_scores):.2f}")
            if overall_scores:
                print(f"  ç·åˆè©•ä¾¡å¹³å‡: {sum(overall_scores) / len(overall_scores):.2f}")
            
            # APIä½¿ç”¨çµ±è¨ˆ
            api_usage = {}
            for result in successful_evaluations:
                api_used = result.get('api_used', 'Unknown')
                api_usage[api_used] = api_usage.get(api_used, 0) + 1
            
            print(f"\nğŸ“Š APIä½¿ç”¨çµ±è¨ˆ:")
            for api, count in api_usage.items():
                print(f"  {api}: {count}æš")
        
        print(f"\nâœ… è©•ä¾¡å®Œäº†! çµæœ: {output_file}")
        
    except Exception as e:
        print(f"âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return


if __name__ == "__main__":
    main()