#!/usr/bin/env python3
"""
ã‚¢ãƒ‹ãƒ¡ç”»åƒå‰å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
é¡”æ¤œå‡ºç‡å‘ä¸Šã®ãŸã‚ã®ç‰¹åŒ–å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import numpy as np
import cv2

import logging
from typing import Optional, Tuple

logger = logging.getLogger(__name__)


class AnimeImagePreprocessor:
    """ã‚¢ãƒ‹ãƒ¡ç”»åƒå‰å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AnimeImagePreprocessor")

        # CLAHEï¼ˆé©å¿œçš„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å‡åŒ–ï¼‰åˆæœŸåŒ–
        self.clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))  # ã‚¢ãƒ‹ãƒ¡ç”»åƒç”¨ã«èª¿æ•´

    def enhance_for_face_detection(
        self, image: np.ndarray, lightweight_mode: bool = False
    ) -> np.ndarray:
        """é¡”æ¤œå‡ºç”¨ç·åˆå‰å‡¦ç†"""
        try:
            if lightweight_mode:
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰: å¿…è¦æœ€å°é™ã®å‡¦ç†ã®ã¿ï¼ˆ1-2ç§’ç›®æ¨™ï¼‰
                return self._lightweight_preprocessing(image)
            else:
                # é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰: å…¨5æ®µéšå‡¦ç†ï¼ˆå¾“æ¥ç‰ˆï¼‰
                return self._full_preprocessing(image)

        except Exception as e:
            self.logger.warning(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼ã€å…ƒç”»åƒã‚’è¿”å´: {e}")
            return image

    def _lightweight_preprocessing(self, image: np.ndarray) -> np.ndarray:
        """è»½é‡å‰å‡¦ç†ï¼ˆGPT-4Oææ¡ˆç‰ˆ: 1-2ç§’ç›®æ¨™ï¼‰"""
        try:
            # 1. é«˜é€Ÿãƒã‚¤ã‚ºé™¤å»ï¼ˆfastNlMeansã®ç°¡æ˜“ç‰ˆï¼‰
            denoised = self._fast_denoise(image)

            # 2. é©å¿œçš„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å‡åŒ–ï¼ˆCLAHEï¼‰
            clahe_enhanced = self._apply_clahe(denoised)

            self.logger.debug("è»½é‡å‰å‡¦ç†å®Œäº†")
            return clahe_enhanced

        except Exception as e:
            self.logger.warning(f"è»½é‡å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return image

    def _full_preprocessing(self, image: np.ndarray) -> np.ndarray:
        """é«˜å“è³ªå‰å‡¦ç†ï¼ˆå¾“æ¥ã®5æ®µéšå‡¦ç†ï¼‰"""
        # 1. åŸºæœ¬çš„ãªãƒã‚¤ã‚ºé™¤å»
        denoised = self._denoise_image(image)

        # 2. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–
        contrast_enhanced = self._enhance_contrast(denoised)

        # 3. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å‡åŒ–
        histogram_equalized = self._histogram_equalization(contrast_enhanced)

        # 4. é©å¿œçš„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å‡åŒ–ï¼ˆCLAHEï¼‰
        clahe_enhanced = self._apply_clahe(histogram_equalized)

        # 5. ã‚¨ãƒƒã‚¸ä¿æŒå¹³æ»‘åŒ–
        edge_preserved = self._edge_preserving_smoothing(clahe_enhanced)

        self.logger.debug("é«˜å“è³ªå‰å‡¦ç†å®Œäº†")
        return edge_preserved

    def _fast_denoise(self, image: np.ndarray) -> np.ndarray:
        """é«˜é€Ÿãƒã‚¤ã‚ºé™¤å»ï¼ˆè»½é‡ç‰ˆï¼‰"""
        if len(image.shape) == 3:
            # ã‚«ãƒ©ãƒ¼ç”»åƒ - ã‚ˆã‚Šè»½é‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            denoised = cv2.fastNlMeansDenoisingColored(
                image, None, h=5, hColor=5, templateWindowSize=5, searchWindowSize=15
            )
        else:
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒ
            denoised = cv2.fastNlMeansDenoising(
                image, None, h=5, templateWindowSize=5, searchWindowSize=15
            )

        return denoised

    def _denoise_image(self, image: np.ndarray) -> np.ndarray:
        """ãƒã‚¤ã‚ºé™¤å»"""
        # Non-local Means Denoisingã‚’ä½¿ç”¨ï¼ˆã‚¢ãƒ‹ãƒ¡ç”»åƒã«é©ã—ã¦ã„ã‚‹ï¼‰
        if len(image.shape) == 3:
            # ã‚«ãƒ©ãƒ¼ç”»åƒ
            denoised = cv2.fastNlMeansDenoisingColored(
                image, None, h=10, hColor=10, templateWindowSize=7, searchWindowSize=21
            )
        else:
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒ
            denoised = cv2.fastNlMeansDenoising(
                image, None, h=10, templateWindowSize=7, searchWindowSize=21
            )

        return denoised

    def _enhance_contrast(self, image: np.ndarray) -> np.ndarray:
        """ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–"""
        # LABè‰²ç©ºé–“ã§Lï¼ˆæ˜åº¦ï¼‰ãƒãƒ£ãƒ³ãƒãƒ«ã®ã¿ã‚’å¼·åŒ–
        if len(image.shape) == 3:
            lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
            l_channel, a_channel, b_channel = cv2.split(lab)

            # L ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–
            alpha = 1.2  # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ä¿‚æ•°
            beta = 10  # æ˜åº¦èª¿æ•´
            l_enhanced = cv2.convertScaleAbs(l_channel, alpha=alpha, beta=beta)

            # LABè‰²ç©ºé–“ã‚’å†æ§‹ç¯‰
            lab_enhanced = cv2.merge([l_enhanced, a_channel, b_channel])
            enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        else:
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã®å ´åˆ
            alpha = 1.2
            beta = 10
            enhanced = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)

        return enhanced

    def _histogram_equalization(self, image: np.ndarray) -> np.ndarray:
        """ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å‡åŒ–"""
        if len(image.shape) == 3:
            # ã‚«ãƒ©ãƒ¼ç”»åƒã®å ´åˆã€YUVè‰²ç©ºé–“ã§Yï¼ˆè¼åº¦ï¼‰ãƒãƒ£ãƒ³ãƒãƒ«ã‚’å‡¦ç†
            yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)
            yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])
            equalized = cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
        else:
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã®å ´åˆ
            equalized = cv2.equalizeHist(image)

        return equalized

    def _apply_clahe(self, image: np.ndarray) -> np.ndarray:
        """é©å¿œçš„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å‡åŒ–ï¼ˆCLAHEï¼‰é©ç”¨"""
        if len(image.shape) == 3:
            # ã‚«ãƒ©ãƒ¼ç”»åƒã®å ´åˆã€LABè‰²ç©ºé–“ã§Lãƒãƒ£ãƒ³ãƒãƒ«ã‚’å‡¦ç†
            lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
            l_channel, a_channel, b_channel = cv2.split(lab)

            # CLAHEã‚’Lãƒãƒ£ãƒ³ãƒãƒ«ã«é©ç”¨
            l_clahe = self.clahe.apply(l_channel)

            # LABè‰²ç©ºé–“ã‚’å†æ§‹ç¯‰
            lab_clahe = cv2.merge([l_clahe, a_channel, b_channel])
            clahe_result = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)
        else:
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã®å ´åˆ
            clahe_result = self.clahe.apply(image)

        return clahe_result

    def _edge_preserving_smoothing(self, image: np.ndarray) -> np.ndarray:
        """ã‚¨ãƒƒã‚¸ä¿æŒå¹³æ»‘åŒ–"""
        # ã‚¢ãƒ‹ãƒ¡ç”»åƒã®ç‰¹å¾´ã‚’ä¿æŒã—ãªãŒã‚‰å¹³æ»‘åŒ–
        smoothed = cv2.edgePreservingFilter(
            image,
            flags=cv2.RECURS_FILTER,  # å†å¸°ãƒ•ã‚£ãƒ«ã‚¿ä½¿ç”¨
            sigma_s=50,  # ãƒã‚¤ãƒãƒ¼ãƒ•ãƒƒãƒ‰ã‚µã‚¤ã‚º
            sigma_r=0.4,  # ç•°ãªã‚‹è‰²ã®å¹³å‡åŒ–å…·åˆ
        )

        return smoothed

    def create_multi_scale_versions(
        self, image: np.ndarray, lightweight_mode: bool = False
    ) -> list:
        """ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰ˆç”»åƒç”Ÿæˆ"""
        if lightweight_mode:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰: 3ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ï¼ˆGPT-4Oææ¡ˆï¼‰
            scales = [0.75, 1.0, 1.25]
        else:
            # é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰: 5ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆå¾“æ¥ç‰ˆï¼‰
            scales = [0.5, 0.75, 1.0, 1.25, 1.5]

        multi_scale_images = []
        height, width = image.shape[:2]

        for scale in scales:
            new_width = int(width * scale)
            new_height = int(height * scale)

            if scale == 1.0:
                scaled_image = image.copy()
            else:
                scaled_image = cv2.resize(
                    image,
                    (new_width, new_height),
                    interpolation=cv2.INTER_CUBIC if scale > 1.0 else cv2.INTER_AREA,
                )

            multi_scale_images.append(
                {"scale": scale, "image": scaled_image, "size": (new_width, new_height)}
            )

        return multi_scale_images

    def detect_optimal_brightness(self, image: np.ndarray) -> Tuple[float, bool]:
        """æœ€é©æ˜åº¦æ¤œå‡ºã¨èª¿æ•´è¦å¦åˆ¤å®š"""
        # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()

        # æ˜åº¦çµ±è¨ˆ
        mean_brightness = np.mean(gray)
        std_brightness = np.std(gray)

        # èª¿æ•´è¦å¦åˆ¤å®š
        needs_adjustment = False

        if mean_brightness < 80:  # æš—ã™ãã‚‹
            needs_adjustment = True
            self.logger.debug(f"ç”»åƒãŒæš—ã™ãã¾ã™: å¹³å‡æ˜åº¦ {mean_brightness:.1f}")
        elif mean_brightness > 200:  # æ˜ã‚‹ã™ãã‚‹
            needs_adjustment = True
            self.logger.debug(f"ç”»åƒãŒæ˜ã‚‹ã™ãã¾ã™: å¹³å‡æ˜åº¦ {mean_brightness:.1f}")
        elif std_brightness < 30:  # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒä½ã„
            needs_adjustment = True
            self.logger.debug(f"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒä½ã™ãã¾ã™: æ¨™æº–åå·® {std_brightness:.1f}")

        return mean_brightness, needs_adjustment

    def adaptive_brightness_adjustment(self, image: np.ndarray) -> np.ndarray:
        """é©å¿œçš„æ˜åº¦èª¿æ•´"""
        mean_brightness, needs_adjustment = self.detect_optimal_brightness(image)

        if not needs_adjustment:
            return image

        # ç›®æ¨™æ˜åº¦
        target_brightness = 128

        # èª¿æ•´ä¿‚æ•°è¨ˆç®—
        if mean_brightness < 80:
            # æš—ã™ãã‚‹å ´åˆ
            beta = target_brightness - mean_brightness
            alpha = 1.1
        elif mean_brightness > 200:
            # æ˜ã‚‹ã™ãã‚‹å ´åˆ
            beta = target_brightness - mean_brightness
            alpha = 0.9
        else:
            # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒä½ã„å ´åˆ
            beta = 0
            alpha = 1.3

        adjusted = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)

        self.logger.debug(f"æ˜åº¦èª¿æ•´: alpha={alpha:.2f}, beta={beta:.1f}")
        return adjusted


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ã‚¢ãƒ‹ãƒ¡ç”»åƒå‰å‡¦ç†ãƒ†ã‚¹ãƒˆ")
    parser.add_argument("--image", "-i", required=True, help="ãƒ†ã‚¹ãƒˆç”»åƒãƒ‘ã‚¹")
    parser.add_argument("--output", "-o", help="å‡ºåŠ›ç”»åƒãƒ‘ã‚¹")

    args = parser.parse_args()

    # ç”»åƒèª­ã¿è¾¼ã¿
    image = cv2.imread(args.image)
    if image is None:
        print(f"ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {args.image}")
        return 1

    print(f"ğŸ¨ ã‚¢ãƒ‹ãƒ¡ç”»åƒå‰å‡¦ç†ãƒ†ã‚¹ãƒˆ: {args.image}")
    print("=" * 60)

    # å‰å‡¦ç†å®Ÿè¡Œ
    preprocessor = AnimeImagePreprocessor()

    # å…ƒç”»åƒã®çµ±è¨ˆ
    original_brightness, needs_adjustment = preprocessor.detect_optimal_brightness(image)
    print(f"å…ƒç”»åƒçµ±è¨ˆ:")
    print(f"  å¹³å‡æ˜åº¦: {original_brightness:.1f}")
    print(f"  èª¿æ•´å¿…è¦: {'Yes' if needs_adjustment else 'No'}")

    # å‰å‡¦ç†å®Ÿè¡Œ
    enhanced_image = preprocessor.enhance_for_face_detection(image)

    # å‡¦ç†å¾Œã®çµ±è¨ˆ
    enhanced_brightness, _ = preprocessor.detect_optimal_brightness(enhanced_image)
    print(f"\nå‡¦ç†å¾Œçµ±è¨ˆ:")
    print(f"  å¹³å‡æ˜åº¦: {enhanced_brightness:.1f}")
    print(f"  æ˜åº¦æ”¹å–„: {enhanced_brightness - original_brightness:+.1f}")

    # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰ˆç”Ÿæˆ
    multi_scale_versions = preprocessor.create_multi_scale_versions(enhanced_image)
    print(f"\nãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰ˆ: {len(multi_scale_versions)}ç¨®é¡ç”Ÿæˆ")
    for version in multi_scale_versions:
        print(f"  ã‚¹ã‚±ãƒ¼ãƒ« {version['scale']}: {version['size']}")

    # å‡ºåŠ›ä¿å­˜
    if args.output:
        cv2.imwrite(args.output, enhanced_image)
        print(f"\nğŸ’¾ å‰å‡¦ç†çµæœä¿å­˜: {args.output}")

    print("âœ… ã‚¢ãƒ‹ãƒ¡ç”»åƒå‰å‡¦ç†ãƒ†ã‚¹ãƒˆå®Œäº†")
    return 0


if __name__ == "__main__":
    exit(main())
