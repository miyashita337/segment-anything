#!/usr/bin/env python3
"""
P1-018: æ»‘ã‚‰ã‹ã•è©•ä¾¡æŒ‡æ¨™ã®å®Ÿè£…
å¢ƒç•Œç·šã®æ»‘ã‚‰ã‹ã•ã‚’å¤šè§’çš„ã«å®šé‡è©•ä¾¡ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 

Features:
- Curvature-based smoothness analysis
- Frequency domain analysis
- Local variation assessment
- Multi-scale smoothness evaluation
- A-F grading system
"""

import numpy as np
import cv2
from typing import Dict, List, Tuple, Optional, Any
import json
from pathlib import Path
from datetime import datetime

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ç”¨
HAS_SCIPY = True
HAS_SKLEARN = True

try:
    from scipy import ndimage, signal
    from scipy.interpolate import interp1d
    from scipy.stats import variation
except ImportError:
    HAS_SCIPY = False

try:
    from sklearn.preprocessing import MinMaxScaler
except ImportError:
    HAS_SKLEARN = False


class SmoothnessMetrics:
    """å¢ƒç•Œç·šæ»‘ã‚‰ã‹ã•è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.name = "SmoothnessMetrics"
        self.version = "1.0.0"
        
        # æ»‘ã‚‰ã‹ã•è©•ä¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.smoothness_params = {
            'curvature_window': 5,
            'frequency_cutoff': 0.1,
            'variation_threshold': 0.3,
            'gradient_smoothing': 2,
            'multi_scale_levels': 3
        }
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åŸºæº–
        self.grading_thresholds = {
            'A': 0.85,  # Excellent smoothness
            'B': 0.70,  # Good smoothness
            'C': 0.55,  # Acceptable smoothness
            'D': 0.40,  # Poor smoothness
            'F': 0.00   # Very poor smoothness
        }
    
    def analyze_boundary_smoothness(self, mask: np.ndarray) -> Dict[str, Any]:
        """
        å¢ƒç•Œç·šã®åŒ…æ‹¬çš„æ»‘ã‚‰ã‹ã•åˆ†æ
        
        Args:
            mask: ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ç”»åƒ
            
        Returns:
            Dict: æ»‘ã‚‰ã‹ã•åˆ†æçµæœ
        """
        if mask is None or mask.size == 0:
            return self._generate_error_result("Empty or invalid mask")
        
        try:
            # å¢ƒç•Œç·šæŠ½å‡º
            contours = self._extract_contours(mask)
            if not contours:
                return self._generate_error_result("No contours found")
            
            # ãƒ¡ã‚¤ãƒ³å¢ƒç•Œç·šé¸æŠï¼ˆæœ€å¤§é¢ç©ï¼‰
            main_contour = max(contours, key=cv2.contourArea)
            
            if len(main_contour) < 10:
                return self._generate_error_result("Contour too small for analysis")
            
            # å„ç¨®æ»‘ã‚‰ã‹ã•æŒ‡æ¨™è¨ˆç®—
            curvature_metrics = self._analyze_curvature_smoothness(main_contour)
            frequency_metrics = self._analyze_frequency_smoothness(main_contour)
            variation_metrics = self._analyze_local_variation(main_contour)
            gradient_metrics = self._analyze_gradient_smoothness(main_contour)
            multiscale_metrics = self._analyze_multiscale_smoothness(main_contour)
            
            # ç·åˆè©•ä¾¡è¨ˆç®—
            overall_assessment = self._calculate_overall_smoothness(
                curvature_metrics, frequency_metrics, variation_metrics,
                gradient_metrics, multiscale_metrics
            )
            
            return {
                'analysis_type': 'boundary_smoothness',
                'contour_info': {
                    'point_count': len(main_contour),
                    'area': float(cv2.contourArea(main_contour)),
                    'perimeter': float(cv2.arcLength(main_contour, True))
                },
                'curvature_analysis': curvature_metrics,
                'frequency_analysis': frequency_metrics,
                'variation_analysis': variation_metrics,
                'gradient_analysis': gradient_metrics,
                'multiscale_analysis': multiscale_metrics,
                'overall_assessment': overall_assessment,
                'processing_info': {
                    'timestamp': datetime.now().isoformat(),
                    'version': self.version
                }
            }
            
        except Exception as e:
            return self._generate_error_result(f"Analysis failed: {str(e)}")
    
    def _extract_contours(self, mask: np.ndarray) -> List[np.ndarray]:
        """å¢ƒç•Œç·šæŠ½å‡º"""
        # ãƒã‚¹ã‚¯ã‚’8bit unsignedã«å¤‰æ›
        if mask.dtype != np.uint8:
            mask = (mask > 0).astype(np.uint8) * 255
        
        # å¢ƒç•Œç·šæ¤œå‡º
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
        
        # æœ€å°ã‚µã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        min_contour_length = 20
        valid_contours = [c for c in contours if len(c) >= min_contour_length]
        
        return valid_contours
    
    def _analyze_curvature_smoothness(self, contour: np.ndarray) -> Dict[str, Any]:
        """æ›²ç‡ãƒ™ãƒ¼ã‚¹æ»‘ã‚‰ã‹ã•åˆ†æ"""
        try:
            # å¢ƒç•Œç‚¹åº§æ¨™æŠ½å‡º
            points = contour.reshape(-1, 2).astype(np.float32)
            
            # æ›²ç‡è¨ˆç®—
            curvatures = self._calculate_curvature(points)
            
            if len(curvatures) == 0:
                return {'error': 'Failed to calculate curvatures'}
            
            # æ›²ç‡çµ±è¨ˆ
            curvature_stats = {
                'mean_curvature': float(np.mean(np.abs(curvatures))),
                'std_curvature': float(np.std(curvatures)),
                'max_curvature': float(np.max(np.abs(curvatures))),
                'curvature_variation': float(np.var(curvatures))
            }
            
            # æ€¥æ¿€ãªæ›²ç‡å¤‰åŒ–ã®æ¤œå‡º
            curvature_changes = np.abs(np.diff(curvatures))
            sharp_changes = np.sum(curvature_changes > np.percentile(curvature_changes, 90))
            
            # æ»‘ã‚‰ã‹ã•ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ›²ç‡å¤‰åŒ–ã®å°‘ãªã•ãƒ™ãƒ¼ã‚¹ï¼‰
            curvature_score = self._calculate_curvature_smoothness_score(curvatures)
            
            return {
                'curvature_statistics': curvature_stats,
                'sharp_change_count': int(sharp_changes),
                'curvature_smoothness_score': curvature_score,
                'curvature_grade': self._score_to_grade(curvature_score)
            }
            
        except Exception as e:
            return {'error': f'Curvature analysis failed: {str(e)}'}
    
    def _calculate_curvature(self, points: np.ndarray) -> np.ndarray:
        """æ›²ç‡è¨ˆç®—ï¼ˆ3ç‚¹æ³•ï¼‰"""
        if len(points) < 3:
            return np.array([])
        
        # å¢ƒç•Œã‚’å¾ªç’°ã¨ã—ã¦æ‰±ã†
        extended_points = np.vstack([points[-1:], points, points[:1]])
        curvatures = []
        
        for i in range(1, len(extended_points) - 1):
            p1, p2, p3 = extended_points[i-1], extended_points[i], extended_points[i+1]
            
            # ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—
            v1 = p2 - p1
            v2 = p3 - p2
            
            # å¤–ç©ã«ã‚ˆã‚‹æ›²ç‡è¨ˆç®—
            cross_prod = v1[0] * v2[1] - v1[1] * v2[0]
            v1_norm = np.linalg.norm(v1)
            v2_norm = np.linalg.norm(v2)
            
            if v1_norm > 0 and v2_norm > 0:
                curvature = cross_prod / (v1_norm * v2_norm)
                curvatures.append(curvature)
        
        return np.array(curvatures)
    
    def _calculate_curvature_smoothness_score(self, curvatures: np.ndarray) -> float:
        """æ›²ç‡ãƒ™ãƒ¼ã‚¹æ»‘ã‚‰ã‹ã•ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if len(curvatures) == 0:
            return 0.0
        
        # æ›²ç‡å¤‰åŒ–ã®åˆ†æ•£ï¼ˆå°ã•ã„ã»ã©æ»‘ã‚‰ã‹ï¼‰
        curvature_variance = np.var(curvatures)
        
        # æ€¥æ¿€ãªå¤‰åŒ–ã®é »åº¦
        changes = np.abs(np.diff(curvatures))
        high_changes = np.sum(changes > np.percentile(changes, 75))
        change_ratio = high_changes / len(changes) if len(changes) > 0 else 1.0
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-1ç¯„å›²ã€1ãŒæœ€ã‚‚æ»‘ã‚‰ã‹ï¼‰
        variance_score = 1.0 / (1.0 + curvature_variance * 10)
        change_score = 1.0 - min(change_ratio, 1.0)
        
        return (variance_score + change_score) / 2.0
    
    def _analyze_frequency_smoothness(self, contour: np.ndarray) -> Dict[str, Any]:
        """å‘¨æ³¢æ•°ãƒ‰ãƒ¡ã‚¤ãƒ³æ»‘ã‚‰ã‹ã•åˆ†æ"""
        try:
            points = contour.reshape(-1, 2)
            
            # X, Yåº§æ¨™ã®å‘¨æ³¢æ•°è§£æ
            x_coords = points[:, 0]
            y_coords = points[:, 1]
            
            # FFTè§£æï¼ˆscipyåˆ©ç”¨å¯èƒ½æ™‚ï¼‰
            if HAS_SCIPY:
                x_fft = np.fft.fft(x_coords)
                y_fft = np.fft.fft(y_coords)
                
                # é«˜å‘¨æ³¢æˆåˆ†ã®å¼·åº¦
                freqs = np.fft.fftfreq(len(x_coords))
                high_freq_mask = np.abs(freqs) > self.smoothness_params['frequency_cutoff']
                
                x_high_freq_power = np.sum(np.abs(x_fft[high_freq_mask])**2)
                y_high_freq_power = np.sum(np.abs(y_fft[high_freq_mask])**2)
                total_power = np.sum(np.abs(x_fft)**2) + np.sum(np.abs(y_fft)**2)
                
                high_freq_ratio = (x_high_freq_power + y_high_freq_power) / total_power if total_power > 0 else 0
                
                # å‘¨æ³¢æ•°ãƒ™ãƒ¼ã‚¹æ»‘ã‚‰ã‹ã•ã‚¹ã‚³ã‚¢ï¼ˆé«˜å‘¨æ³¢æˆåˆ†ãŒå°‘ãªã„ã»ã©æ»‘ã‚‰ã‹ï¼‰
                frequency_score = 1.0 - min(high_freq_ratio * 2, 1.0)
                
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå·®åˆ†ãƒ™ãƒ¼ã‚¹é«˜å‘¨æ³¢æ¨å®š
                x_diff = np.abs(np.diff(x_coords))
                y_diff = np.abs(np.diff(y_coords))
                
                high_freq_estimate = np.mean(x_diff) + np.mean(y_diff)
                frequency_score = 1.0 / (1.0 + high_freq_estimate * 0.1)
            
            return {
                'frequency_smoothness_score': frequency_score,
                'frequency_grade': self._score_to_grade(frequency_score),
                'analysis_method': 'scipy_fft' if HAS_SCIPY else 'fallback_diff'
            }
            
        except Exception as e:
            return {'error': f'Frequency analysis failed: {str(e)}'}
    
    def _analyze_local_variation(self, contour: np.ndarray) -> Dict[str, Any]:
        """å±€æ‰€å¤‰å‹•åˆ†æ"""
        try:
            points = contour.reshape(-1, 2).astype(np.float32)
            
            # éš£æ¥ç‚¹é–“è·é›¢ã®å¤‰å‹•
            distances = []
            for i in range(len(points)):
                p1 = points[i]
                p2 = points[(i + 1) % len(points)]
                dist = np.linalg.norm(p2 - p1)
                distances.append(dist)
            
            distances = np.array(distances)
            
            # å¤‰å‹•ä¿‚æ•°è¨ˆç®—
            if HAS_SCIPY:
                variation_coeff = variation(distances) if np.mean(distances) > 0 else 0
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
                mean_dist = np.mean(distances)
                variation_coeff = np.std(distances) / mean_dist if mean_dist > 0 else 0
            
            # å±€æ‰€è§’åº¦å¤‰åŒ–
            angle_changes = self._calculate_angle_changes(points)
            angle_variation = np.std(angle_changes) if len(angle_changes) > 0 else 0
            
            # å±€æ‰€å¤‰å‹•ã‚¹ã‚³ã‚¢
            distance_score = 1.0 / (1.0 + variation_coeff * 5)
            angle_score = 1.0 / (1.0 + angle_variation * 2)
            variation_score = (distance_score + angle_score) / 2.0
            
            return {
                'distance_variation_coefficient': float(variation_coeff),
                'angle_variation': float(angle_variation),
                'local_variation_score': variation_score,
                'variation_grade': self._score_to_grade(variation_score)
            }
            
        except Exception as e:
            return {'error': f'Local variation analysis failed: {str(e)}'}
    
    def _calculate_angle_changes(self, points: np.ndarray) -> np.ndarray:
        """éš£æ¥ãƒ™ã‚¯ãƒˆãƒ«é–“è§’åº¦å¤‰åŒ–è¨ˆç®—"""
        if len(points) < 3:
            return np.array([])
        
        angle_changes = []
        
        for i in range(len(points)):
            p1 = points[i]
            p2 = points[(i + 1) % len(points)]
            p3 = points[(i + 2) % len(points)]
            
            v1 = p2 - p1
            v2 = p3 - p2
            
            # è§’åº¦è¨ˆç®—
            v1_norm = np.linalg.norm(v1)
            v2_norm = np.linalg.norm(v2)
            
            if v1_norm > 0 and v2_norm > 0:
                cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)
                cos_angle = np.clip(cos_angle, -1, 1)
                angle_change = np.arccos(cos_angle)
                angle_changes.append(angle_change)
        
        return np.array(angle_changes)
    
    def _analyze_gradient_smoothness(self, contour: np.ndarray) -> Dict[str, Any]:
        """å‹¾é…ãƒ™ãƒ¼ã‚¹æ»‘ã‚‰ã‹ã•åˆ†æ"""
        try:
            points = contour.reshape(-1, 2).astype(np.float32)
            
            # X, Yæ–¹å‘å‹¾é…è¨ˆç®—
            x_coords = points[:, 0]
            y_coords = points[:, 1]
            
            # å‹¾é…è¨ˆç®—ï¼ˆä¸­å¤®å·®åˆ†æ³•ï¼‰
            x_grad = np.gradient(x_coords)
            y_grad = np.gradient(y_coords)
            
            # å‹¾é…ã®å¤§ãã•ã¨æ–¹å‘
            grad_magnitude = np.sqrt(x_grad**2 + y_grad**2)
            grad_direction = np.arctan2(y_grad, x_grad)
            
            # å‹¾é…ã®æ»‘ã‚‰ã‹ã•æŒ‡æ¨™
            grad_smoothness = {
                'magnitude_variation': float(np.std(grad_magnitude)),
                'direction_variation': float(np.std(np.diff(grad_direction))),
                'magnitude_range': float(np.ptp(grad_magnitude))
            }
            
            # å‹¾é…ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
            mag_score = 1.0 / (1.0 + grad_smoothness['magnitude_variation'] * 0.1)
            dir_score = 1.0 / (1.0 + grad_smoothness['direction_variation'] * 2)
            gradient_score = (mag_score + dir_score) / 2.0
            
            return {
                'gradient_statistics': grad_smoothness,
                'gradient_smoothness_score': gradient_score,
                'gradient_grade': self._score_to_grade(gradient_score)
            }
            
        except Exception as e:
            return {'error': f'Gradient analysis failed: {str(e)}'}
    
    def _analyze_multiscale_smoothness(self, contour: np.ndarray) -> Dict[str, Any]:
        """ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ»‘ã‚‰ã‹ã•åˆ†æ"""
        try:
            points = contour.reshape(-1, 2)
            
            scale_results = {}
            scale_scores = []
            
            # è¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§è§£æ
            for level in range(self.smoothness_params['multi_scale_levels']):
                scale_factor = 2 ** level
                
                # ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                if scale_factor < len(points):
                    sampled_indices = np.arange(0, len(points), scale_factor)
                    sampled_points = points[sampled_indices]
                    
                    # ã“ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®æ»‘ã‚‰ã‹ã•è©•ä¾¡
                    if len(sampled_points) >= 3:
                        scale_curvatures = self._calculate_curvature(sampled_points.astype(np.float32))
                        scale_score = self._calculate_curvature_smoothness_score(scale_curvatures)
                        
                        scale_results[f'scale_{level}'] = {
                            'scale_factor': scale_factor,
                            'point_count': len(sampled_points),
                            'smoothness_score': scale_score
                        }
                        
                        scale_scores.append(scale_score)
            
            # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç·åˆè©•ä¾¡
            if scale_scores:
                multiscale_score = np.mean(scale_scores)
                scale_consistency = 1.0 - np.std(scale_scores)  # ã‚¹ã‚±ãƒ¼ãƒ«é–“ã®ä¸€è²«æ€§
            else:
                multiscale_score = 0.0
                scale_consistency = 0.0
            
            return {
                'scale_analyses': scale_results,
                'multiscale_smoothness_score': multiscale_score,
                'scale_consistency': scale_consistency,
                'multiscale_grade': self._score_to_grade(multiscale_score)
            }
            
        except Exception as e:
            return {'error': f'Multiscale analysis failed: {str(e)}'}
    
    def _calculate_overall_smoothness(self, curvature_metrics: Dict, frequency_metrics: Dict,
                                    variation_metrics: Dict, gradient_metrics: Dict,
                                    multiscale_metrics: Dict) -> Dict[str, Any]:
        """ç·åˆæ»‘ã‚‰ã‹ã•è©•ä¾¡è¨ˆç®—"""
        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ã‚¹ã‚³ã‚¢æŠ½å‡º
        scores = {}
        weights = {}
        
        if 'curvature_smoothness_score' in curvature_metrics:
            scores['curvature'] = curvature_metrics['curvature_smoothness_score']
            weights['curvature'] = 0.3
        
        if 'frequency_smoothness_score' in frequency_metrics:
            scores['frequency'] = frequency_metrics['frequency_smoothness_score']
            weights['frequency'] = 0.2
        
        if 'local_variation_score' in variation_metrics:
            scores['variation'] = variation_metrics['local_variation_score']
            weights['variation'] = 0.2
        
        if 'gradient_smoothness_score' in gradient_metrics:
            scores['gradient'] = gradient_metrics['gradient_smoothness_score']
            weights['gradient'] = 0.15
        
        if 'multiscale_smoothness_score' in multiscale_metrics:
            scores['multiscale'] = multiscale_metrics['multiscale_smoothness_score']
            weights['multiscale'] = 0.15
        
        # é‡ã¿ä»˜ãå¹³å‡è¨ˆç®—
        if scores:
            total_weight = sum(weights.values())
            weighted_sum = sum(scores[key] * weights[key] for key in scores)
            overall_score = weighted_sum / total_weight if total_weight > 0 else 0.0
        else:
            overall_score = 0.0
        
        # ä¿¡é ¼æ€§è©•ä¾¡
        available_metrics = len(scores)
        confidence = min(available_metrics / 5.0, 1.0)  # 5ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¨ã¦åˆ©ç”¨å¯èƒ½ã§ä¿¡é ¼æ€§100%
        
        return {
            'overall_smoothness_score': overall_score,
            'smoothness_grade': self._score_to_grade(overall_score),
            'individual_scores': scores,
            'confidence': confidence,
            'available_metrics': available_metrics,
            'assessment': self._generate_smoothness_assessment(overall_score, scores)
        }
    
    def _score_to_grade(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã¸ã®å¤‰æ›"""
        for grade, threshold in self.grading_thresholds.items():
            if score >= threshold:
                return grade
        return 'F'
    
    def _generate_smoothness_assessment(self, overall_score: float, scores: Dict) -> str:
        """æ»‘ã‚‰ã‹ã•è©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"""
        if overall_score >= 0.85:
            return "excellent_smoothness"
        elif overall_score >= 0.70:
            return "good_smoothness"
        elif overall_score >= 0.55:
            return "acceptable_smoothness"
        elif overall_score >= 0.40:
            return "poor_smoothness"
        else:
            return "very_poor_smoothness"
    
    def _generate_error_result(self, error_message: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœç”Ÿæˆ"""
        return {
            'error': error_message,
            'overall_assessment': {
                'overall_smoothness_score': 0.0,
                'smoothness_grade': 'F',
                'assessment': 'analysis_failed'
            }
        }


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ P1-018: æ»‘ã‚‰ã‹ã•è©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ†ã‚¹ãƒˆç”¨å††å½¢ãƒã‚¹ã‚¯ä½œæˆ
    test_mask = np.zeros((200, 200), dtype=np.uint8)
    cv2.circle(test_mask, (100, 100), 80, 255, -1)
    
    # æ»‘ã‚‰ã‹ã•åˆ†æå®Ÿè¡Œ
    analyzer = SmoothnessMetrics()
    result = analyzer.analyze_boundary_smoothness(test_mask)
    
    print("\nğŸ“Š æ»‘ã‚‰ã‹ã•åˆ†æçµæœ:")
    if 'error' not in result:
        overall = result.get('overall_assessment', {})
        print(f"  ç·åˆã‚¹ã‚³ã‚¢: {overall.get('overall_smoothness_score', 0):.3f}")
        print(f"  æ»‘ã‚‰ã‹ã•ã‚°ãƒ¬ãƒ¼ãƒ‰: {overall.get('smoothness_grade', 'N/A')}")
        print(f"  ä¿¡é ¼æ€§: {overall.get('confidence', 0):.3f}")
        print(f"  è©•ä¾¡: {overall.get('assessment', 'N/A')}")
        
        # å€‹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        individual_scores = overall.get('individual_scores', {})
        if individual_scores:
            print(f"\nğŸ” å€‹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
            for metric, score in individual_scores.items():
                print(f"    {metric}: {score:.3f}")
    else:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    print(f"\nâœ… [P1-018] æ»‘ã‚‰ã‹ã•è©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")


if __name__ == "__main__":
    main()