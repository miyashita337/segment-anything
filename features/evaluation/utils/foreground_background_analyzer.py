#!/usr/bin/env python3
"""
Foreground Background Analyzer - P1-021
èƒŒæ™¯ãƒ»å‰æ™¯åˆ†é›¢ç²¾åº¦æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ 

èƒŒæ™¯æ··å…¥ã‚’å®šé‡åŒ–ã—ã€æŠ½å‡ºå“è³ªã‚’å‘ä¸Š
"""

import os
import numpy as np
import cv2
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import json
from dataclasses import dataclass


@dataclass
class ColorCluster:
    """è‰²ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å®šç¾©"""
    center: Tuple[int, int, int]  # RGBä¸­å¿ƒå€¤
    pixel_count: int
    percentage: float
    variance: float


class ForegroundBackgroundAnalyzer:
    """
    èƒŒæ™¯ãƒ»å‰æ™¯åˆ†é›¢ç²¾åº¦æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ 
    
    è‰²å½©åˆ†æã«ã‚ˆã‚‹èƒŒæ™¯/å‰æ™¯åˆ¤å®šã¨æ··å…¥ç‡ã®å®šé‡åŒ–
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.analysis_results = {}
        
        # åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.analysis_params = {
            'color_clusters': 8,           # K-meansã‚¯ãƒ©ã‚¹ã‚¿æ•°
            'edge_threshold': 50,          # ã‚¨ãƒƒã‚¸æ¤œå‡ºé–¾å€¤
            'texture_window': 9,           # ãƒ†ã‚¯ã‚¹ãƒãƒ£è§£æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
            'uniformity_threshold': 30,    # å‡ä¸€æ€§é–¾å€¤
            'contamination_threshold': 0.1  # æ··å…¥åˆ¤å®šé–¾å€¤
        }
    
    def analyze_separation_quality(self, image: np.ndarray, mask: np.ndarray) -> Dict[str, Any]:
        """åˆ†é›¢å“è³ªã®è§£æ"""
        if image is None or mask is None:
            return {'error': 'ç”»åƒã¾ãŸã¯ãƒã‚¹ã‚¯ãŒç„¡åŠ¹ã§ã™'}
        
        # ç”»åƒå½¢å¼ã®æ­£è¦åŒ–
        if len(image.shape) == 3 and image.shape[2] == 3:
            image_bgr = image
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            return {'error': 'ç”»åƒã¯3ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚«ãƒ©ãƒ¼ç”»åƒã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™'}
        
        # ãƒã‚¹ã‚¯ã®æ­£è¦åŒ–
        if len(mask.shape) == 3:
            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
        
        _, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
        
        # å‰æ™¯ãƒ»èƒŒæ™¯é ˜åŸŸã®æŠ½å‡º
        foreground_analysis = self._analyze_foreground_region(image_rgb, binary_mask)
        background_analysis = self._analyze_background_region(image_rgb, binary_mask)
        
        # å¢ƒç•Œåˆ†æ
        boundary_analysis = self._analyze_boundary_region(image_rgb, binary_mask)
        
        # æ··å…¥ç‡ã®è¨ˆç®—
        contamination_analysis = self._calculate_contamination_rates(
            foreground_analysis, background_analysis, boundary_analysis
        )
        
        # åˆ†é›¢å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        separation_score = self._calculate_separation_score(
            foreground_analysis, background_analysis, contamination_analysis
        )
        
        return {
            'foreground_analysis': foreground_analysis,
            'background_analysis': background_analysis,
            'boundary_analysis': boundary_analysis,
            'contamination_analysis': contamination_analysis,
            'separation_score': separation_score,
            'overall_assessment': self._generate_separation_assessment(separation_score, contamination_analysis)
        }
    
    def _analyze_foreground_region(self, image: np.ndarray, mask: np.ndarray) -> Dict[str, Any]:
        """å‰æ™¯é ˜åŸŸã®è§£æ"""
        # å‰æ™¯ãƒ”ã‚¯ã‚»ãƒ«ã®æŠ½å‡º
        foreground_pixels = image[mask > 0]
        
        if len(foreground_pixels) == 0:
            return {'error': 'å‰æ™¯é ˜åŸŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
        
        # è‰²å½©çµ±è¨ˆ
        color_stats = self._calculate_color_statistics(foreground_pixels)
        
        # è‰²ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        color_clusters = self._perform_color_clustering(foreground_pixels)
        
        # ãƒ†ã‚¯ã‚¹ãƒãƒ£è§£æ
        texture_analysis = self._analyze_texture(image, mask, is_foreground=True)
        
        # ã‚¨ãƒƒã‚¸å¯†åº¦
        edge_density = self._calculate_edge_density(image, mask, is_foreground=True)
        
        return {
            'pixel_count': len(foreground_pixels),
            'color_statistics': color_stats,
            'color_clusters': [self._cluster_to_dict(cluster) for cluster in color_clusters],
            'texture_analysis': texture_analysis,
            'edge_density': edge_density,
            'uniformity_score': self._calculate_uniformity_score(foreground_pixels)
        }
    
    def _analyze_background_region(self, image: np.ndarray, mask: np.ndarray) -> Dict[str, Any]:
        """èƒŒæ™¯é ˜åŸŸã®è§£æ"""
        # èƒŒæ™¯ãƒ”ã‚¯ã‚»ãƒ«ã®æŠ½å‡º
        background_pixels = image[mask == 0]
        
        if len(background_pixels) == 0:
            return {'error': 'èƒŒæ™¯é ˜åŸŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
        
        # è‰²å½©çµ±è¨ˆ
        color_stats = self._calculate_color_statistics(background_pixels)
        
        # è‰²ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        color_clusters = self._perform_color_clustering(background_pixels)
        
        # ãƒ†ã‚¯ã‚¹ãƒãƒ£è§£æ
        texture_analysis = self._analyze_texture(image, mask, is_foreground=False)
        
        # ã‚¨ãƒƒã‚¸å¯†åº¦
        edge_density = self._calculate_edge_density(image, mask, is_foreground=False)
        
        return {
            'pixel_count': len(background_pixels),
            'color_statistics': color_stats,
            'color_clusters': [self._cluster_to_dict(cluster) for cluster in color_clusters],
            'texture_analysis': texture_analysis,
            'edge_density': edge_density,
            'uniformity_score': self._calculate_uniformity_score(background_pixels)
        }
    
    def _analyze_boundary_region(self, image: np.ndarray, mask: np.ndarray) -> Dict[str, Any]:
        """å¢ƒç•Œé ˜åŸŸã®è§£æ"""
        # å¢ƒç•Œã®æŠ½å‡º (ãƒã‚¹ã‚¯ã®ã‚¨ãƒƒã‚¸è¿‘å‚)
        kernel = np.ones((5, 5), np.uint8)
        dilated = cv2.dilate(mask, kernel, iterations=1)
        eroded = cv2.erode(mask, kernel, iterations=1)
        boundary = dilated - eroded
        
        boundary_pixels = image[boundary > 0]
        
        if len(boundary_pixels) == 0:
            return {'error': 'å¢ƒç•Œé ˜åŸŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
        
        # å¢ƒç•Œã®è‰²å½©è§£æ
        color_stats = self._calculate_color_statistics(boundary_pixels)
        
        # å¢ƒç•Œã®æ€¥æ¿€ãªå¤‰åŒ–ã‚’è§£æ
        gradient_analysis = self._analyze_color_gradients(image, boundary)
        
        return {
            'pixel_count': len(boundary_pixels),
            'color_statistics': color_stats,
            'gradient_analysis': gradient_analysis,
            'boundary_sharpness': self._calculate_boundary_sharpness(image, mask)
        }
    
    def _calculate_color_statistics(self, pixels: np.ndarray) -> Dict[str, Any]:
        """è‰²å½©çµ±è¨ˆã®è¨ˆç®—"""
        if len(pixels) == 0:
            return {}
        
        # RGBå„ãƒãƒ£ãƒ³ãƒãƒ«ã®çµ±è¨ˆ
        r_stats = {
            'mean': float(np.mean(pixels[:, 0])),
            'std': float(np.std(pixels[:, 0])),
            'min': int(np.min(pixels[:, 0])),
            'max': int(np.max(pixels[:, 0]))
        }
        
        g_stats = {
            'mean': float(np.mean(pixels[:, 1])),
            'std': float(np.std(pixels[:, 1])),
            'min': int(np.min(pixels[:, 1])),
            'max': int(np.max(pixels[:, 1]))
        }
        
        b_stats = {
            'mean': float(np.mean(pixels[:, 2])),
            'std': float(np.std(pixels[:, 2])),
            'min': int(np.min(pixels[:, 2])),
            'max': int(np.max(pixels[:, 2]))
        }
        
        # å…¨ä½“ã®æ˜åº¦ãƒ»å½©åº¦
        hsv_pixels = cv2.cvtColor(pixels.reshape(1, -1, 3), cv2.COLOR_RGB2HSV).reshape(-1, 3)
        
        brightness = float(np.mean(hsv_pixels[:, 2]))
        saturation = float(np.mean(hsv_pixels[:, 1]))
        
        return {
            'red': r_stats,
            'green': g_stats,
            'blue': b_stats,
            'brightness': brightness,
            'saturation': saturation,
            'color_variance': float(np.var(pixels.reshape(-1)))
        }
    
    def _perform_color_clustering(self, pixels: np.ndarray) -> List[ColorCluster]:
        """è‰²ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®å®Ÿè¡Œ"""
        if len(pixels) < self.analysis_params['color_clusters']:
            # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯å˜ç´”åŒ–
            unique_colors, counts = np.unique(pixels.reshape(-1, 3), axis=0, return_counts=True)
            clusters = []
            total_pixels = len(pixels)
            
            for color, count in zip(unique_colors, counts):
                cluster = ColorCluster(
                    center=tuple(color.astype(int)),
                    pixel_count=int(count),
                    percentage=float(count / total_pixels * 100),
                    variance=0.0
                )
                clusters.append(cluster)
            
            return clusters[:self.analysis_params['color_clusters']]
        
        # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        try:
            data = pixels.reshape(-1, 3).astype(np.float32)
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
            _, labels, centers = cv2.kmeans(
                data, self.analysis_params['color_clusters'], None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS
            )
            
            clusters = []
            total_pixels = len(pixels)
            
            for i, center in enumerate(centers):
                cluster_pixels = data[labels.flatten() == i]
                pixel_count = len(cluster_pixels)
                
                if pixel_count > 0:
                    variance = float(np.var(cluster_pixels))
                    cluster = ColorCluster(
                        center=tuple(center.astype(int)),
                        pixel_count=pixel_count,
                        percentage=float(pixel_count / total_pixels * 100),
                        variance=variance
                    )
                    clusters.append(cluster)
            
            # ãƒ”ã‚¯ã‚»ãƒ«æ•°ã§é™é †ã‚½ãƒ¼ãƒˆ
            clusters.sort(key=lambda x: x.pixel_count, reverse=True)
            return clusters
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªè‰²åˆ†æ
            mean_color = np.mean(pixels, axis=0)
            cluster = ColorCluster(
                center=tuple(mean_color.astype(int)),
                pixel_count=len(pixels),
                percentage=100.0,
                variance=float(np.var(pixels))
            )
            return [cluster]
    
    def _analyze_texture(self, image: np.ndarray, mask: np.ndarray, is_foreground: bool) -> Dict[str, Any]:
        """ãƒ†ã‚¯ã‚¹ãƒãƒ£è§£æ"""
        # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        
        # å¯¾è±¡é ˜åŸŸã®ãƒã‚¹ã‚¯
        region_mask = mask if is_foreground else (255 - mask)
        
        # Local Binary Patternè¿‘ä¼¼
        lbp_variance = self._calculate_lbp_variance(gray, region_mask)
        
        # ã‚¨ãƒƒã‚¸æ–¹å‘æ€§
        edge_orientation = self._calculate_edge_orientation(gray, region_mask)
        
        return {
            'lbp_variance': lbp_variance,
            'edge_orientation_variance': edge_orientation,
            'texture_complexity': self._classify_texture_complexity(lbp_variance)
        }
    
    def _calculate_lbp_variance(self, gray: np.ndarray, mask: np.ndarray) -> float:
        """Local Binary Patternåˆ†æ•£ã®è¨ˆç®—"""
        # ç°¡æ˜“ç‰ˆLBP
        h, w = gray.shape
        lbp_values = []
        
        for y in range(1, h-1):
            for x in range(1, w-1):
                if mask[y, x] > 0:
                    center = gray[y, x]
                    neighbors = [
                        gray[y-1, x-1], gray[y-1, x], gray[y-1, x+1],
                        gray[y, x+1], gray[y+1, x+1], gray[y+1, x],
                        gray[y+1, x-1], gray[y, x-1]
                    ]
                    
                    lbp_code = 0
                    for i, neighbor in enumerate(neighbors):
                        if neighbor > center:
                            lbp_code += 2**i
                    
                    lbp_values.append(lbp_code)
        
        return float(np.var(lbp_values)) if lbp_values else 0.0
    
    def _calculate_edge_orientation(self, gray: np.ndarray, mask: np.ndarray) -> float:
        """ã‚¨ãƒƒã‚¸æ–¹å‘æ€§ã®åˆ†æ•£è¨ˆç®—"""
        # Sobelãƒ•ã‚£ãƒ«ã‚¿ã§ã‚¨ãƒƒã‚¸æ¤œå‡º
        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        
        # ã‚¨ãƒƒã‚¸ã®è§’åº¦è¨ˆç®—
        angles = np.arctan2(sobel_y, sobel_x)
        
        # ãƒã‚¹ã‚¯é ˜åŸŸã®ã‚¨ãƒƒã‚¸è§’åº¦ã®ã¿æŠ½å‡º
        masked_angles = angles[mask > 0]
        
        return float(np.var(masked_angles)) if len(masked_angles) > 0 else 0.0
    
    def _classify_texture_complexity(self, lbp_variance: float) -> str:
        """ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦ã®åˆ†é¡"""
        if lbp_variance < 100:
            return 'smooth'
        elif lbp_variance < 500:
            return 'moderate'
        else:
            return 'complex'
    
    def _calculate_edge_density(self, image: np.ndarray, mask: np.ndarray, is_foreground: bool) -> float:
        """ã‚¨ãƒƒã‚¸å¯†åº¦ã®è¨ˆç®—"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        
        region_mask = mask if is_foreground else (255 - mask)
        region_edges = edges & region_mask
        
        total_region_pixels = np.sum(region_mask > 0)
        edge_pixels = np.sum(region_edges > 0)
        
        return float(edge_pixels / total_region_pixels) if total_region_pixels > 0 else 0.0
    
    def _calculate_uniformity_score(self, pixels: np.ndarray) -> float:
        """å‡ä¸€æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        if len(pixels) == 0:
            return 0.0
        
        # è‰²ã®åˆ†æ•£ã‚’åŸºã«ã—ãŸå‡ä¸€æ€§
        color_variance = np.var(pixels.reshape(-1))
        
        # 0-1ã‚¹ã‚±ãƒ¼ãƒ«ã«æ­£è¦åŒ– (åˆ†æ•£ãŒå°ã•ã„ã»ã©å‡ä¸€æ€§ãŒé«˜ã„)
        uniformity = 1.0 / (1.0 + color_variance / 1000.0)
        
        return float(uniformity)
    
    def _analyze_color_gradients(self, image: np.ndarray, boundary: np.ndarray) -> Dict[str, float]:
        """è‰²ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®è§£æ"""
        # å¢ƒç•Œã§ã®è‰²å¤‰åŒ–ã®æ€¥æ¿€ã•ã‚’æ¸¬å®š
        gradient_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
        gradient_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)
        
        gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2)
        
        # å¢ƒç•Œé ˜åŸŸã§ã®å¹³å‡ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        boundary_gradients = gradient_magnitude[boundary > 0]
        
        if len(boundary_gradients) > 0:
            mean_gradient = float(np.mean(boundary_gradients))
            max_gradient = float(np.max(boundary_gradients))
            std_gradient = float(np.std(boundary_gradients))
        else:
            mean_gradient = max_gradient = std_gradient = 0.0
        
        return {
            'mean_gradient': mean_gradient,
            'max_gradient': max_gradient,
            'std_gradient': std_gradient
        }
    
    def _calculate_boundary_sharpness(self, image: np.ndarray, mask: np.ndarray) -> float:
        """å¢ƒç•Œã®é‹­ã•è¨ˆç®—"""
        # ãƒã‚¹ã‚¯ã‚¨ãƒƒã‚¸ã®æŠ½å‡º
        edges = cv2.Canny(mask, 50, 150)
        
        # ã‚¨ãƒƒã‚¸å‘¨è¾ºã§ã®è‰²å¤‰åŒ–
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        gradient = cv2.Laplacian(gray, cv2.CV_64F)
        
        edge_gradients = np.abs(gradient[edges > 0])
        
        return float(np.mean(edge_gradients)) if len(edge_gradients) > 0 else 0.0
    
    def _calculate_contamination_rates(self, fg_analysis: Dict, bg_analysis: Dict, boundary_analysis: Dict) -> Dict[str, Any]:
        """æ··å…¥ç‡ã®è¨ˆç®—"""
        contamination = {}
        
        # è‰²ç›¸ä¼¼æ€§ã«ã‚ˆã‚‹æ··å…¥æ¤œå‡º
        fg_clusters = fg_analysis.get('color_clusters', [])
        bg_clusters = bg_analysis.get('color_clusters', [])
        
        if fg_clusters and bg_clusters:
            color_similarity = self._calculate_color_similarity(fg_clusters, bg_clusters)
            contamination['color_similarity'] = color_similarity
            contamination['high_similarity_risk'] = color_similarity > 0.7
        else:
            contamination['color_similarity'] = 0.0
            contamination['high_similarity_risk'] = False
        
        # ãƒ†ã‚¯ã‚¹ãƒãƒ£é¡ä¼¼æ€§
        fg_texture = fg_analysis.get('texture_analysis', {})
        bg_texture = bg_analysis.get('texture_analysis', {})
        
        texture_similarity = self._calculate_texture_similarity(fg_texture, bg_texture)
        contamination['texture_similarity'] = texture_similarity
        contamination['texture_confusion_risk'] = texture_similarity > 0.8
        
        # å¢ƒç•Œã®æ›–æ˜§ã•
        boundary_sharpness = boundary_analysis.get('boundary_sharpness', 0)
        contamination['boundary_ambiguity'] = 1.0 - min(1.0, boundary_sharpness / 100.0)
        contamination['blurry_boundary_risk'] = contamination['boundary_ambiguity'] > 0.5
        
        # ç·åˆæ··å…¥ãƒªã‚¹ã‚¯
        overall_risk = (
            color_similarity * 0.4 +
            texture_similarity * 0.3 +
            contamination['boundary_ambiguity'] * 0.3
        )
        contamination['overall_contamination_risk'] = float(overall_risk)
        contamination['contamination_level'] = self._classify_contamination_level(overall_risk)
        
        return contamination
    
    def _calculate_color_similarity(self, fg_clusters: List[Dict], bg_clusters: List[Dict]) -> float:
        """è‰²é¡ä¼¼æ€§ã®è¨ˆç®—"""
        if not fg_clusters or not bg_clusters:
            return 0.0
        
        max_similarity = 0.0
        
        for fg_cluster in fg_clusters[:3]:  # ä¸Šä½3ã‚¯ãƒ©ã‚¹ã‚¿
            fg_color = np.array(fg_cluster['center'])
            
            for bg_cluster in bg_clusters[:3]:
                bg_color = np.array(bg_cluster['center'])
                
                # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ€§
                distance = np.linalg.norm(fg_color - bg_color)
                similarity = 1.0 - min(1.0, distance / (255.0 * np.sqrt(3)))
                
                max_similarity = max(max_similarity, similarity)
        
        return float(max_similarity)
    
    def _calculate_texture_similarity(self, fg_texture: Dict, bg_texture: Dict) -> float:
        """ãƒ†ã‚¯ã‚¹ãƒãƒ£é¡ä¼¼æ€§ã®è¨ˆç®—"""
        if not fg_texture or not bg_texture:
            return 0.0
        
        fg_complexity = fg_texture.get('texture_complexity', 'smooth')
        bg_complexity = bg_texture.get('texture_complexity', 'smooth')
        
        # è¤‡é›‘åº¦ã®é¡ä¼¼æ€§
        complexity_similarity = 1.0 if fg_complexity == bg_complexity else 0.5
        
        # LBPåˆ†æ•£ã®é¡ä¼¼æ€§
        fg_lbp = fg_texture.get('lbp_variance', 0)
        bg_lbp = bg_texture.get('lbp_variance', 0)
        
        if fg_lbp + bg_lbp > 0:
            lbp_similarity = 1.0 - abs(fg_lbp - bg_lbp) / (fg_lbp + bg_lbp)
        else:
            lbp_similarity = 1.0
        
        return float((complexity_similarity + lbp_similarity) / 2.0)
    
    def _classify_contamination_level(self, risk_score: float) -> str:
        """æ··å…¥ãƒ¬ãƒ™ãƒ«ã®åˆ†é¡"""
        if risk_score >= 0.8:
            return 'severe'
        elif risk_score >= 0.6:
            return 'high'
        elif risk_score >= 0.4:
            return 'moderate'
        elif risk_score >= 0.2:
            return 'low'
        else:
            return 'minimal'
    
    def _calculate_separation_score(self, fg_analysis: Dict, bg_analysis: Dict, contamination_analysis: Dict) -> Dict[str, Any]:
        """åˆ†é›¢å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        # å‰æ™¯ãƒ»èƒŒæ™¯ã®åŒºåˆ¥åº¦
        fg_uniformity = fg_analysis.get('uniformity_score', 0)
        bg_uniformity = bg_analysis.get('uniformity_score', 0)
        
        # æ··å…¥ãƒªã‚¹ã‚¯ã®é€†æ•°
        contamination_risk = contamination_analysis.get('overall_contamination_risk', 1.0)
        separation_quality = 1.0 - contamination_risk
        
        # å¢ƒç•Œã®æ˜ç¢ºã•
        boundary_clarity = 1.0 - contamination_analysis.get('boundary_ambiguity', 1.0)
        
        # é‡ã¿ä»˜ãç·åˆã‚¹ã‚³ã‚¢
        overall_score = (
            fg_uniformity * 0.2 +
            bg_uniformity * 0.2 +
            separation_quality * 0.4 +
            boundary_clarity * 0.2
        )
        
        return {
            'overall_score': float(overall_score),
            'separation_quality': float(separation_quality),
            'boundary_clarity': float(boundary_clarity),
            'foreground_uniformity': float(fg_uniformity),
            'background_uniformity': float(bg_uniformity),
            'quality_grade': self._grade_separation_quality(overall_score)
        }
    
    def _grade_separation_quality(self, score: float) -> str:
        """åˆ†é›¢å“è³ªã‚¹ã‚³ã‚¢ã‚’ã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¤‰æ›"""
        if score >= 0.9:
            return 'A'
        elif score >= 0.8:
            return 'B'
        elif score >= 0.7:
            return 'C'
        elif score >= 0.6:
            return 'D'
        elif score >= 0.5:
            return 'E'
        else:
            return 'F'
    
    def _generate_separation_assessment(self, separation_score: Dict, contamination_analysis: Dict) -> Dict[str, Any]:
        """åˆ†é›¢è©•ä¾¡ã®ç·åˆã‚¢ã‚»ã‚¹ãƒ¡ãƒ³ãƒˆ"""
        overall_score = separation_score['overall_score']
        contamination_level = contamination_analysis['contamination_level']
        
        # æ”¹å–„æ¨å¥¨äº‹é …
        recommendations = []
        
        if contamination_analysis['high_similarity_risk']:
            recommendations.append('improve_color_contrast')
        
        if contamination_analysis['texture_confusion_risk']:
            recommendations.append('enhance_texture_differentiation')
        
        if contamination_analysis['blurry_boundary_risk']:
            recommendations.append('sharpen_segmentation_boundary')
        
        if overall_score < 0.7:
            recommendations.append('review_segmentation_parameters')
        
        # ä¸»è¦å•é¡Œã®ç‰¹å®š
        primary_issues = []
        
        if contamination_level in ['severe', 'high']:
            primary_issues.append('high_contamination_risk')
        
        if separation_score['boundary_clarity'] < 0.6:
            primary_issues.append('unclear_boundaries')
        
        if separation_score['foreground_uniformity'] < 0.5:
            primary_issues.append('inconsistent_foreground')
        
        return {
            'overall_assessment': 'good' if overall_score >= 0.7 else 'needs_improvement',
            'contamination_level': contamination_level,
            'primary_issues': primary_issues,
            'recommendations': recommendations,
            'extraction_reliability': self._assess_extraction_reliability(overall_score, contamination_level)
        }
    
    def _assess_extraction_reliability(self, score: float, contamination_level: str) -> str:
        """æŠ½å‡ºä¿¡é ¼æ€§ã®è©•ä¾¡"""
        if score >= 0.8 and contamination_level in ['minimal', 'low']:
            return 'high'
        elif score >= 0.6 and contamination_level in ['minimal', 'low', 'moderate']:
            return 'medium'
        else:
            return 'low'
    
    def _cluster_to_dict(self, cluster: ColorCluster) -> Dict[str, Any]:
        """ColorClusterã‚’è¾æ›¸ã«å¤‰æ›"""
        return {
            'center': cluster.center,
            'pixel_count': cluster.pixel_count,
            'percentage': cluster.percentage,
            'variance': cluster.variance
        }
    
    def analyze_extracted_image(self, original_image_path: str, extracted_image_path: str) -> Dict[str, Any]:
        """æŠ½å‡ºç”»åƒã®è§£æ"""
        try:
            # å…ƒç”»åƒèª­ã¿è¾¼ã¿
            original = cv2.imread(original_image_path)
            if original is None:
                return {'error': f'å…ƒç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {original_image_path}'}
            
            # æŠ½å‡ºç”»åƒèª­ã¿è¾¼ã¿
            extracted = cv2.imread(extracted_image_path)
            if extracted is None:
                return {'error': f'æŠ½å‡ºç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {extracted_image_path}'}
            
            # ã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰ãƒã‚¹ã‚¯ã‚’ä½œæˆ
            if extracted.shape[2] == 4:  # RGBA
                mask = extracted[:, :, 3]
            else:
                # RGBç”»åƒã®å ´åˆã€é»’èƒŒæ™¯ã‚’é™¤å¤–ã—ã¦ãƒã‚¹ã‚¯ã‚’ä½œæˆ
                gray_extracted = cv2.cvtColor(extracted, cv2.COLOR_BGR2GRAY)
                _, mask = cv2.threshold(gray_extracted, 10, 255, cv2.THRESH_BINARY)
            
            # å…ƒç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if original.shape[:2] != extracted.shape[:2]:
                original = cv2.resize(original, (extracted.shape[1], extracted.shape[0]))
            
            # åˆ†é›¢å“è³ªè§£æ
            analysis_result = self.analyze_separation_quality(original, mask)
            analysis_result['original_image_path'] = original_image_path
            analysis_result['extracted_image_path'] = extracted_image_path
            analysis_result['timestamp'] = datetime.now().isoformat()
            
            return analysis_result
            
        except Exception as e:
            return {'error': f'è§£æã‚¨ãƒ©ãƒ¼: {str(e)}'}
    
    def print_analysis_summary(self, analysis_result: Dict[str, Any]):
        """è§£æçµæœã®ã‚µãƒãƒªãƒ¼å‡ºåŠ›"""
        if 'error' in analysis_result:
            print(f"âŒ {analysis_result['error']}")
            return
        
        print("\n" + "="*50)
        print("ğŸ¨ èƒŒæ™¯ãƒ»å‰æ™¯åˆ†é›¢å“è³ªåˆ†æçµæœ")
        print("="*50)
        
        # åˆ†é›¢ã‚¹ã‚³ã‚¢
        separation_score = analysis_result.get('separation_score', {})
        print(f"ğŸ“Š åˆ†é›¢å“è³ª:")
        print(f"  ç·åˆã‚¹ã‚³ã‚¢: {separation_score.get('overall_score', 0):.3f}")
        print(f"  å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰: {separation_score.get('quality_grade', 'unknown')}")
        print(f"  å¢ƒç•Œæ˜ç¢ºåº¦: {separation_score.get('boundary_clarity', 0):.3f}")
        
        # æ··å…¥åˆ†æ
        contamination = analysis_result.get('contamination_analysis', {})
        print(f"\nâš ï¸ æ··å…¥ãƒªã‚¹ã‚¯:")
        print(f"  æ··å…¥ãƒ¬ãƒ™ãƒ«: {contamination.get('contamination_level', 'unknown')}")
        print(f"  ç·åˆãƒªã‚¹ã‚¯: {contamination.get('overall_contamination_risk', 0):.3f}")
        print(f"  è‰²é¡ä¼¼æ€§: {contamination.get('color_similarity', 0):.3f}")
        
        # ç·åˆè©•ä¾¡
        assessment = analysis_result.get('overall_assessment', {})
        print(f"\nğŸ¯ ç·åˆè©•ä¾¡:")
        print(f"  è©•ä¾¡: {assessment.get('overall_assessment', 'unknown')}")
        print(f"  æŠ½å‡ºä¿¡é ¼æ€§: {assessment.get('extraction_reliability', 'unknown')}")
        
        issues = assessment.get('primary_issues', [])
        if issues:
            print(f"  ä¸»è¦å•é¡Œ: {', '.join(issues)}")
        
        recommendations = assessment.get('recommendations', [])
        if recommendations:
            print(f"  æ¨å¥¨æ”¹å–„: {', '.join(recommendations)}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ èƒŒæ™¯ãƒ»å‰æ™¯åˆ†é›¢ç²¾åº¦æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ç”»åƒãƒ»ãƒã‚¹ã‚¯ä½œæˆ
    test_image = np.zeros((100, 100, 3), dtype=np.uint8)
    
    # èƒŒæ™¯ (é’)
    test_image[:, :] = [100, 150, 200]
    
    # å‰æ™¯ (èµ¤ã„å††)
    cv2.circle(test_image, (50, 50), 30, [200, 100, 100], -1)
    
    # ãƒã‚¹ã‚¯ä½œæˆ
    test_mask = np.zeros((100, 100), dtype=np.uint8)
    cv2.circle(test_mask, (50, 50), 30, 255, -1)
    
    # åˆ†æå™¨åˆæœŸåŒ–
    analyzer = ForegroundBackgroundAnalyzer()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆç”»åƒã§èƒŒæ™¯ãƒ»å‰æ™¯åˆ†é›¢åˆ†æä¸­...")
    analysis_result = analyzer.analyze_separation_quality(test_image, test_mask)
    
    # çµæœå‡ºåŠ›
    analyzer.print_analysis_summary(analysis_result)
    
    print(f"\nâœ… [P1-021] èƒŒæ™¯ãƒ»å‰æ™¯åˆ†é›¢ç²¾åº¦æ¸¬å®šå®Œäº†")


if __name__ == "__main__":
    main()