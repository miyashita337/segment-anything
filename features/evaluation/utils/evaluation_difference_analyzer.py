#!/usr/bin/env python3
"""
Evaluation Difference Analyzer - P1-015
è©•ä¾¡å·®åˆ†ã®å®šé‡åŒ–ã‚·ã‚¹ãƒ†ãƒ 

è‡ªå‹•è©•ä¾¡ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã®å·®åˆ†ã‚’å®šé‡åŒ–ã—ã€æ”¹å–„ç‚¹ã‚’ç‰¹å®š
"""

import os
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from collections import defaultdict
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_PLOTTING = True
except ImportError:
    HAS_PLOTTING = False

try:
    from scipy import stats
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


class EvaluationDifferenceAnalyzer:
    """
    è‡ªå‹•è©•ä¾¡ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã®å·®åˆ†åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    
    è‡ªå‹•å“è³ªã‚¹ã‚³ã‚¢ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã®ç›¸é–¢ã‚’åˆ†æã—ã€æ”¹å–„ç‚¹ã‚’ç‰¹å®š
    """
    
    def __init__(self, evaluation_data_path: str = None):
        """
        åˆæœŸåŒ–
        
        Args:
            evaluation_data_path: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
        """
        self.evaluation_data_path = evaluation_data_path or self._find_evaluation_data()
        self.evaluation_data = []
        self.analysis_results = {}
        self.correlation_results = {}
        
        # è©•ä¾¡ã‚¹ã‚³ã‚¢ãƒãƒƒãƒ”ãƒ³ã‚°
        self.rating_to_score = {
            'A': 5.0, 'B': 4.0, 'C': 3.0, 'D': 2.0, 'E': 1.0, 'F': 0.0
        }
        
        if self.evaluation_data_path and os.path.exists(self.evaluation_data_path):
            self.load_evaluation_data()
    
    def _find_evaluation_data(self) -> Optional[str]:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        possible_paths = [
            "features/evaluation/logs/kaname07_user_evaluation.jsonl",
            "logs/user_evaluation.jsonl",
            "evaluation_data.jsonl"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        return None
    
    def load_evaluation_data(self) -> bool:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        try:
            with open(self.evaluation_data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.evaluation_data.append(json.loads(line))
            
            print(f"âœ… è©•ä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.evaluation_data)}ä»¶")
            return True
            
        except Exception as e:
            print(f"âŒ è©•ä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def simulate_automatic_quality_scores(self) -> Dict[str, float]:
        """
        è‡ªå‹•å“è³ªã‚¹ã‚³ã‚¢ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        å®Ÿéš›ã®å®Ÿè£…ã§ã¯æ—¢å­˜ã®å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å–å¾—
        """
        simulated_scores = {}
        
        for item in self.evaluation_data:
            image_path = item.get('image_path', '')
            image_name = os.path.basename(image_path)
            
            # æ—¢å­˜ã®æˆåŠŸ/å¤±æ•—æƒ…å ±ã‹ã‚‰åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º
            base_score = 0.5
            
            # æŠ½å‡ºæˆåŠŸã®å ´åˆã¯ã‚¹ã‚³ã‚¢å‘ä¸Š
            if item.get('extraction_success', False):
                base_score += 0.3
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã‹ã‚‰ãƒ’ãƒ³ãƒˆã‚’å¾—ã‚‹ï¼ˆé€†ç®—ï¼‰
            user_rating = item.get('user_rating')
            if user_rating:
                user_score = self.rating_to_score.get(user_rating, 2.5) / 5.0
                # è‡ªå‹•è©•ä¾¡ã¯å¹³å‡çš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã‚ˆã‚Š0.1-0.2é«˜ãå‡ºã‚‹å‚¾å‘ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                base_score = user_score + np.random.normal(0.15, 0.1)
            else:
                # ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãŒãªã„å ´åˆã¯å•é¡Œã‚¿ã‚¤ãƒ—ã‹ã‚‰æ¨å®š
                problem = item.get('actual_problem', 'unknown')
                if problem == 'none':
                    base_score = np.random.uniform(0.7, 0.9)
                elif problem in ['extraction_failure', 'inappropriate_extraction_area']:
                    base_score = np.random.uniform(0.1, 0.4)
                else:
                    base_score = np.random.uniform(0.3, 0.7)
            
            # ã‚¹ã‚³ã‚¢ã‚’0-1ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
            simulated_scores[image_name] = np.clip(base_score, 0.0, 1.0)
        
        return simulated_scores
    
    def calculate_correlations(self) -> Dict[str, Any]:
        """ç›¸é–¢åˆ†æã®å®Ÿè¡Œ"""
        # è‡ªå‹•å“è³ªã‚¹ã‚³ã‚¢ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        auto_scores = self.simulate_automatic_quality_scores()
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        user_scores = []
        automatic_scores = []
        valid_data = []
        
        for item in self.evaluation_data:
            image_path = item.get('image_path', '')
            image_name = os.path.basename(image_path)
            user_rating = item.get('user_rating')
            
            if user_rating and image_name in auto_scores:
                user_score = self.rating_to_score[user_rating] / 5.0  # 0-1æ­£è¦åŒ–
                auto_score = auto_scores[image_name]
                
                user_scores.append(user_score)
                automatic_scores.append(auto_score)
                valid_data.append({
                    'image_name': image_name,
                    'user_score': user_score,
                    'auto_score': auto_score,
                    'user_rating': user_rating,
                    'difference': auto_score - user_score,
                    'abs_difference': abs(auto_score - user_score),
                    'actual_problem': item.get('actual_problem', 'unknown'),
                    'desired_region': item.get('desired_region', 'unknown')
                })
        
        if len(valid_data) < 3:
            return {"error": "ç›¸é–¢åˆ†æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        # çµ±è¨ˆæŒ‡æ¨™è¨ˆç®—
        user_scores = np.array(user_scores)
        automatic_scores = np.array(automatic_scores)
        
        if HAS_SCIPY:
            correlation_coef, p_value = stats.pearsonr(user_scores, automatic_scores)
            spearman_coef, spearman_p = stats.spearmanr(user_scores, automatic_scores)
            mse = mean_squared_error(user_scores, automatic_scores)
            mae = mean_absolute_error(user_scores, automatic_scores)
            r2 = r2_score(user_scores, automatic_scores)
        else:
            # Fallback implementations
            correlation_coef = np.corrcoef(user_scores, automatic_scores)[0, 1]
            p_value = 0.0  # Not calculated without scipy
            spearman_coef = correlation_coef  # Approximation
            spearman_p = 0.0
            mse = np.mean((user_scores - automatic_scores) ** 2)
            mae = np.mean(np.abs(user_scores - automatic_scores))
            r2 = 1 - (np.sum((user_scores - automatic_scores) ** 2) / np.sum((user_scores - np.mean(user_scores)) ** 2))
        
        self.correlation_results = {
            'sample_count': len(valid_data),
            'pearson_correlation': correlation_coef,
            'pearson_p_value': p_value,
            'spearman_correlation': spearman_coef,
            'spearman_p_value': spearman_p,
            'mean_squared_error': mse,
            'mean_absolute_error': mae,
            'r_squared': r2,
            'user_score_mean': float(user_scores.mean()),
            'user_score_std': float(user_scores.std()),
            'auto_score_mean': float(automatic_scores.mean()),
            'auto_score_std': float(automatic_scores.std()),
            'detailed_data': valid_data
        }
        
        return self.correlation_results
    
    def analyze_difference_patterns(self) -> Dict[str, Any]:
        """å·®åˆ†ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        if not self.correlation_results:
            self.calculate_correlations()
        
        valid_data = self.correlation_results.get('detailed_data', [])
        if not valid_data:
            return {"error": "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        # å·®åˆ†ã«ã‚ˆã‚‹åˆ†é¡
        overestimated = []  # è‡ªå‹•è©•ä¾¡ãŒé«˜ã™ãã‚‹
        underestimated = []  # è‡ªå‹•è©•ä¾¡ãŒä½ã™ãã‚‹
        accurate = []  # é©åˆ‡
        
        threshold = 0.2  # 20%ä»¥ä¸Šã®å·®åˆ†ã§åˆ†é¡
        
        for data in valid_data:
            diff = data['difference']
            if diff > threshold:
                overestimated.append(data)
            elif diff < -threshold:
                underestimated.append(data)
            else:
                accurate.append(data)
        
        # å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥ã®å·®åˆ†åˆ†æ
        problem_differences = defaultdict(list)
        for data in valid_data:
            problem = data['actual_problem']
            problem_differences[problem].append(data['difference'])
        
        # åœ°åŸŸåˆ¥ã®å·®åˆ†åˆ†æ
        region_differences = defaultdict(list)
        for data in valid_data:
            region = data['desired_region']
            if region not in ['unknown', 'success']:
                region_differences[region].append(data['difference'])
        
        # è©•ä¾¡ãƒ¬ãƒ™ãƒ«åˆ¥ã®å·®åˆ†åˆ†æ
        rating_differences = defaultdict(list)
        for data in valid_data:
            rating = data['user_rating']
            rating_differences[rating].append(data['difference'])
        
        pattern_analysis = {
            'classification': {
                'overestimated_count': len(overestimated),
                'underestimated_count': len(underestimated),
                'accurate_count': len(accurate),
                'overestimated_ratio': len(overestimated) / len(valid_data),
                'underestimated_ratio': len(underestimated) / len(valid_data),
                'accurate_ratio': len(accurate) / len(valid_data)
            },
            'problem_patterns': {
                problem: {
                    'count': len(diffs),
                    'mean_difference': float(np.mean(diffs)),
                    'std_difference': float(np.std(diffs)),
                    'bias_direction': 'overestimate' if np.mean(diffs) > 0.1 else 'underestimate' if np.mean(diffs) < -0.1 else 'neutral'
                }
                for problem, diffs in problem_differences.items()
                if len(diffs) >= 2
            },
            'region_patterns': {
                region: {
                    'count': len(diffs),
                    'mean_difference': float(np.mean(diffs)),
                    'std_difference': float(np.std(diffs)),
                    'bias_direction': 'overestimate' if np.mean(diffs) > 0.1 else 'underestimate' if np.mean(diffs) < -0.1 else 'neutral'
                }
                for region, diffs in region_differences.items()
                if len(diffs) >= 2
            },
            'rating_patterns': {
                rating: {
                    'count': len(diffs),
                    'mean_difference': float(np.mean(diffs)),
                    'std_difference': float(np.std(diffs)),
                    'bias_direction': 'overestimate' if np.mean(diffs) > 0.1 else 'underestimate' if np.mean(diffs) < -0.1 else 'neutral'
                }
                for rating, diffs in rating_differences.items()
                if len(diffs) >= 2
            },
            'worst_cases': {
                'most_overestimated': sorted([d for d in valid_data], key=lambda x: x['difference'], reverse=True)[:5],
                'most_underestimated': sorted([d for d in valid_data], key=lambda x: x['difference'])[:5]
            }
        }
        
        return pattern_analysis
    
    def generate_improvement_recommendations(self) -> List[Dict[str, str]]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        if not self.correlation_results:
            self.calculate_correlations()
        
        pattern_analysis = self.analyze_difference_patterns()
        
        recommendations = []
        
        # ç›¸é–¢ã®ä½ã•ã«åŸºã¥ãæ¨å¥¨
        correlation = self.correlation_results.get('pearson_correlation', 0)
        if correlation < 0.6:
            recommendations.append({
                'priority': 'high',
                'category': 'correlation_improvement',
                'issue': f"è‡ªå‹•è©•ä¾¡ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã®ç›¸é–¢ãŒä½ã„ (r={correlation:.3f})",
                'recommendation': "è‡ªå‹•è©•ä¾¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ ¹æœ¬çš„è¦‹ç›´ã—ãŒå¿…è¦ã€‚ã‚ˆã‚Šäººé–“ã®æ„Ÿè¦šã«è¿‘ã„æŒ‡æ¨™ã®å°å…¥ã‚’æ¤œè¨ã€‚",
                'implementation': "RegionPrioritySystemã®é‡ã¿èª¿æ•´ã€æ–°ã—ã„å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¿½åŠ "
            })
        
        # ç³»çµ±çš„ãƒã‚¤ã‚¢ã‚¹ã«åŸºã¥ãæ¨å¥¨
        classification = pattern_analysis.get('classification', {})
        overestimate_ratio = classification.get('overestimated_ratio', 0)
        underestimate_ratio = classification.get('underestimated_ratio', 0)
        
        if overestimate_ratio > 0.4:
            recommendations.append({
                'priority': 'medium',
                'category': 'bias_correction',
                'issue': f"è‡ªå‹•è©•ä¾¡ãŒç³»çµ±çš„ã«é«˜ã™ãã‚‹ (éå¤§è©•ä¾¡: {overestimate_ratio:.1%})",
                'recommendation': "å“è³ªã‚¹ã‚³ã‚¢ç®—å‡ºã®é–¾å€¤ã‚’ä¸‹ã’ã‚‹ã€ã‚ˆã‚Šå³ã—ã„è©•ä¾¡åŸºæº–ã®å°å…¥",
                'implementation': "confidence_threshold, size_thresholdç­‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´"
            })
        
        if underestimate_ratio > 0.4:
            recommendations.append({
                'priority': 'medium',
                'category': 'bias_correction',
                'issue': f"è‡ªå‹•è©•ä¾¡ãŒç³»çµ±çš„ã«ä½ã™ãã‚‹ (éå°è©•ä¾¡: {underestimate_ratio:.1%})",
                'recommendation': "å“è³ªã‚¹ã‚³ã‚¢ç®—å‡ºã®é–¾å€¤ã‚’ä¸Šã’ã‚‹ã€ã‚ˆã‚Šå¯›å®¹ãªè©•ä¾¡åŸºæº–ã®å°å…¥",
                'implementation': "è©•ä¾¡åŸºæº–ã®ç·©å’Œã€éƒ¨åˆ†çš„æˆåŠŸã®ç©æ¥µçš„è©•ä¾¡"
            })
        
        # å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥ã®æ¨å¥¨
        problem_patterns = pattern_analysis.get('problem_patterns', {})
        for problem, pattern in problem_patterns.items():
            if abs(pattern['mean_difference']) > 0.3:
                bias_direction = pattern['bias_direction']
                recommendations.append({
                    'priority': 'medium',
                    'category': 'problem_specific',
                    'issue': f"'{problem}'ã§è©•ä¾¡å·®åˆ†ãŒå¤§ãã„ (å¹³å‡å·®åˆ†: {pattern['mean_difference']:.3f})",
                    'recommendation': f"'{problem}'ç‰¹åŒ–ã®è©•ä¾¡èª¿æ•´ãŒå¿…è¦ ({bias_direction})",
                    'implementation': f"problem_type='{problem}'ã§ã®è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯è¦‹ç›´ã—"
                })
        
        # è©•ä¾¡ãƒ¬ãƒ™ãƒ«åˆ¥ã®æ¨å¥¨
        rating_patterns = pattern_analysis.get('rating_patterns', {})
        for rating, pattern in rating_patterns.items():
            if rating in ['A', 'F'] and abs(pattern['mean_difference']) > 0.2:
                recommendations.append({
                    'priority': 'high',
                    'category': 'extreme_rating',
                    'issue': f"{rating}è©•ä¾¡ã§ã®å·®åˆ†ãŒå¤§ãã„ (å¹³å‡å·®åˆ†: {pattern['mean_difference']:.3f})",
                    'recommendation': f"{rating}è©•ä¾¡ã‚±ãƒ¼ã‚¹ã®ç‰¹åˆ¥å‡¦ç†ãŒå¿…è¦",
                    'implementation': f"æ¥µç«¯ãªè©•ä¾¡({rating})ã®åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯æ”¹å–„"
                })
        
        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«åŸºã¥ãæ¨å¥¨
        sample_count = self.correlation_results.get('sample_count', 0)
        if sample_count < 50:
            recommendations.append({
                'priority': 'high',
                'category': 'data_insufficiency',
                'issue': f"åˆ†æã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ (ç¾åœ¨: {sample_count}ä»¶)",
                'recommendation': "ã‚ˆã‚Šå¤šãã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿åé›†ãŒå¿…è¦",
                'implementation': "è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡ã€ç¶™ç¶šçš„ãªè©•ä¾¡ãƒ‡ãƒ¼ã‚¿åé›†"
            })
        
        # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
        priority_order = {'high': 3, 'medium': 2, 'low': 1}
        recommendations.sort(key=lambda x: priority_order.get(x['priority'], 0), reverse=True)
        
        return recommendations
    
    def create_analysis_report(self) -> Dict[str, Any]:
        """ç·åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
        correlation_results = self.calculate_correlations()
        pattern_analysis = self.analyze_difference_patterns()
        recommendations = self.generate_improvement_recommendations()
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'sample_count': correlation_results.get('sample_count', 0),
                'correlation_strength': self._interpret_correlation(correlation_results.get('pearson_correlation', 0)),
                'mean_absolute_error': correlation_results.get('mean_absolute_error', 0),
                'accuracy_rate': pattern_analysis.get('classification', {}).get('accurate_ratio', 0),
                'primary_issue': recommendations[0]['issue'] if recommendations else "ç‰¹å®šã®å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
            },
            'correlation_analysis': correlation_results,
            'pattern_analysis': pattern_analysis,
            'recommendations': recommendations,
            'action_items': self._generate_action_items(recommendations)
        }
        
        return report
    
    def _interpret_correlation(self, correlation: float) -> str:
        """ç›¸é–¢ã®å¼·ã•ã‚’è§£é‡ˆ"""
        if abs(correlation) >= 0.8:
            return "å¼·ã„ç›¸é–¢"
        elif abs(correlation) >= 0.6:
            return "ä¸­ç¨‹åº¦ã®ç›¸é–¢"
        elif abs(correlation) >= 0.3:
            return "å¼±ã„ç›¸é–¢"
        else:
            return "ç›¸é–¢ãªã—"
    
    def _generate_action_items(self, recommendations: List[Dict]) -> List[str]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®ç”Ÿæˆ"""
        action_items = []
        
        for rec in recommendations[:5]:  # ä¸Šä½5ã¤
            category = rec['category']
            priority = rec['priority']
            implementation = rec['implementation']
            
            action_items.append(f"[{priority.upper()}] {category}: {implementation}")
        
        return action_items
    
    def save_analysis_report(self, output_path: str = None) -> str:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        if not output_path:
            output_path = f"evaluation_difference_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report = self.create_analysis_report()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·®åˆ†åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
        return output_path
    
    def print_summary(self):
        """ã‚µãƒãƒªãƒ¼ã®å‡ºåŠ›"""
        report = self.create_analysis_report()
        summary = report['summary']
        recommendations = report['recommendations']
        
        print("\n" + "="*60)
        print("ğŸ“Š è©•ä¾¡å·®åˆ†åˆ†æ - ã‚µãƒãƒªãƒ¼")
        print("="*60)
        
        print(f"\nğŸ“ˆ åˆ†æçµæœ:")
        print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {summary['sample_count']}ä»¶")
        print(f"  ç›¸é–¢ã®å¼·ã•: {summary['correlation_strength']}")
        print(f"  å¹³å‡çµ¶å¯¾èª¤å·®: {summary['mean_absolute_error']:.3f}")
        print(f"  é©åˆ‡è©•ä¾¡ç‡: {summary['accuracy_rate']:.1%}")
        
        print(f"\nğŸš¨ ä¸»è¦ãªå•é¡Œ:")
        print(f"  {summary['primary_issue']}")
        
        print(f"\nğŸ¯ æ”¹å–„æ¨å¥¨äº‹é … (ä¸Šä½3ä»¶):")
        for i, rec in enumerate(recommendations[:3], 1):
            print(f"  {i}. [{rec['priority'].upper()}] {rec['issue']}")
            print(f"     â†’ {rec['recommendation']}")
        
        if len(recommendations) > 3:
            print(f"  ... ä»–{len(recommendations)-3}ä»¶ã®æ¨å¥¨äº‹é …ãŒã‚ã‚Šã¾ã™")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ è©•ä¾¡å·®åˆ†åˆ†æã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    # åˆæœŸåŒ–
    analyzer = EvaluationDifferenceAnalyzer()
    
    if not analyzer.evaluation_data:
        print("âŒ è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # åˆ†æå®Ÿè¡Œ
    print("\nğŸ“Š è©•ä¾¡å·®åˆ†åˆ†æä¸­...")
    analyzer.calculate_correlations()
    
    # ã‚µãƒãƒªãƒ¼å‡ºåŠ›
    analyzer.print_summary()
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    print("\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ä¸­...")
    report_path = analyzer.save_analysis_report()
    
    print(f"\nâœ… [P1-015] è©•ä¾¡å·®åˆ†ã®å®šé‡åŒ–å®Œäº†")
    print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")


if __name__ == "__main__":
    main()