#!/usr/bin/env python3
"""
P1-016: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—æ§‹ç¯‰
è©•ä¾¡å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸç¶™ç¶šçš„å“è³ªæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 

Features:
- Automatic feedback integration
- Performance trend analysis
- Adaptive parameter adjustment
- Quality prediction improvement
- Learning effectiveness measurement
"""

import numpy as np
import json
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
from datetime import datetime, timedelta
import json

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ç”¨
HAS_SCIPY = True
HAS_SKLEARN = True

try:
    from scipy import stats
    from scipy.optimize import minimize_scalar
except ImportError:
    HAS_SCIPY = False

try:
    from sklearn.linear_model import LinearRegression, Ridge
    from sklearn.metrics import mean_squared_error, r2_score
    from sklearn.preprocessing import StandardScaler
except ImportError:
    HAS_SKLEARN = False


class FeedbackLoopSystem:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, feedback_data_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.name = "FeedbackLoopSystem"
        self.version = "1.0.0"
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‘ã‚¹
        if feedback_data_path:
            self.feedback_data_path = Path(feedback_data_path)
        else:
            self.feedback_data_path = Path("features/evaluation/logs/feedback_history.jsonl")
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.feedback_params = {
            'learning_rate': 0.1,           # å­¦ç¿’ç‡
            'momentum': 0.9,                # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
            'adaptation_threshold': 0.05,   # é©å¿œé–¾å€¤
            'trend_window': 10,             # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            'prediction_horizon': 5,        # äºˆæ¸¬æœŸé–“
            'confidence_threshold': 0.7     # ä¿¡é ¼åº¦é–¾å€¤
        }
        
        # æ”¹å–„å±¥æ­´
        self.improvement_history = []
        self.parameter_history = []
        
        # åˆæœŸåŒ–æ™‚ã«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self._load_feedback_history()
    
    def integrate_evaluation_feedback(self, evaluation_data: Dict[str, Any], 
                                    processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        è©•ä¾¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®çµ±åˆå‡¦ç†
        
        Args:
            evaluation_data: ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ãƒ‡ãƒ¼ã‚¿
            processing_results: è‡ªå‹•å‡¦ç†çµæœ
            
        Returns:
            Dict: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±åˆçµæœ
        """
        try:
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
            feedback_entry = self._create_feedback_entry(evaluation_data, processing_results)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            trend_analysis = self._analyze_performance_trend(feedback_entry)
            
            # é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
            parameter_adjustments = self._adjust_parameters_adaptively(feedback_entry, trend_analysis)
            
            # å“è³ªäºˆæ¸¬æ”¹å–„
            prediction_improvements = self._improve_quality_prediction(feedback_entry)
            
            # å­¦ç¿’åŠ¹æœæ¸¬å®š
            learning_effectiveness = self._measure_learning_effectiveness()
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä¿å­˜
            self._save_feedback_entry(feedback_entry)
            
            # æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ
            improvement_recommendations = self._generate_improvement_recommendations(
                trend_analysis, parameter_adjustments, prediction_improvements
            )
            
            return {
                'feedback_integration': {
                    'entry_id': feedback_entry['entry_id'],
                    'timestamp': feedback_entry['timestamp'],
                    'integration_success': True
                },
                'trend_analysis': trend_analysis,
                'parameter_adjustments': parameter_adjustments,
                'prediction_improvements': prediction_improvements,
                'learning_effectiveness': learning_effectiveness,
                'improvement_recommendations': improvement_recommendations,
                'processing_info': {
                    'version': self.version,
                    'feedback_count': len(self.improvement_history)
                }
            }
            
        except Exception as e:
            return self._generate_error_result(f"Feedback integration failed: {str(e)}")
    
    def _create_feedback_entry(self, evaluation_data: Dict, processing_results: Dict) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒªä½œæˆ"""
        entry_id = f"feedback_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(self.improvement_history):04d}"
        
        # è©•ä¾¡å·®åˆ†è¨ˆç®—
        auto_score = processing_results.get('quality_score', 0.0)
        user_score = evaluation_data.get('user_rating', 0.0)
        evaluation_diff = abs(auto_score - user_score)
        
        # å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º
        processing_params = self._extract_processing_parameters(processing_results)
        
        # çµæœãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
        result_metrics = self._extract_result_metrics(processing_results)
        
        feedback_entry = {
            'entry_id': entry_id,
            'timestamp': datetime.now().isoformat(),
            'evaluation_data': {
                'user_rating': user_score,
                'user_comments': evaluation_data.get('comments', ''),
                'evaluation_aspects': evaluation_data.get('aspects', {})
            },
            'processing_results': {
                'auto_quality_score': auto_score,
                'processing_time': processing_results.get('processing_time', 0.0),
                'success': processing_results.get('success', False)
            },
            'evaluation_difference': {
                'score_difference': evaluation_diff,
                'agreement_level': self._calculate_agreement_level(evaluation_diff)
            },
            'processing_parameters': processing_params,
            'result_metrics': result_metrics
        }
        
        return feedback_entry
    
    def _extract_processing_parameters(self, processing_results: Dict) -> Dict[str, Any]:
        """å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º"""
        return {
            'quality_method': processing_results.get('quality_method', 'unknown'),
            'enhancement_applied': processing_results.get('enhancement_applied', []),
            'yolo_score_threshold': processing_results.get('yolo_score_threshold', 0.0),
            'sam_parameters': processing_results.get('sam_parameters', {}),
            'preprocessing_steps': processing_results.get('preprocessing_steps', [])
        }
    
    def _extract_result_metrics(self, processing_results: Dict) -> Dict[str, Any]:
        """çµæœãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º"""
        return {
            'extraction_success': processing_results.get('success', False),
            'quality_score': processing_results.get('quality_score', 0.0),
            'confidence_score': processing_results.get('confidence_score', 0.0),
            'boundary_quality': processing_results.get('boundary_quality', 0.0),
            'completeness_score': processing_results.get('completeness_score', 0.0)
        }
    
    def _calculate_agreement_level(self, score_diff: float) -> str:
        """è©•ä¾¡ä¸€è‡´åº¦è¨ˆç®—"""
        if score_diff <= 0.1:
            return "high_agreement"
        elif score_diff <= 0.2:
            return "moderate_agreement"
        elif score_diff <= 0.4:
            return "low_agreement"
        else:
            return "strong_disagreement"
    
    def _analyze_performance_trend(self, current_entry: Dict) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        try:
            if len(self.improvement_history) < 2:
                return {'status': 'insufficient_data', 'trend': 'unknown'}
            
            # æœ€è¿‘ã®ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰è©•ä¾¡å·®åˆ†ã®æ¨ç§»ã‚’åˆ†æ
            recent_entries = self.improvement_history[-self.feedback_params['trend_window']:]
            recent_diffs = [entry['evaluation_difference']['score_difference'] for entry in recent_entries]
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            if HAS_SCIPY:
                # ç·šå½¢å›å¸°ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰
                x = np.arange(len(recent_diffs))
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, recent_diffs)
                
                trend_direction = "improving" if slope < 0 else "degrading" if slope > 0 else "stable"
                trend_strength = abs(r_value)
                
                # çµ±è¨ˆçš„æœ‰æ„æ€§
                is_significant = p_value < 0.05
                
                trend_analysis = {
                    'trend_direction': trend_direction,
                    'trend_strength': float(trend_strength),
                    'slope': float(slope),
                    'r_squared': float(r_value**2),
                    'p_value': float(p_value),
                    'is_statistically_significant': is_significant
                }
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç°¡æ˜“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
                first_half = recent_diffs[:len(recent_diffs)//2]
                second_half = recent_diffs[len(recent_diffs)//2:]
                
                first_avg = np.mean(first_half) if first_half else 0
                second_avg = np.mean(second_half) if second_half else 0
                
                if second_avg < first_avg - 0.02:
                    trend_direction = "improving"
                elif second_avg > first_avg + 0.02:
                    trend_direction = "degrading"
                else:
                    trend_direction = "stable"
                
                trend_analysis = {
                    'trend_direction': trend_direction,
                    'first_half_avg': float(first_avg),
                    'second_half_avg': float(second_avg),
                    'improvement': float(first_avg - second_avg)
                }
            
            # ç¾åœ¨ã®è©•ä¾¡å·®åˆ†ã¨ã®æ¯”è¼ƒ
            current_diff = current_entry['evaluation_difference']['score_difference']
            recent_avg = np.mean(recent_diffs)
            
            trend_analysis.update({
                'current_vs_recent_avg': float(current_diff - recent_avg),
                'recent_average_difference': float(recent_avg),
                'data_points_analyzed': len(recent_diffs)
            })
            
            return trend_analysis
            
        except Exception as e:
            return {'error': f'Trend analysis failed: {str(e)}'}
    
    def _adjust_parameters_adaptively(self, feedback_entry: Dict, 
                                    trend_analysis: Dict) -> Dict[str, Any]:
        """é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´"""
        try:
            adjustments = {}
            
            evaluation_diff = feedback_entry['evaluation_difference']['score_difference']
            agreement_level = feedback_entry['evaluation_difference']['agreement_level']
            
            # è©•ä¾¡å·®åˆ†ã«åŸºã¥ãèª¿æ•´
            if evaluation_diff > self.feedback_params['adaptation_threshold']:
                if agreement_level in ['low_agreement', 'strong_disagreement']:
                    # å¤§ããªè©•ä¾¡å·®åˆ†ãŒã‚ã‚‹å ´åˆã®èª¿æ•´
                    adjustments.update(self._generate_disagreement_adjustments(feedback_entry))
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ã«åŸºã¥ãèª¿æ•´
            if 'trend_direction' in trend_analysis:
                if trend_analysis['trend_direction'] == 'degrading':
                    adjustments.update(self._generate_degradation_adjustments(trend_analysis))
                elif trend_analysis['trend_direction'] == 'improving':
                    adjustments.update(self._generate_improvement_adjustments(trend_analysis))
            
            # å“è³ªæ‰‹æ³•å›ºæœ‰ã®èª¿æ•´
            quality_method = feedback_entry['processing_parameters'].get('quality_method')
            if quality_method:
                method_adjustments = self._generate_method_specific_adjustments(
                    quality_method, feedback_entry
                )
                adjustments.update(method_adjustments)
            
            # èª¿æ•´ã®å®ŸåŠ¹æ€§è©•ä¾¡
            adjustment_confidence = self._evaluate_adjustment_confidence(adjustments, trend_analysis)
            
            return {
                'parameter_adjustments': adjustments,
                'adjustment_confidence': adjustment_confidence,
                'adjustment_rationale': self._generate_adjustment_rationale(adjustments, trend_analysis),
                'recommended_application': adjustment_confidence > self.feedback_params['confidence_threshold']
            }
            
        except Exception as e:
            return {'error': f'Parameter adjustment failed: {str(e)}'}
    
    def _generate_disagreement_adjustments(self, feedback_entry: Dict) -> Dict[str, Any]:
        """è©•ä¾¡ä¸ä¸€è‡´æ™‚ã®èª¿æ•´"""
        adjustments = {}
        
        auto_score = feedback_entry['processing_results']['auto_quality_score']
        user_score = feedback_entry['evaluation_data']['user_rating']
        
        if auto_score > user_score:
            # è‡ªå‹•è©•ä¾¡ãŒéå¤§è©•ä¾¡ã—ã¦ã„ã‚‹å ´åˆ
            adjustments['quality_threshold'] = {'adjustment': -0.05, 'reason': 'auto_overestimation'}
            adjustments['confidence_scaling'] = {'adjustment': 0.9, 'reason': 'reduce_overconfidence'}
        else:
            # è‡ªå‹•è©•ä¾¡ãŒéå°è©•ä¾¡ã—ã¦ã„ã‚‹å ´åˆ
            adjustments['quality_threshold'] = {'adjustment': 0.03, 'reason': 'auto_underestimation'}
            adjustments['sensitivity_increase'] = {'adjustment': 1.1, 'reason': 'increase_sensitivity'}
        
        return adjustments
    
    def _generate_degradation_adjustments(self, trend_analysis: Dict) -> Dict[str, Any]:
        """æ€§èƒ½åŠ£åŒ–æ™‚ã®èª¿æ•´"""
        adjustments = {}
        
        # ã‚ˆã‚Šä¿å®ˆçš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        adjustments['learning_rate'] = {
            'adjustment': self.feedback_params['learning_rate'] * 0.8, 
            'reason': 'performance_degradation'
        }
        
        # å“è³ªé–¾å€¤ã®èª¿æ•´
        adjustments['quality_threshold_increase'] = {
            'adjustment': 0.02, 
            'reason': 'compensate_degradation'
        }
        
        return adjustments
    
    def _generate_improvement_adjustments(self, trend_analysis: Dict) -> Dict[str, Any]:
        """æ€§èƒ½æ”¹å–„æ™‚ã®èª¿æ•´"""
        adjustments = {}
        
        # ã‚ˆã‚Šç©æ¥µçš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        adjustments['learning_rate'] = {
            'adjustment': self.feedback_params['learning_rate'] * 1.1, 
            'reason': 'performance_improvement'
        }
        
        # é©å¿œçš„é–¾å€¤èª¿æ•´
        adjustments['adaptive_threshold'] = {
            'adjustment': -0.01, 
            'reason': 'leverage_improvement'
        }
        
        return adjustments
    
    def _generate_method_specific_adjustments(self, quality_method: str, 
                                            feedback_entry: Dict) -> Dict[str, Any]:
        """å“è³ªæ‰‹æ³•å›ºæœ‰ã®èª¿æ•´"""
        adjustments = {}
        
        user_score = feedback_entry['evaluation_data']['user_rating']
        auto_score = feedback_entry['processing_results']['auto_quality_score']
        
        if quality_method == 'balanced':
            if user_score < auto_score - 0.2:
                adjustments['balanced_weight_adjustment'] = {
                    'adjustment': {'size_weight': -0.1, 'confidence_weight': 0.1},
                    'reason': 'balanced_method_recalibration'
                }
        
        elif quality_method == 'size_priority':
            if user_score > auto_score + 0.15:
                adjustments['size_priority_enhancement'] = {
                    'adjustment': {'size_threshold': 0.05},
                    'reason': 'size_priority_underperforming'
                }
        
        return adjustments
    
    def _evaluate_adjustment_confidence(self, adjustments: Dict, 
                                      trend_analysis: Dict) -> float:
        """èª¿æ•´ä¿¡é ¼æ€§è©•ä¾¡"""
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼æ€§
        
        # ãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã‚‹ä¿¡é ¼æ€§èª¿æ•´
        data_points = trend_analysis.get('data_points_analyzed', 0)
        if data_points >= 10:
            confidence += 0.2
        elif data_points >= 5:
            confidence += 0.1
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ã®çµ±è¨ˆçš„æœ‰æ„æ€§
        if trend_analysis.get('is_statistically_significant', False):
            confidence += 0.2
        
        # èª¿æ•´ã®ä¸€è²«æ€§
        if len(adjustments) > 0:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _generate_adjustment_rationale(self, adjustments: Dict, 
                                     trend_analysis: Dict) -> str:
        """èª¿æ•´æ ¹æ‹ èª¬æ˜ç”Ÿæˆ"""
        if not adjustments:
            return "no_adjustments_needed"
        
        trend_direction = trend_analysis.get('trend_direction', 'unknown')
        
        if trend_direction == 'degrading':
            return "performance_degradation_detected"
        elif trend_direction == 'improving':
            return "performance_improvement_leveraging"
        else:
            return "evaluation_disagreement_compensation"
    
    def _improve_quality_prediction(self, feedback_entry: Dict) -> Dict[str, Any]:
        """å“è³ªäºˆæ¸¬æ”¹å–„"""
        try:
            if len(self.improvement_history) < 5:
                return {'status': 'insufficient_data_for_prediction'}
            
            # ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™
            features, targets = self._prepare_prediction_data()
            
            if HAS_SKLEARN and len(features) > 0:
                # æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹äºˆæ¸¬æ”¹å–„
                prediction_improvement = self._ml_based_prediction_improvement(features, targets)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šçµ±è¨ˆãƒ™ãƒ¼ã‚¹æ”¹å–„
                prediction_improvement = self._statistical_prediction_improvement(features, targets)
            
            # ç¾åœ¨ã‚¨ãƒ³ãƒˆãƒªã§ã®äºˆæ¸¬ç²¾åº¦è©•ä¾¡
            current_prediction_accuracy = self._evaluate_current_prediction(feedback_entry)
            
            prediction_improvement.update({
                'current_prediction_accuracy': current_prediction_accuracy,
                'prediction_improvement_available': prediction_improvement.get('model_r2', 0) > 0.3
            })
            
            return prediction_improvement
            
        except Exception as e:
            return {'error': f'Prediction improvement failed: {str(e)}'}
    
    def _prepare_prediction_data(self) -> Tuple[List[List[float]], List[float]]:
        """äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        features = []
        targets = []
        
        for entry in self.improvement_history:
            # ç‰¹å¾´é‡ï¼šå‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨çµæœãƒ¡ãƒˆãƒªã‚¯ã‚¹
            feature_vector = [
                entry['processing_results'].get('auto_quality_score', 0.0),
                entry['processing_results'].get('processing_time', 0.0),
                len(entry['processing_parameters'].get('enhancement_applied', [])),
                entry['result_metrics'].get('confidence_score', 0.0),
                entry['result_metrics'].get('boundary_quality', 0.0)
            ]
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡
            target = entry['evaluation_data'].get('user_rating', 0.0)
            
            features.append(feature_vector)
            targets.append(target)
        
        return features, targets
    
    def _ml_based_prediction_improvement(self, features: List[List[float]], 
                                       targets: List[float]) -> Dict[str, Any]:
        """æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹äºˆæ¸¬æ”¹å–„"""
        try:
            X = np.array(features)
            y = np.array(targets)
            
            # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # ãƒªãƒƒã‚¸å›å¸°ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æŠ‘åˆ¶
            model = Ridge(alpha=1.0)
            model.fit(X_scaled, y)
            
            # äºˆæ¸¬ç²¾åº¦è©•ä¾¡
            predictions = model.predict(X_scaled)
            r2 = r2_score(y, predictions)
            mse = mean_squared_error(y, predictions)
            
            # ç‰¹å¾´é‡é‡è¦åº¦
            feature_importance = np.abs(model.coef_)
            feature_names = ['auto_score', 'processing_time', 'enhancement_count', 
                           'confidence', 'boundary_quality']
            
            importance_ranking = sorted(zip(feature_names, feature_importance), 
                                      key=lambda x: x[1], reverse=True)
            
            return {
                'model_type': 'ridge_regression',
                'model_r2': float(r2),
                'model_mse': float(mse),
                'feature_importance': dict(importance_ranking),
                'prediction_model_available': True
            }
            
        except Exception as e:
            return {'error': f'ML prediction improvement failed: {str(e)}'}
    
    def _statistical_prediction_improvement(self, features: List[List[float]], 
                                          targets: List[float]) -> Dict[str, Any]:
        """çµ±è¨ˆãƒ™ãƒ¼ã‚¹äºˆæ¸¬æ”¹å–„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            if not features or not targets:
                return {'status': 'no_data'}
            
            # è‡ªå‹•ã‚¹ã‚³ã‚¢ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã®ç›¸é–¢
            auto_scores = [f[0] for f in features]  # æœ€åˆã®ç‰¹å¾´é‡ãŒè‡ªå‹•ã‚¹ã‚³ã‚¢
            
            if HAS_SCIPY:
                correlation, p_value = stats.pearsonr(auto_scores, targets)
            else:
                # ç°¡æ˜“ç›¸é–¢è¨ˆç®—
                correlation = np.corrcoef(auto_scores, targets)[0, 1]
                p_value = 1.0  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            # ç·šå½¢è£œæ­£ä¿‚æ•°è¨ˆç®—
            if len(auto_scores) > 1:
                slope = (np.mean(targets) - np.mean(auto_scores)) / np.std(auto_scores) if np.std(auto_scores) > 0 else 1.0
                intercept = np.mean(targets) - slope * np.mean(auto_scores)
            else:
                slope, intercept = 1.0, 0.0
            
            return {
                'model_type': 'statistical_correlation',
                'correlation': float(correlation),
                'p_value': float(p_value),
                'linear_correction': {'slope': float(slope), 'intercept': float(intercept)},
                'prediction_model_available': abs(correlation) > 0.3
            }
            
        except Exception as e:
            return {'error': f'Statistical prediction improvement failed: {str(e)}'}
    
    def _evaluate_current_prediction(self, feedback_entry: Dict) -> float:
        """ç¾åœ¨äºˆæ¸¬ç²¾åº¦è©•ä¾¡"""
        auto_score = feedback_entry['processing_results']['auto_quality_score']
        user_score = feedback_entry['evaluation_data']['user_rating']
        
        # äºˆæ¸¬èª¤å·®ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
        prediction_error = abs(auto_score - user_score)
        
        # ç²¾åº¦ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
        accuracy = max(0, 1.0 - prediction_error)
        
        return accuracy
    
    def _measure_learning_effectiveness(self) -> Dict[str, Any]:
        """å­¦ç¿’åŠ¹æœæ¸¬å®š"""
        try:
            if len(self.improvement_history) < 10:
                return {'status': 'insufficient_data', 'effectiveness': 'unknown'}
            
            # æ™‚ç³»åˆ—ã§ã®æ”¹å–„åŠ¹æœæ¸¬å®š
            recent_period = self.improvement_history[-5:]
            earlier_period = self.improvement_history[-10:-5]
            
            # è©•ä¾¡å·®åˆ†ã®æ”¹å–„
            recent_diffs = [e['evaluation_difference']['score_difference'] for e in recent_period]
            earlier_diffs = [e['evaluation_difference']['score_difference'] for e in earlier_period]
            
            recent_avg = np.mean(recent_diffs)
            earlier_avg = np.mean(earlier_diffs)
            improvement = earlier_avg - recent_avg  # æ­£ã®å€¤ãŒæ”¹å–„
            
            # ä¸€è‡´åº¦ã®æ”¹å–„
            recent_agreements = [e['evaluation_difference']['agreement_level'] for e in recent_period]
            high_agreement_ratio = sum(1 for a in recent_agreements if a in ['high_agreement', 'moderate_agreement']) / len(recent_agreements)
            
            # å­¦ç¿’åŠ¹æœè©•ä¾¡
            if improvement > 0.02 and high_agreement_ratio > 0.6:
                effectiveness = "high"
            elif improvement > 0.01 or high_agreement_ratio > 0.4:
                effectiveness = "moderate"
            elif improvement >= 0:
                effectiveness = "low"
            else:
                effectiveness = "negative"
            
            return {
                'effectiveness': effectiveness,
                'improvement_score': float(improvement),
                'high_agreement_ratio': float(high_agreement_ratio),
                'recent_avg_difference': float(recent_avg),
                'earlier_avg_difference': float(earlier_avg),
                'feedback_entries_analyzed': len(self.improvement_history)
            }
            
        except Exception as e:
            return {'error': f'Learning effectiveness measurement failed: {str(e)}'}
    
    def _generate_improvement_recommendations(self, trend_analysis: Dict, 
                                            parameter_adjustments: Dict,
                                            prediction_improvements: Dict) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        trend_direction = trend_analysis.get('trend_direction')
        if trend_direction == 'degrading':
            recommendations.append("investigate_performance_degradation")
            recommendations.append("review_recent_parameter_changes")
        elif trend_direction == 'improving':
            recommendations.append("maintain_current_improvements")
            recommendations.append("consider_parameter_optimization")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´æ¨å¥¨
        if parameter_adjustments.get('recommended_application', False):
            recommendations.append("apply_adaptive_parameter_adjustments")
        
        # äºˆæ¸¬æ”¹å–„æ¨å¥¨
        if prediction_improvements.get('prediction_model_available', False):
            recommendations.append("integrate_improved_prediction_model")
        
        # ãƒ‡ãƒ¼ã‚¿åé›†æ¨å¥¨
        if len(self.improvement_history) < 20:
            recommendations.append("increase_feedback_data_collection")
        
        return recommendations
    
    def _load_feedback_history(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´èª­ã¿è¾¼ã¿"""
        try:
            if self.feedback_data_path.exists():
                with open(self.feedback_data_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            entry = json.loads(line.strip())
                            self.improvement_history.append(entry)
                            
                print(f"ğŸ“š ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´èª­ã¿è¾¼ã¿: {len(self.improvement_history)}ä»¶")
            else:
                print(f"ğŸ“ æ–°è¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ã‚’é–‹å§‹: {self.feedback_data_path}")
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                self.feedback_data_path.parent.mkdir(parents=True, exist_ok=True)
                
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.improvement_history = []
    
    def _save_feedback_entry(self, feedback_entry: Dict):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒªä¿å­˜"""
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            self.feedback_data_path.parent.mkdir(parents=True, exist_ok=True)
            
            # JSONLå½¢å¼ã§è¿½è¨˜
            with open(self.feedback_data_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(feedback_entry, ensure_ascii=False) + '\n')
            
            # ãƒ¡ãƒ¢ãƒªå†…å±¥æ­´ã‚‚æ›´æ–°
            self.improvement_history.append(feedback_entry)
            
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒªä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _generate_error_result(self, error_message: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœç”Ÿæˆ"""
        return {
            'error': error_message,
            'feedback_integration': {
                'integration_success': False
            }
        }


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ P1-016: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ†ã‚¹ãƒˆç”¨è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    test_evaluation = {
        'user_rating': 0.75,
        'comments': 'Good extraction but some background noise',
        'aspects': {'quality': 0.8, 'completeness': 0.7}
    }
    
    test_processing = {
        'quality_score': 0.68,
        'processing_time': 3.2,
        'success': True,
        'quality_method': 'balanced',
        'enhancement_applied': ['contrast_enhancement'],
        'yolo_score_threshold': 0.05
    }
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
    feedback_system = FeedbackLoopSystem()
    result = feedback_system.integrate_evaluation_feedback(test_evaluation, test_processing)
    
    print("\nğŸ“Š ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—çµ±åˆçµæœ:")
    if 'error' not in result:
        integration = result.get('feedback_integration', {})
        print(f"  çµ±åˆæˆåŠŸ: {integration.get('integration_success', False)}")
        print(f"  ã‚¨ãƒ³ãƒˆãƒªID: {integration.get('entry_id', 'N/A')}")
        
        # æ”¹å–„æ¨å¥¨äº‹é …
        recommendations = result.get('improvement_recommendations', [])
        if recommendations:
            print(f"  æ”¹å–„æ¨å¥¨: {', '.join(recommendations)}")
        
        # å­¦ç¿’åŠ¹æœ
        learning = result.get('learning_effectiveness', {})
        if 'effectiveness' in learning:
            print(f"  å­¦ç¿’åŠ¹æœ: {learning['effectiveness']}")
    else:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    print(f"\nâœ… [P1-016] ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")


if __name__ == "__main__":
    main()