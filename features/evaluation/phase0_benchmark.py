#!/usr/bin/env python3
"""
Phase 0: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ã‚·ã‚¹ãƒ†ãƒ 
æ—¢å­˜YOLO/SAM + é¢ç©æœ€å¤§é¸æŠã«ã‚ˆã‚‹ç¾çŠ¶æ€§èƒ½ã®å®šé‡åŒ–
"""

import numpy as np
import cv2

import json
import logging
import time
from dataclasses import asdict, dataclass
from features.common.project_tracker import ProjectTracker
from features.extraction.models.sam_wrapper import SAMModelWrapper
# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from features.extraction.models.yolo_wrapper import YOLOModelWrapper
from pathlib import Path
from PIL import Image
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    image_id: str
    image_path: str
    largest_char_predicted: bool  # æœ€å¤§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’æ­£ã—ãæŠ½å‡ºã§ããŸã‹
    prediction_bbox: Optional[Tuple[int, int, int, int]]  # äºˆæ¸¬bbox (x, y, w, h)
    ground_truth_bbox: Tuple[int, int, int, int]  # äººé–“ãƒ©ãƒ™ãƒ«bbox
    iou_score: float  # IoU ã‚¹ã‚³ã‚¢
    confidence_score: float  # äºˆæ¸¬ä¿¡é ¼åº¦
    processing_time: float  # å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
    character_count: int  # æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å€™è£œæ•°
    area_largest_ratio: float  # æœ€å¤§é¢ç©ã®å€™è£œãŒå ã‚ã‚‹æ¯”ç‡
    quality_grade: str  # A/B/C/D/E/Fè©•ä¾¡
    notes: str = ""  # è¿½åŠ æƒ…å ±


@dataclass
class BenchmarkSummary:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é›†è¨ˆçµæœ"""
    total_images: int
    largest_char_accuracy: float  # æœ€å¤§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ­£è§£ç‡
    mean_iou: float  # å¹³å‡IoU
    ab_evaluation_rate: float  # A/Bè©•ä¾¡ç‡
    mean_processing_time: float  # å¹³å‡å‡¦ç†æ™‚é–“
    grade_distribution: Dict[str, int]  # è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
    processing_stats: Dict[str, float]  # å‡¦ç†çµ±è¨ˆæƒ…å ±


class Phase0Benchmark:
    """Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_root: Path):
        """
        åˆæœŸåŒ–
        
        Args:
            project_root: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.project_root = project_root
        self.results_dir = project_root / "benchmark_results" / "phase0"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.labels_file = project_root / "extracted_labels.json"
        self.ground_truth_labels = self.load_ground_truth_labels()
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.yolo_wrapper = None
        self.sam_wrapper = None
        
        # çµæœæ ¼ç´
        self.benchmark_results: List[BenchmarkResult] = []
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒˆãƒ©ãƒƒã‚«ãƒ¼
        self.tracker = ProjectTracker(project_root)
        
    def load_ground_truth_labels(self) -> Dict[str, Any]:
        """äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            if not self.labels_file.exists():
                raise FileNotFoundError(f"ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.labels_file}")
            
            with open(self.labels_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(data)}ãƒ•ã‚¡ã‚¤ãƒ«")
            return data
            
        except Exception as e:
            logger.error(f"ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def initialize_models(self):
        """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        try:
            logger.info("ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–é–‹å§‹")
            
            # YOLOåˆæœŸåŒ–
            yolo_model_path = self.project_root / "yolov8x.pt"
            if not yolo_model_path.exists():
                yolo_model_path = self.project_root / "yolov8n.pt"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            self.yolo_wrapper = YOLOModelWrapper(
                model_path=str(yolo_model_path),
                device="cuda" if self._check_cuda() else "cpu"
            )
            
            # SAMåˆæœŸåŒ–
            sam_checkpoint = self.project_root / "sam_vit_h_4b8939.pth"
            if not sam_checkpoint.exists():
                raise FileNotFoundError(f"SAMãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sam_checkpoint}")
                
            self.sam_wrapper = SAMModelWrapper(
                checkpoint_path=str(sam_checkpoint),
                model_type="vit_h",
                device="cuda" if self._check_cuda() else "cpu"
            )
            
            logger.info("ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
            
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _check_cuda(self) -> bool:
        """CUDAåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def calculate_iou(self, bbox1: Tuple[int, int, int, int], 
                     bbox2: Tuple[int, int, int, int]) -> float:
        """
        IoUè¨ˆç®—
        
        Args:
            bbox1: (x, y, w, h) å½¢å¼
            bbox2: (x, y, w, h) å½¢å¼
            
        Returns:
            IoU ã‚¹ã‚³ã‚¢
        """
        try:
            # (x, y, w, h) -> (x1, y1, x2, y2) å¤‰æ›
            x1_1, y1_1, w1, h1 = bbox1
            x2_1, y2_1 = x1_1 + w1, y1_1 + h1
            
            x1_2, y1_2, w2, h2 = bbox2
            x2_2, y2_2 = x1_2 + w2, y1_2 + h2
            
            # äº¤å·®é ˜åŸŸè¨ˆç®—
            x1_inter = max(x1_1, x1_2)
            y1_inter = max(y1_1, y1_2)
            x2_inter = min(x2_1, x2_2)
            y2_inter = min(y2_1, y2_2)
            
            if x2_inter <= x1_inter or y2_inter <= y1_inter:
                return 0.0
            
            intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)
            
            # åˆè¨ˆé ˜åŸŸè¨ˆç®—
            area1 = w1 * h1
            area2 = w2 * h2
            union = area1 + area2 - intersection
            
            return intersection / union if union > 0 else 0.0
            
        except Exception as e:
            logger.error(f"IoUè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def extract_character_with_current_system(self, image_path: Path) -> Dict[str, Any]:
        """
        ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºå®Ÿè¡Œ
        
        Args:
            image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            æŠ½å‡ºçµæœè¾æ›¸
        """
        try:
            start_time = time.time()
            
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(str(image_path))
            if image is None:
                raise ValueError(f"ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {image_path}")
            
            # YOLOåˆæœŸåŒ–ï¼ˆå¿…è¦ãªå ´åˆï¼‰
            if not self.yolo_wrapper.is_loaded:
                self.yolo_wrapper.load_model()
            
            # YOLOæ¤œå‡º
            yolo_results = self.yolo_wrapper.detect_persons(image)
            
            if not yolo_results or len(yolo_results) == 0:
                return {
                    "success": False,
                    "reason": "YOLOæ¤œå‡ºçµæœãªã—",
                    "processing_time": time.time() - start_time,
                    "character_count": 0
                }
            
            # é¢ç©æœ€å¤§ã®å€™è£œã‚’é¸æŠ
            largest_detection = max(yolo_results, key=lambda x: x.get('area', 0))
            
            # SAMã§ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            bbox = largest_detection['bbox']  # (x, y, w, h)
            sam_result = self.sam_wrapper.segment_from_bbox(image, bbox)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "largest_bbox": bbox,
                "confidence": largest_detection.get('confidence', 0.0),
                "character_count": len(yolo_results),
                "area_largest_ratio": largest_detection.get('area', 0) / sum(r.get('area', 0) for r in yolo_results),
                "processing_time": processing_time,
                "sam_mask": sam_result.get('mask') if sam_result else None
            }
            
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({image_path}): {e}")
            return {
                "success": False,
                "reason": str(e),
                "processing_time": time.time() - start_time,
                "character_count": 0
            }
    
    def evaluate_single_image(self, image_id: str, image_data: Dict[str, Any]) -> BenchmarkResult:
        """
        å˜ä¸€ç”»åƒã®è©•ä¾¡å®Ÿè¡Œ
        
        Args:
            image_id: ç”»åƒID
            image_data: äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        try:
            # ç”»åƒãƒ‘ã‚¹æ§‹ç¯‰
            image_path = self.project_root / "test_small" / f"{image_id}.png"
            if not image_path.exists():
                image_path = self.project_root / "test_small" / f"{image_id}.jpg"
            
            if not image_path.exists():
                logger.warning(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_id}")
                return self._create_error_result(image_id, str(image_path), "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸åœ¨")
            
            # äººé–“ãƒ©ãƒ™ãƒ«ï¼ˆground truthï¼‰
            gt_bbox = tuple(image_data['red_box_coords'])  # (x, y, w, h)
            
            # ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã§æŠ½å‡º
            extraction_result = self.extract_character_with_current_system(image_path)
            
            if not extraction_result["success"]:
                return self._create_error_result(
                    image_id, str(image_path), 
                    extraction_result.get("reason", "æŠ½å‡ºå¤±æ•—")
                )
            
            # äºˆæ¸¬çµæœ
            pred_bbox = tuple(extraction_result["largest_bbox"])
            
            # IoUè¨ˆç®—
            iou_score = self.calculate_iou(pred_bbox, gt_bbox)
            
            # æœ€å¤§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ­£è§£åˆ¤å®šï¼ˆIoUé–¾å€¤: 0.5ï¼‰
            largest_char_predicted = iou_score >= 0.5
            
            # å“è³ªè©•ä¾¡
            quality_grade = self._calculate_quality_grade(iou_score, extraction_result["confidence"])
            
            return BenchmarkResult(
                image_id=image_id,
                image_path=str(image_path),
                largest_char_predicted=largest_char_predicted,
                prediction_bbox=pred_bbox,
                ground_truth_bbox=gt_bbox,
                iou_score=iou_score,
                confidence_score=extraction_result["confidence"],
                processing_time=extraction_result["processing_time"],
                character_count=extraction_result["character_count"],
                area_largest_ratio=extraction_result.get("area_largest_ratio", 0.0),
                quality_grade=quality_grade,
                notes=f"IoU: {iou_score:.3f}, Conf: {extraction_result['confidence']:.3f}"
            )
            
        except Exception as e:
            logger.error(f"ç”»åƒè©•ä¾¡ã‚¨ãƒ©ãƒ¼ ({image_id}): {e}")
            return self._create_error_result(image_id, str(image_path), str(e))
    
    def _create_error_result(self, image_id: str, image_path: str, error_msg: str) -> BenchmarkResult:
        """ã‚¨ãƒ©ãƒ¼æ™‚ã®çµæœä½œæˆ"""
        return BenchmarkResult(
            image_id=image_id,
            image_path=image_path,
            largest_char_predicted=False,
            prediction_bbox=None,
            ground_truth_bbox=(0, 0, 0, 0),
            iou_score=0.0,
            confidence_score=0.0,
            processing_time=0.0,
            character_count=0,
            area_largest_ratio=0.0,
            quality_grade="F",
            notes=f"ã‚¨ãƒ©ãƒ¼: {error_msg}"
        )
    
    def _calculate_quality_grade(self, iou_score: float, confidence: float) -> str:
        """å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰è¨ˆç®—"""
        combined_score = (iou_score * 0.7) + (confidence * 0.3)
        
        if combined_score >= 0.9:
            return "A"
        elif combined_score >= 0.8:
            return "B"
        elif combined_score >= 0.6:
            return "C"
        elif combined_score >= 0.4:
            return "D"
        elif combined_score >= 0.2:
            return "E"
        else:
            return "F"
    
    def run_full_benchmark(self) -> BenchmarkSummary:
        """
        å…¨ä½“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        
        Returns:
            ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é›†è¨ˆçµæœ
        """
        try:
            logger.info("Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            
            # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            if self.yolo_wrapper is None or self.sam_wrapper is None:
                self.initialize_models()
            
            # å…¨ç”»åƒã§è©•ä¾¡å®Ÿè¡Œ
            total_images = len(self.ground_truth_labels)
            processed_count = 0
            
            for image_id, image_data in self.ground_truth_labels.items():
                logger.info(f"è©•ä¾¡é€²è¡Œä¸­: {processed_count + 1}/{total_images} ({image_id})")
                
                result = self.evaluate_single_image(image_id, image_data)
                self.benchmark_results.append(result)
                
                processed_count += 1
                
                # é€²æ—ãƒ¬ãƒãƒ¼ãƒˆï¼ˆ10ç”»åƒã”ã¨ï¼‰
                if processed_count % 10 == 0:
                    current_accuracy = sum(1 for r in self.benchmark_results if r.largest_char_predicted) / processed_count
                    logger.info(f"ä¸­é–“çµæœ: {processed_count}/{total_images} å®Œäº†, ç¾åœ¨ç²¾åº¦: {current_accuracy:.1%}")
            
            # çµæœé›†è¨ˆ
            summary = self.calculate_summary()
            
            # çµæœä¿å­˜
            self.save_results(summary)
            
            logger.info("Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒˆãƒ©ãƒƒã‚«ãƒ¼æ›´æ–°
            self.tracker.update_task_status("phase0-benchmark", "completed")
            
            return summary
            
        except Exception as e:
            logger.error(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def calculate_summary(self) -> BenchmarkSummary:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœé›†è¨ˆ"""
        if not self.benchmark_results:
            return BenchmarkSummary(0, 0.0, 0.0, 0.0, 0.0, {}, {})
        
        total_images = len(self.benchmark_results)
        
        # æœ€å¤§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ­£è§£ç‡
        largest_char_accuracy = sum(1 for r in self.benchmark_results if r.largest_char_predicted) / total_images
        
        # å¹³å‡IoU
        mean_iou = sum(r.iou_score for r in self.benchmark_results) / total_images
        
        # A/Bè©•ä¾¡ç‡
        ab_count = sum(1 for r in self.benchmark_results if r.quality_grade in ['A', 'B'])
        ab_evaluation_rate = ab_count / total_images
        
        # å¹³å‡å‡¦ç†æ™‚é–“
        mean_processing_time = sum(r.processing_time for r in self.benchmark_results) / total_images
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
        grade_distribution = {}
        for grade in ['A', 'B', 'C', 'D', 'E', 'F']:
            grade_distribution[grade] = sum(1 for r in self.benchmark_results if r.quality_grade == grade)
        
        # å‡¦ç†çµ±è¨ˆ
        processing_times = [r.processing_time for r in self.benchmark_results]
        processing_stats = {
            "mean": mean_processing_time,
            "min": min(processing_times),
            "max": max(processing_times),
            "std": float(np.std(processing_times))
        }
        
        return BenchmarkSummary(
            total_images=total_images,
            largest_char_accuracy=largest_char_accuracy,
            mean_iou=mean_iou,
            ab_evaluation_rate=ab_evaluation_rate,
            mean_processing_time=mean_processing_time,
            grade_distribution=grade_distribution,
            processing_stats=processing_stats
        )
    
    def save_results(self, summary: BenchmarkSummary):
        """çµæœä¿å­˜"""
        try:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            # è©³ç´°çµæœ
            detailed_results = {
                "summary": asdict(summary),
                "detailed_results": [asdict(r) for r in self.benchmark_results],
                "metadata": {
                    "timestamp": timestamp,
                    "total_images": len(self.benchmark_results),
                    "system_info": {
                        "yolo_model": "YOLOv8",
                        "sam_model": "ViT-H",
                        "selection_method": "area_largest"
                    }
                }
            }
            
            # JSONä¿å­˜
            results_file = self.results_dir / f"phase0_benchmark_{timestamp}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, indent=2, ensure_ascii=False)
            
            # æœ€æ–°çµæœã¨ã—ã¦ã‚‚ã‚³ãƒ”ãƒ¼
            latest_file = self.results_dir / "latest_benchmark_results.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜å®Œäº†: {results_file}")
            
        except Exception as e:
            logger.error(f"çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_report(self, summary: BenchmarkSummary) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""
# Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š ç·åˆçµæœ

- **å‡¦ç†ç”»åƒæ•°**: {summary.total_images}æš
- **Largest-Character Accuracy**: {summary.largest_char_accuracy:.1%}
- **å¹³å‡IoU**: {summary.mean_iou:.3f}
- **A/Bè©•ä¾¡ç‡**: {summary.ab_evaluation_rate:.1%}
- **å¹³å‡å‡¦ç†æ™‚é–“**: {summary.mean_processing_time:.2f}ç§’

## ğŸ“ˆ è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ

"""
        for grade, count in summary.grade_distribution.items():
            percentage = (count / summary.total_images) * 100
            report += f"- **{grade}è©•ä¾¡**: {count}æš ({percentage:.1f}%)\n"
        
        report += f"""

## âš¡ å‡¦ç†æ€§èƒ½

- **å¹³å‡å‡¦ç†æ™‚é–“**: {summary.processing_stats['mean']:.2f}ç§’
- **æœ€é€Ÿå‡¦ç†**: {summary.processing_stats['min']:.2f}ç§’
- **æœ€é…å‡¦ç†**: {summary.processing_stats['max']:.2f}ç§’
- **æ¨™æº–åå·®**: {summary.processing_stats['std']:.2f}ç§’

## ğŸ¯ æ”¹å–„å¯¾è±¡ã®ç‰¹å®š

### ä¸»è¦èª²é¡Œ
"""
        # å¤±æ•—ã‚±ãƒ¼ã‚¹åˆ†æ
        failed_cases = [r for r in self.benchmark_results if not r.largest_char_predicted]
        if failed_cases:
            report += f"- **å¤±æ•—ç”»åƒæ•°**: {len(failed_cases)}æš ({len(failed_cases)/summary.total_images:.1%})\n"
            
            # ä½IoUã‚±ãƒ¼ã‚¹
            low_iou_cases = [r for r in failed_cases if r.iou_score < 0.3]
            report += f"- **æ¥µä½IoU (<0.3)**: {len(low_iou_cases)}æš\n"
            
            # ä½ä¿¡é ¼åº¦ã‚±ãƒ¼ã‚¹
            low_conf_cases = [r for r in failed_cases if r.confidence_score < 0.5]
            report += f"- **ä½ä¿¡é ¼åº¦ (<0.5)**: {len(low_conf_cases)}æš\n"
        
        report += f"""

## ğŸ“‹ æ¬¡ã®Phaseã¸ã®æè¨€

### Phase 1ã§ã®æ³¨åŠ›ç‚¹
- IoU < 0.5ã®å¤±æ•—ã‚±ãƒ¼ã‚¹ {len(failed_cases)}æšã®è©³ç´°åˆ†æ
- ã‚³ãƒæ¤œå‡ºç²¾åº¦å‘ä¸Šã«ã‚ˆã‚‹å‰å‡¦ç†æ”¹å–„
- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã«ã‚ˆã‚‹å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å¢—å¼·

### ç›®æ¨™è¨­å®š
- **Phase 1ç›®æ¨™**: Largest-Character Accuracy 75%ä»¥ä¸Š
- **Phaseçµ‚äº†ç›®æ¨™**: A/Bè©•ä¾¡ç‡ 70%ä»¥ä¸Šé”æˆ

---
*ç”Ÿæˆæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
    project_root = Path("/mnt/c/AItools/segment-anything")
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmark = Phase0Benchmark(project_root)
    
    try:
        logger.info("=== Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹ ===")
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        summary = benchmark.run_full_benchmark()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»è¡¨ç¤º
        report = benchmark.generate_report(summary)
        print(report)
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_file = benchmark.results_dir / f"phase0_report_{time.strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_file}")
        logger.info("=== Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† ===")
        
    except Exception as e:
        logger.error(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())