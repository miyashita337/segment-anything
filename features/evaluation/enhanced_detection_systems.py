#!/usr/bin/env python3
"""
å¼·åŒ–æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ  (Phase A2)
æœ€é«˜å“è³ªã®é¡”æ¤œå‡ºãƒ»ãƒãƒ¼ã‚ºæ¤œå‡ºã‚’å®Ÿç¾

å“è³ªé‡è¦–ã®è¨­è¨ˆæ–¹é‡:
- å‡¦ç†æ™‚é–“: 10-12ç§’/ç”»åƒï¼ˆå“è³ªæœ€å„ªå…ˆï¼‰
- é¡”æ¤œå‡ºç‡: 90%ä»¥ä¸Š
- ãƒãƒ¼ã‚ºæ¤œå‡ºç‡: 80%ä»¥ä¸Š
- è¤‡æ•°æ‰‹æ³•çµ±åˆã«ã‚ˆã‚‹é«˜ç²¾åº¦å®Ÿç¾
"""

import numpy as np
import cv2

import logging
import os
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from PIL import Image
from typing import Dict, List, NamedTuple, Optional, Tuple

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from features.evaluation.anime_image_preprocessor import AnimeImagePreprocessor

# MediaPipeã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
try:
    import mediapipe as mp

    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    logging.warning("MediaPipeãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - OpenCVã®ã¿ã§å‹•ä½œ")

# dlibã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
try:
    import dlib

    DLIB_AVAILABLE = True
except ImportError:
    DLIB_AVAILABLE = False
    logging.warning("dlibãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - MediaPipe+OpenCVã§å‹•ä½œ")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class FaceDetection:
    """é¡”æ¤œå‡ºçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    bbox: Tuple[int, int, int, int]  # (x, y, width, height)
    confidence: float
    method: str  # "mediapipe", "opencv", "dlib"
    landmarks: Optional[np.ndarray] = None


@dataclass
class PoseDetectionResult:
    """ãƒãƒ¼ã‚ºæ¤œå‡ºçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    detected: bool
    landmarks: Optional[np.ndarray]
    visibility_score: float
    pose_category: str
    completeness_score: float
    confidence: float
    keypoints_detected: int


@dataclass
class EnhancedDetectionReport:
    """å¼·åŒ–æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ"""

    face_detections: List[FaceDetection]
    pose_result: PoseDetectionResult
    overall_confidence: float
    processing_time: float
    detection_summary: Dict[str, float]


class EnhancedFaceDetector:
    """æœ€é«˜å“è³ªå¤šæ‰‹æ³•çµ±åˆé¡”æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.EnhancedFaceDetector")

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹
        project_root = Path(__file__).parent.parent.parent

        # ã‚¢ãƒ‹ãƒ¡ç”»åƒå‰å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
        self.preprocessor = AnimeImagePreprocessor()

        # MediaPipeé¡”æ¤œå‡ºåˆæœŸåŒ–
        self.mediapipe_detector = None
        if MEDIAPIPE_AVAILABLE:
            try:
                self.mediapipe_detector = mp.solutions.face_detection.FaceDetection(
                    model_selection=1, min_detection_confidence=0.1  # é è·é›¢ãƒ¢ãƒ‡ãƒ«ï¼ˆé«˜ç²¾åº¦ï¼‰  # é–¾å€¤ã‚’å¤§å¹…ã«ä¸‹ã’ã¦æ¤œå‡ºç‡å‘ä¸Š
                )
                self.logger.info("MediaPipeé¡”æ¤œå‡ºåˆæœŸåŒ–å®Œäº†")
            except Exception as e:
                self.logger.warning(f"MediaPipeåˆæœŸåŒ–å¤±æ•—: {e}")

        # OpenCV Cascadeåˆ†é¡å™¨åˆæœŸåŒ–
        self.cascade_detectors = {}
        cascade_files = {
            "frontal": "haarcascade_frontalface_default.xml",
            "profile": "haarcascade_profileface.xml",
            "anime": "lbpcascade_animeface.xml",  # ã‚¢ãƒ‹ãƒ¡é¡”ç”¨å°‚ç”¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰
        }

        for name, filename in cascade_files.items():
            try:
                if name == "anime":
                    # ã‚¢ãƒ‹ãƒ¡é¡”å°‚ç”¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®ãƒ‘ã‚¹
                    cascade_path = str(project_root / "models" / "cascades" / filename)
                else:
                    # æ¨™æº–OpenCVã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®ãƒ‘ã‚¹
                    cascade_path = cv2.data.haarcascades + filename

                if os.path.exists(cascade_path):
                    self.cascade_detectors[name] = cv2.CascadeClassifier(cascade_path)
                    self.logger.info(f"OpenCV {name} CascadeåˆæœŸåŒ–å®Œäº†")
                else:
                    self.logger.warning(f"Cascadeãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹: {cascade_path}")
            except Exception as e:
                self.logger.warning(f"OpenCV {name} CascadeåˆæœŸåŒ–å¤±æ•—: {e}")

        # dlibé¡”æ¤œå‡ºå™¨åˆæœŸåŒ–
        self.dlib_detector = None
        if DLIB_AVAILABLE:
            try:
                self.dlib_detector = dlib.get_frontal_face_detector()
                self.logger.info("dlibé¡”æ¤œå‡ºå™¨åˆæœŸåŒ–å®Œäº†")
            except Exception as e:
                self.logger.warning(f"dlibåˆæœŸåŒ–å¤±æ•—: {e}")

    def detect_faces_comprehensive(
        self, image: np.ndarray, efficient_mode: bool = False, target_detection_rate: float = 0.90
    ) -> List[FaceDetection]:
        """åŒ…æ‹¬çš„å¤šæ‰‹æ³•é¡”æ¤œå‡ºï¼ˆå‰å‡¦ç†çµ±åˆç‰ˆï¼‰"""
        start_time = datetime.now()
        all_detections = []

        # 0. ã‚¢ãƒ‹ãƒ¡ç”»åƒå‰å‡¦ç†é©ç”¨ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
        enhanced_image = self.preprocessor.enhance_for_face_detection(
            image, lightweight_mode=efficient_mode
        )

        if efficient_mode:
            # åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰: æ®µéšçš„æ¤œå‡ºï¼ˆç›®æ¨™é”æˆã§æ—©æœŸçµ‚äº†ï¼‰
            return self._efficient_detection_pipeline(enhanced_image, image, target_detection_rate)
        else:
            # é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰: å…¨æ‰‹æ³•å®Ÿè¡Œï¼ˆå¾“æ¥ç‰ˆï¼‰
            return self._full_detection_pipeline(enhanced_image, image)

    def _efficient_detection_pipeline(
        self, enhanced_image: np.ndarray, original_image: np.ndarray, target_rate: float
    ) -> List[FaceDetection]:
        """åŠ¹ç‡çš„æ¤œå‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆæ®µéšçš„å®Ÿè¡Œãƒ»æ—©æœŸçµ‚äº†ï¼‰"""
        start_time = datetime.now()
        all_detections = []
        detection_methods = []

        # å„ªå…ˆåº¦1: MediaPipeï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
        if self.mediapipe_detector:
            mp_detections = self._detect_mediapipe(enhanced_image)
            all_detections.extend(mp_detections)
            detection_methods.append(f"MediaPipe: {len(mp_detections)}ä»¶")

            # ååˆ†ãªæ¤œå‡ºãŒã‚ã‚Œã°æ—©æœŸçµ‚äº†ã‚’æ¤œè¨
            if len(mp_detections) >= 1:  # 1ä»¶ä»¥ä¸Šæ¤œå‡ºãŒã‚ã‚Œã°æ¬¡ã®æ®µéšã¸
                merged = self._merge_detections(all_detections)
                if self._estimate_detection_quality(merged) >= target_rate:
                    self.logger.info(f"æ—©æœŸçµ‚äº†: MediaPipeã§ç›®æ¨™é”æˆ ({len(merged)}ä»¶)")
                    return merged

        # å„ªå…ˆåº¦2: ã‚¢ãƒ‹ãƒ¡å°‚ç”¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ï¼ˆã‚¢ãƒ‹ãƒ¡ç‰¹åŒ–ï¼‰
        if "anime" in self.cascade_detectors:
            anime_detections = self._detect_opencv_cascade(
                enhanced_image, self.cascade_detectors["anime"], "anime"
            )
            all_detections.extend(anime_detections)
            detection_methods.append(f"Anime Cascade: {len(anime_detections)}ä»¶")

            merged = self._merge_detections(all_detections)
            if self._estimate_detection_quality(merged) >= target_rate:
                self.logger.info(f"æ—©æœŸçµ‚äº†: Anime Cascadeã§ç›®æ¨™é”æˆ ({len(merged)}ä»¶)")
                return merged

        # å„ªå…ˆåº¦3: ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡ºï¼ˆ3ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ï¼‰
        if "anime" in self.cascade_detectors:
            multi_scale_detections = self._detect_multi_scale_anime_efficient(original_image)
            all_detections.extend(multi_scale_detections)
            detection_methods.append(f"Multi-Scale: {len(multi_scale_detections)}ä»¶")

            merged = self._merge_detections(all_detections)
            if self._estimate_detection_quality(merged) >= target_rate:
                self.logger.info(f"æ—©æœŸçµ‚äº†: Multi-Scaleã§ç›®æ¨™é”æˆ ({len(merged)}ä»¶)")
                return merged

        # å„ªå…ˆåº¦4: dlibï¼ˆè£œå®Œç”¨ï¼‰
        if self.dlib_detector:
            dlib_detections = self._detect_dlib(enhanced_image)
            all_detections.extend(dlib_detections)
            detection_methods.append(f"dlib: {len(dlib_detections)}ä»¶")

        # æœ€çµ‚çµ±åˆ
        merged_detections = self._merge_detections(all_detections)

        processing_time = (datetime.now() - start_time).total_seconds()
        self.logger.info(
            f"åŠ¹ç‡çš„é¡”æ¤œå‡ºå®Œäº†: {len(merged_detections)}ä»¶æ¤œå‡º "
            f"ï¼ˆ{', '.join(detection_methods)}ï¼‰"
            f"ï¼ˆå‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ï¼‰"
        )

        return merged_detections

    def _full_detection_pipeline(
        self, enhanced_image: np.ndarray, original_image: np.ndarray
    ) -> List[FaceDetection]:
        """é«˜å“è³ªæ¤œå‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå…¨æ‰‹æ³•å®Ÿè¡Œï¼‰"""
        start_time = datetime.now()
        all_detections = []

        # 1. MediaPipeæ¤œå‡ºï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
        if self.mediapipe_detector:
            mp_detections = self._detect_mediapipe(enhanced_image)
            all_detections.extend(mp_detections)
            self.logger.debug(f"MediaPipeæ¤œå‡º: {len(mp_detections)}ä»¶")

        # 2. OpenCV Cascadeæ¤œå‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        for detector_name, detector in self.cascade_detectors.items():
            cv_detections = self._detect_opencv_cascade(enhanced_image, detector, detector_name)
            all_detections.extend(cv_detections)
            self.logger.debug(f"OpenCV {detector_name}æ¤œå‡º: {len(cv_detections)}ä»¶")

        # 3. dlibæ¤œå‡ºï¼ˆè£œå®Œç”¨ï¼‰
        if self.dlib_detector:
            dlib_detections = self._detect_dlib(enhanced_image)
            all_detections.extend(dlib_detections)
            self.logger.debug(f"dlibæ¤œå‡º: {len(dlib_detections)}ä»¶")

        # 4. ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡ºï¼ˆå…¨5ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        if "anime" in self.cascade_detectors:
            multi_scale_detections = self._detect_multi_scale_anime(original_image)
            all_detections.extend(multi_scale_detections)
            self.logger.debug(f"ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡º: {len(multi_scale_detections)}ä»¶")

        # 5. é‡è¤‡é™¤å»ãƒ»çµ±åˆ
        merged_detections = self._merge_detections(all_detections)

        processing_time = (datetime.now() - start_time).total_seconds()
        self.logger.info(
            f"é«˜å“è³ªé¡”æ¤œå‡ºå®Œäº†: {len(merged_detections)}ä»¶æ¤œå‡º " f"ï¼ˆå‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ï¼‰"
        )

        return merged_detections

    def _estimate_detection_quality(self, detections: List[FaceDetection]) -> float:
        """æ¤œå‡ºå“è³ªã®æ¨å®šï¼ˆæ—©æœŸçµ‚äº†åˆ¤å®šç”¨ï¼‰"""
        if not detections:
            return 0.0

        # ä¿¡é ¼åº¦ã®é‡ã¿ä»˜ãå¹³å‡ã§å“è³ªæ¨å®š
        total_confidence = sum(det.confidence for det in detections)
        avg_confidence = total_confidence / len(detections)

        # æ¤œå‡ºæ•°ã«ã‚ˆã‚‹è£œæ­£ï¼ˆ1ä»¶=0.8, 2ä»¶ä»¥ä¸Š=1.0ï¼‰
        count_factor = min(1.0, len(detections) * 0.8)

        return avg_confidence * count_factor

    def _detect_multi_scale_anime_efficient(self, image: np.ndarray) -> List[FaceDetection]:
        """è»½é‡ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒ‹ãƒ¡é¡”æ¤œå‡ºï¼ˆ3ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ï¼‰"""
        detections = []

        if "anime" not in self.cascade_detectors:
            return detections

        try:
            # è»½é‡ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒç”Ÿæˆï¼ˆ3ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ï¼‰
            multi_scale_images = self.preprocessor.create_multi_scale_versions(
                image, lightweight_mode=True
            )

            anime_detector = self.cascade_detectors["anime"]

            for scale_data in multi_scale_images:
                scale = scale_data["scale"]
                scaled_image = scale_data["image"]

                # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
                gray = cv2.cvtColor(scaled_image, cv2.COLOR_BGR2GRAY)

                # ã‚¢ãƒ‹ãƒ¡é¡”ç‰¹åŒ–æ¤œå‡º
                faces = anime_detector.detectMultiScale(
                    gray,
                    scaleFactor=1.02,
                    minNeighbors=1,
                    minSize=(8, 8),
                    maxSize=(600, 600),
                    flags=cv2.CASCADE_SCALE_IMAGE,
                )

                # å…ƒç”»åƒåº§æ¨™ç³»ã«å¤‰æ›
                for x, y, w, h in faces:
                    orig_x = int(x / scale)
                    orig_y = int(y / scale)
                    orig_w = int(w / scale)
                    orig_h = int(h / scale)

                    # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                    orig_x = max(0, orig_x)
                    orig_y = max(0, orig_y)
                    orig_w = min(orig_w, image.shape[1] - orig_x)
                    orig_h = min(orig_h, image.shape[0] - orig_y)

                    scale_confidence = self._calculate_scale_confidence(scale, w, h)

                    detections.append(
                        FaceDetection(
                            bbox=(orig_x, orig_y, orig_w, orig_h),
                            confidence=0.90 * scale_confidence,
                            method=f"anime_efficient_{scale:.2f}",
                        )
                    )

        except Exception as e:
            self.logger.warning(f"è»½é‡ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return detections

    def _detect_mediapipe(self, image: np.ndarray) -> List[FaceDetection]:
        """MediaPipeé¡”æ¤œå‡º"""
        detections = []
        try:
            # RGBå¤‰æ›ï¼ˆMediaPipeè¦ä»¶ï¼‰
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = self.mediapipe_detector.process(rgb_image)

            if results.detections:
                h, w = image.shape[:2]
                for detection in results.detections:
                    bbox_data = detection.location_data.relative_bounding_box

                    # ç›¸å¯¾åº§æ¨™ã‚’çµ¶å¯¾åº§æ¨™ã«å¤‰æ›
                    x = int(bbox_data.xmin * w)
                    y = int(bbox_data.ymin * h)
                    width = int(bbox_data.width * w)
                    height = int(bbox_data.height * h)

                    # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                    x = max(0, x)
                    y = max(0, y)
                    width = min(width, w - x)
                    height = min(height, h - y)

                    confidence = detection.score[0] if detection.score else 0.5

                    detections.append(
                        FaceDetection(
                            bbox=(x, y, width, height), confidence=confidence, method="mediapipe"
                        )
                    )

        except Exception as e:
            self.logger.warning(f"MediaPipeæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return detections

    def _detect_opencv_cascade(
        self, image: np.ndarray, detector: cv2.CascadeClassifier, detector_name: str
    ) -> List[FaceDetection]:
        """OpenCV Cascadeæ¤œå‡º"""
        detections = []
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

            # ã‚¢ãƒ‹ãƒ¡é¡”ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            if detector_name == "anime":
                faces = detector.detectMultiScale(
                    gray,
                    scaleFactor=1.03,  # ã‚¢ãƒ‹ãƒ¡é¡”ç”¨ã«ã‚ˆã‚Šç´°ã‹ã„ã‚¹ã‚±ãƒ¼ãƒ«
                    minNeighbors=1,  # ã‚¢ãƒ‹ãƒ¡é¡”ç”¨æ¥µå¯›å®¹è¨­å®š
                    minSize=(10, 10),  # éå¸¸ã«å°ã•ãªé¡”ã‚‚æ¤œå‡º
                    maxSize=(500, 500),  # å¤§ããªé¡”ã‚‚å¯¾å¿œ
                )
            else:
                faces = detector.detectMultiScale(
                    gray,
                    scaleFactor=1.05,  # ã‚ˆã‚Šç´°ã‹ã„ã‚¹ã‚±ãƒ¼ãƒ«ã§æ¤œå‡ºç‡å‘ä¸Š
                    minNeighbors=2,  # ã‚ˆã‚Šå¯›å®¹ãªè¨­å®šã§æ¤œå‡ºç‡å‘ä¸Š
                    minSize=(20, 20),  # ã‚ˆã‚Šå°ã•ãªé¡”ã‚‚æ¤œå‡º
                )

            for x, y, w, h in faces:
                # ä¿¡é ¼åº¦ã¯æ¤œå‡ºã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã§æ¨å®š
                face_area = w * h
                image_area = image.shape[0] * image.shape[1]
                size_confidence = min(1.0, face_area / (image_area * 0.01))

                # ã‚¢ãƒ‹ãƒ¡é¡”å°‚ç”¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®å ´åˆã¯ä¿¡é ¼åº¦ã‚’é«˜ãè¨­å®š
                if detector_name == "anime":
                    base_confidence = 0.85  # ã‚¢ãƒ‹ãƒ¡é¡”å°‚ç”¨ãªã®ã§é«˜ä¿¡é ¼åº¦
                else:
                    base_confidence = 0.7  # æ¨™æº–ä¿¡é ¼åº¦

                detections.append(
                    FaceDetection(
                        bbox=(x, y, w, h),
                        confidence=base_confidence * size_confidence,
                        method=f"opencv_{detector_name}",
                    )
                )

        except Exception as e:
            self.logger.warning(f"OpenCV {detector_name}æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return detections

    def _detect_dlib(self, image: np.ndarray) -> List[FaceDetection]:
        """dlibé¡”æ¤œå‡º"""
        detections = []
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            faces = self.dlib_detector(gray)

            for face in faces:
                x = face.left()
                y = face.top()
                w = face.width()
                h = face.height()

                # dlib confidence estimation based on detection quality
                confidence = 0.8  # dlibã®åŸºæœ¬ä¿¡é ¼åº¦

                detections.append(
                    FaceDetection(bbox=(x, y, w, h), confidence=confidence, method="dlib")
                )

        except Exception as e:
            self.logger.warning(f"dlibæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return detections

    def _detect_multi_scale_anime(self, image: np.ndarray) -> List[FaceDetection]:
        """ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒ‹ãƒ¡é¡”æ¤œå‡ºï¼ˆè¤‡æ•°è§£åƒåº¦ã§ã®æ¤œå‡ºçµ±åˆï¼‰"""
        detections = []

        if "anime" not in self.cascade_detectors:
            return detections

        try:
            # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒç”Ÿæˆ
            multi_scale_images = self.preprocessor.create_multi_scale_versions(image)

            anime_detector = self.cascade_detectors["anime"]

            for scale_data in multi_scale_images:
                scale = scale_data["scale"]
                scaled_image = scale_data["image"]

                # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
                gray = cv2.cvtColor(scaled_image, cv2.COLOR_BGR2GRAY)

                # ã‚¢ãƒ‹ãƒ¡é¡”ç‰¹åŒ–æ¤œå‡º
                faces = anime_detector.detectMultiScale(
                    gray,
                    scaleFactor=1.02,  # ã‚ˆã‚Šç´°ã‹ã„ã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡º
                    minNeighbors=1,  # æœ€å°è¿‘å‚ï¼ˆæœ€å¯›å®¹è¨­å®šï¼‰
                    minSize=(8, 8),  # æ¥µå°é¡”ã‚‚æ¤œå‡º
                    maxSize=(600, 600),  # å¤§ããªé¡”ã‚‚å¯¾å¿œ
                    flags=cv2.CASCADE_SCALE_IMAGE,
                )

                # å…ƒç”»åƒåº§æ¨™ç³»ã«å¤‰æ›
                for x, y, w, h in faces:
                    # ã‚¹ã‚±ãƒ¼ãƒ«é€†å¤‰æ›
                    orig_x = int(x / scale)
                    orig_y = int(y / scale)
                    orig_w = int(w / scale)
                    orig_h = int(h / scale)

                    # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                    orig_x = max(0, orig_x)
                    orig_y = max(0, orig_y)
                    orig_w = min(orig_w, image.shape[1] - orig_x)
                    orig_h = min(orig_h, image.shape[0] - orig_y)

                    # ã‚¹ã‚±ãƒ¼ãƒ«åˆ¥ä¿¡é ¼åº¦è¨ˆç®—
                    scale_confidence = self._calculate_scale_confidence(scale, w, h)

                    detections.append(
                        FaceDetection(
                            bbox=(orig_x, orig_y, orig_w, orig_h),
                            confidence=0.90 * scale_confidence,  # ã‚¢ãƒ‹ãƒ¡é¡”å°‚ç”¨ãªã®ã§é«˜ä¿¡é ¼åº¦
                            method=f"anime_multiscale_{scale:.2f}",
                        )
                    )

                    self.logger.debug(
                        f"ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡º: ã‚¹ã‚±ãƒ¼ãƒ«{scale:.2f}, åº§æ¨™({orig_x},{orig_y},{orig_w},{orig_h})"
                    )

        except Exception as e:
            self.logger.warning(f"ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return detections

    def _calculate_scale_confidence(self, scale: float, width: int, height: int) -> float:
        """ã‚¹ã‚±ãƒ¼ãƒ«åˆ¥ä¿¡é ¼åº¦è¨ˆç®—"""
        # é¡”ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
        face_area = width * height

        # æœ€é©ã‚¹ã‚±ãƒ¼ãƒ«ç¯„å›²ï¼ˆ0.75-1.25å€ãŒæœ€ã‚‚ä¿¡é ¼æ€§é«˜ã„ï¼‰
        if 0.75 <= scale <= 1.25:
            scale_factor = 1.0
        elif 0.5 <= scale < 0.75 or 1.25 < scale <= 1.5:
            scale_factor = 0.9
        else:
            scale_factor = 0.8

        # é¡”ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ä¿¡é ¼åº¦ï¼ˆä¸­ç¨‹åº¦ã‚µã‚¤ã‚ºãŒæœ€é©ï¼‰
        if 30 * 30 <= face_area <= 200 * 200:
            size_factor = 1.0
        elif 15 * 15 <= face_area < 30 * 30 or 200 * 200 < face_area <= 400 * 400:
            size_factor = 0.85
        else:
            size_factor = 0.7

        return scale_factor * size_factor

    def _merge_detections(self, detections: List[FaceDetection]) -> List[FaceDetection]:
        """é‡è¤‡é™¤å»ãƒ»æ¤œå‡ºçµæœçµ±åˆ"""
        if not detections:
            return []

        # IoUï¼ˆIntersection over Unionï¼‰ã«ã‚ˆã‚‹é‡è¤‡é™¤å»
        merged = []

        for detection in sorted(detections, key=lambda d: d.confidence, reverse=True):
            is_duplicate = False

            for existing in merged:
                iou = self._calculate_bbox_iou(detection.bbox, existing.bbox)
                if iou > 0.5:  # 50%ä»¥ä¸Šé‡è¤‡ã¯åŒä¸€é¡”ã¨ã¿ãªã™
                    # ã‚ˆã‚Šé«˜ã„ä¿¡é ¼åº¦ã®æ¤œå‡ºã‚’ä¿æŒ
                    if detection.confidence > existing.confidence:
                        merged.remove(existing)
                        merged.append(detection)
                    is_duplicate = True
                    break

            if not is_duplicate:
                merged.append(detection)

        return merged

    def _calculate_bbox_iou(
        self, bbox1: Tuple[int, int, int, int], bbox2: Tuple[int, int, int, int]
    ) -> float:
        """å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã®IoUè¨ˆç®—"""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2

        # äº¤å·®é ˜åŸŸè¨ˆç®—
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)

        if x_right < x_left or y_bottom < y_top:
            return 0.0

        intersection = (x_right - x_left) * (y_bottom - y_top)
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection

        return intersection / union if union > 0 else 0.0


class EnhancedPoseDetector:
    """åŒ…æ‹¬çš„æœ€é«˜å“è³ªãƒãƒ¼ã‚ºæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.EnhancedPoseDetector")

        # MediaPipe PoseåˆæœŸåŒ–ï¼ˆWeek 2æœ€é©åŒ–è¨­å®šï¼‰
        self.mediapipe_pose_high = None
        self.mediapipe_pose_fast = None
        if MEDIAPIPE_AVAILABLE:
            try:
                # é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ï¼ˆè¤‡é›‘ãªå§¿å‹¢ç”¨ï¼‰
                self.mediapipe_pose_high = mp.solutions.pose.Pose(
                    static_image_mode=True,
                    model_complexity=2,  # æœ€é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«
                    enable_segmentation=False,  # Week 2æœ€é©åŒ–: ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç„¡åŠ¹
                    min_detection_confidence=0.05,  # Week 2æœ€é©åŒ–: 0.1â†’0.05ã«ç·©å’Œ
                    min_tracking_confidence=0.05,
                )

                # é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼ˆé«˜é€Ÿå‡¦ç†ç”¨ï¼‰
                self.mediapipe_pose_fast = mp.solutions.pose.Pose(
                    static_image_mode=True,
                    model_complexity=1,  # Week 2æœ€é©åŒ–: è»½é‡é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ä½µç”¨
                    enable_segmentation=False,  # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç„¡åŠ¹
                    min_detection_confidence=0.05,
                    min_tracking_confidence=0.05,
                )

                self.logger.info("MediaPipe Poseï¼ˆWeek 2æœ€é©åŒ–ï¼šä½µç”¨ãƒ¢ãƒ‡ãƒ«ï¼‰åˆæœŸåŒ–å®Œäº†")
            except Exception as e:
                self.logger.warning(f"MediaPipe PoseåˆæœŸåŒ–å¤±æ•—: {e}")

        # ãƒãƒ¼ã‚ºã‚«ãƒ†ã‚´ãƒªåˆ†é¡ç”¨ã®è¨­å®š
        self.pose_categories = {
            "standing": "ç«‹ä½",
            "sitting": "åº§ä½",
            "lying": "æ¨ªè‡¥ä½",
            "action": "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
            "profile": "æ¨ªå‘ã",
            "unknown": "ä¸æ˜",
        }

    def detect_pose_comprehensive(
        self, image: np.ndarray, efficient_mode: bool = False
    ) -> PoseDetectionResult:
        """åŒ…æ‹¬çš„é«˜å“è³ªãƒãƒ¼ã‚ºæ¤œå‡ºï¼ˆWeek 2æœ€é©åŒ–ï¼šä½µç”¨ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰"""
        start_time = datetime.now()

        if not self.mediapipe_pose_high and not self.mediapipe_pose_fast:
            return PoseDetectionResult(
                detected=False,
                landmarks=None,
                visibility_score=0.0,
                pose_category="unknown",
                completeness_score=0.0,
                confidence=0.0,
                keypoints_detected=0,
            )

        try:
            # RGBå¤‰æ›ï¼ˆMediaPipeè¦ä»¶ï¼‰
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Week 2æœ€é©åŒ–: ä½µç”¨ãƒ¢ãƒ‡ãƒ«æˆ¦ç•¥
            results = None
            model_used = "none"

            if efficient_mode and self.mediapipe_pose_fast:
                # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: è»½é‡ãƒ¢ãƒ‡ãƒ«å„ªå…ˆ
                results = self.mediapipe_pose_fast.process(rgb_image)
                model_used = "fast"

                # é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã§æ¤œå‡ºã§ããªã„å ´åˆã¯é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
                if not results.pose_landmarks and self.mediapipe_pose_high:
                    results = self.mediapipe_pose_high.process(rgb_image)
                    model_used = "high_fallback"
            else:
                # é«˜ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«å„ªå…ˆ
                if self.mediapipe_pose_high:
                    results = self.mediapipe_pose_high.process(rgb_image)
                    model_used = "high"
                elif self.mediapipe_pose_fast:
                    results = self.mediapipe_pose_fast.process(rgb_image)
                    model_used = "fast_fallback"

            if not results or not results.pose_landmarks:
                self.logger.debug(f"ãƒãƒ¼ã‚ºãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æœªæ¤œå‡º (model: {model_used})")
                return PoseDetectionResult(
                    detected=False,
                    landmarks=None,
                    visibility_score=0.0,
                    pose_category="unknown",
                    completeness_score=0.0,
                    confidence=0.0,
                    keypoints_detected=0,
                )

            # Week 2æœ€é©åŒ–: éƒ¨åˆ†ãƒãƒ¼ã‚ºå¯¾å¿œãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åˆ†æ
            landmarks = results.pose_landmarks
            visibility_score = self._calculate_visibility_score_optimized(landmarks)
            pose_category = self._classify_pose_anime_optimized(landmarks)
            completeness_score = self._evaluate_partial_pose_completeness(landmarks)
            keypoints_detected = self._count_visible_keypoints_optimized(landmarks)

            # Week 2æœ€é©åŒ–: éƒ¨åˆ†ãƒãƒ¼ã‚ºã§ã‚‚é«˜ä¿¡é ¼åº¦ã‚’ç¶­æŒ
            confidence = self._calculate_optimized_confidence(
                visibility_score, completeness_score, keypoints_detected
            )

            processing_time = (datetime.now() - start_time).total_seconds()
            self.logger.info(
                f"ãƒãƒ¼ã‚ºæ¤œå‡ºå®Œäº†: ã‚«ãƒ†ã‚´ãƒª={pose_category}, "
                f"å¯è¦–æ€§={visibility_score:.3f}, "
                f"å®Œå…¨æ€§={completeness_score:.3f}, "
                f"ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ={keypoints_detected}, "
                f"ãƒ¢ãƒ‡ãƒ«={model_used} "
                f"ï¼ˆå‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ï¼‰"
            )

            return PoseDetectionResult(
                detected=True,
                landmarks=landmarks,
                visibility_score=visibility_score,
                pose_category=pose_category,
                completeness_score=completeness_score,
                confidence=confidence,
                keypoints_detected=keypoints_detected,
            )

        except Exception as e:
            self.logger.error(f"ãƒãƒ¼ã‚ºæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return PoseDetectionResult(
                detected=False,
                landmarks=None,
                visibility_score=0.0,
                pose_category="unknown",
                completeness_score=0.0,
                confidence=0.0,
                keypoints_detected=0,
            )

    def _calculate_visibility_score(self, landmarks) -> float:
        """ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå¯è¦–æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not landmarks or not landmarks.landmark:
            return 0.0

        visible_count = 0
        total_count = len(landmarks.landmark)

        for landmark in landmarks.landmark:
            # MediaPipeã®å¯è¦–æ€§é–¾å€¤ï¼ˆ0.3ä»¥ä¸Šã‚’å¯è¦–ã¨ã¿ãªã™ - ç·©å’Œï¼‰
            if landmark.visibility > 0.3:
                visible_count += 1

        return visible_count / total_count if total_count > 0 else 0.0

    def _classify_pose(self, landmarks) -> str:
        """ãƒãƒ¼ã‚ºã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        if not landmarks or not landmarks.landmark:
            return "unknown"

        try:
            # ä¸»è¦ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆMediaPipe Poseï¼‰
            left_shoulder = landmarks.landmark[11]
            right_shoulder = landmarks.landmark[12]
            left_hip = landmarks.landmark[23]
            right_hip = landmarks.landmark[24]
            left_knee = landmarks.landmark[25]
            right_knee = landmarks.landmark[26]
            left_ankle = landmarks.landmark[27]
            right_ankle = landmarks.landmark[28]

            # è‚©ã¨è…°ã®ä¸­ç‚¹è¨ˆç®—
            shoulder_y = (left_shoulder.y + right_shoulder.y) / 2
            hip_y = (left_hip.y + right_hip.y) / 2
            knee_y = (left_knee.y + right_knee.y) / 2
            ankle_y = (left_ankle.y + right_ankle.y) / 2

            # å§¿å‹¢åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            torso_angle = abs(shoulder_y - hip_y)
            leg_bend = (
                abs(hip_y - knee_y) / abs(knee_y - ankle_y) if abs(knee_y - ankle_y) > 0.01 else 1.0
            )

            # ç«‹ä½åˆ¤å®š
            if torso_angle > 0.2 and leg_bend > 0.8:
                return "standing"

            # åº§ä½åˆ¤å®š
            elif torso_angle > 0.15 and leg_bend < 0.6:
                return "sitting"

            # æ¨ªè‡¥ä½åˆ¤å®š
            elif torso_angle < 0.1:
                return "lying"

            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¤å®šï¼ˆè¤‡é›‘ãªãƒãƒ¼ã‚ºï¼‰
            elif leg_bend < 0.4 or torso_angle > 0.3:
                return "action"

            else:
                return "unknown"

        except Exception as e:
            self.logger.warning(f"ãƒãƒ¼ã‚ºåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            return "unknown"

    def _evaluate_pose_completeness(self, landmarks) -> float:
        """ãƒãƒ¼ã‚ºå®Œå…¨æ€§è©•ä¾¡"""
        if not landmarks or not landmarks.landmark:
            return 0.0

        # é‡è¦ãªèº«ä½“éƒ¨ä½ã®é‡ã¿
        body_parts_weights = {
            # é ­éƒ¨ (20%)
            "head": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
            # ä¸ŠåŠèº« (30%)
            "upper_body": [11, 12, 13, 14, 15, 16],
            # è…°éƒ¨ (20%)
            "torso": [23, 24],
            # ä¸‹åŠèº« (30%)
            "lower_body": [25, 26, 27, 28, 29, 30, 31, 32],
        }

        weights = {"head": 0.2, "upper_body": 0.3, "torso": 0.2, "lower_body": 0.3}

        total_score = 0.0

        for part_name, indices in body_parts_weights.items():
            part_visibility = 0.0
            valid_count = 0

            for idx in indices:
                if idx < len(landmarks.landmark):
                    landmark = landmarks.landmark[idx]
                    if landmark.visibility > 0.3:  # å¯è¦–æ€§é–¾å€¤
                        part_visibility += landmark.visibility
                        valid_count += 1

            if valid_count > 0:
                part_score = part_visibility / valid_count
                total_score += part_score * weights[part_name]

        return min(1.0, total_score)

    def _count_visible_keypoints(self, landmarks) -> int:
        """å¯è¦–ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        if not landmarks or not landmarks.landmark:
            return 0

        visible_count = 0
        for landmark in landmarks.landmark:
            if landmark.visibility > 0.3:  # å¯è¦–æ€§é–¾å€¤ã‚’ç·©å’Œ
                visible_count += 1

        return visible_count

    def _calculate_visibility_score_optimized(self, landmarks) -> float:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå¯è¦–æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆWeek 2ï¼šéƒ¨åˆ†ãƒãƒ¼ã‚ºå¯¾å¿œï¼‰"""
        if not landmarks or not landmarks.landmark:
            return 0.0

        visible_count = 0
        total_count = len(landmarks.landmark)

        for landmark in landmarks.landmark:
            # Week 2æœ€é©åŒ–: å¯è¦–æ€§é–¾å€¤ã‚’ã•ã‚‰ã«ç·©å’Œï¼ˆ0.3â†’0.2ï¼‰
            if landmark.visibility > 0.2:
                visible_count += 1

        return visible_count / total_count if total_count > 0 else 0.0

    def _classify_pose_anime_optimized(self, landmarks) -> str:
        """ã‚¢ãƒ‹ãƒ¡ç‰¹åŒ–ãƒãƒ¼ã‚ºåˆ†é¡ï¼ˆWeek 2æœ€é©åŒ–ï¼šéƒ¨åˆ†ãƒãƒ¼ã‚ºå¯¾å¿œï¼‰"""
        if not landmarks or not landmarks.landmark:
            return "unknown"

        try:
            # Week 2æœ€é©åŒ–: ä¸ŠåŠèº«ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®ã¿ã§åˆ¤å®š
            # è‚©ãƒ»è‚˜ãƒ»æ‰‹é¦–ã®æ¤œå‡ºçŠ¶æ³ã‚’ç¢ºèª
            left_shoulder = landmarks.landmark[11]
            right_shoulder = landmarks.landmark[12]
            left_elbow = landmarks.landmark[13]
            right_elbow = landmarks.landmark[14]
            left_wrist = landmarks.landmark[15]
            right_wrist = landmarks.landmark[16]

            # ä¸ŠåŠèº«ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å¯è¦–æ€§ãƒã‚§ãƒƒã‚¯
            upper_body_visible = 0
            upper_body_points = [
                left_shoulder,
                right_shoulder,
                left_elbow,
                right_elbow,
                left_wrist,
                right_wrist,
            ]

            for point in upper_body_points:
                if point.visibility > 0.2:  # Week 2æœ€é©åŒ–: ç·©å’Œã•ã‚ŒãŸé–¾å€¤
                    upper_body_visible += 1

            # Week 2æœ€é©åŒ–: 3ç‚¹ä»¥ä¸Šæ¤œå‡ºã§æœ‰åŠ¹ãªãƒãƒ¼ã‚ºã¨ã—ã¦åˆ¤å®š
            if upper_body_visible >= 3:
                # è‚©ã®ä½ç½®é–¢ä¿‚ã‹ã‚‰åŸºæœ¬å§¿å‹¢ã‚’æ¨å®š
                if left_shoulder.visibility > 0.2 and right_shoulder.visibility > 0.2:
                    shoulder_y_diff = abs(left_shoulder.y - right_shoulder.y)

                    if shoulder_y_diff < 0.05:
                        return "standing"  # è‚©ãŒæ°´å¹³â†’ç«‹ä½
                    elif shoulder_y_diff > 0.15:
                        return "profile"  # è‚©ã®é«˜ä½å·®å¤§â†’æ¨ªå‘ã
                    else:
                        return "sitting"  # ä¸­é–“â†’åº§ä½
                else:
                    return "partial_pose"  # Week 2æ–°ã‚«ãƒ†ã‚´ãƒª: éƒ¨åˆ†ãƒãƒ¼ã‚º

            # ä¸‹åŠèº«ã‚‚ç¢ºèªï¼ˆè£œåŠ©çš„ï¼‰
            try:
                left_hip = landmarks.landmark[23]
                right_hip = landmarks.landmark[24]

                if left_hip.visibility > 0.2 or right_hip.visibility > 0.2:
                    return "action"  # ä¸‹åŠèº«ã‚‚è¦‹ãˆã‚‹â†’ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç³»
            except (IndexError, AttributeError):
                pass

            return "upper_body_only"  # Week 2æ–°ã‚«ãƒ†ã‚´ãƒª: ä¸ŠåŠèº«ã®ã¿

        except Exception as e:
            self.logger.warning(f"ã‚¢ãƒ‹ãƒ¡ç‰¹åŒ–ãƒãƒ¼ã‚ºåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            return "unknown"

    def _evaluate_partial_pose_completeness(self, landmarks) -> float:
        """éƒ¨åˆ†ãƒãƒ¼ã‚ºå®Œå…¨æ€§è©•ä¾¡ï¼ˆWeek 2æœ€é©åŒ–ï¼šä¸ŠåŠèº«é‡è¦–ï¼‰"""
        if not landmarks or not landmarks.landmark:
            return 0.0

        # Week 2æœ€é©åŒ–: éƒ¨åˆ†ãƒãƒ¼ã‚ºã«ç‰¹åŒ–ã—ãŸé‡ã¿é…åˆ†
        body_parts_weights = {
            # é ­éƒ¨ (30% - é‡è¦åº¦ã‚¢ãƒƒãƒ—)
            "head": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
            # ä¸ŠåŠèº« (50% - æœ€é‡è¦)
            "upper_body": [11, 12, 13, 14, 15, 16],
            # ä¸‹åŠèº« (20% - é‡è¦åº¦ãƒ€ã‚¦ãƒ³ã€éƒ¨åˆ†çš„ã§ã‚‚OK)
            "lower_body": [23, 24, 25, 26, 27, 28],
        }

        weights = {"head": 0.3, "upper_body": 0.5, "lower_body": 0.2}

        total_score = 0.0

        for part_name, indices in body_parts_weights.items():
            part_visibility = 0.0
            valid_count = 0

            for idx in indices:
                if idx < len(landmarks.landmark):
                    landmark = landmarks.landmark[idx]
                    if landmark.visibility > 0.2:  # Week 2æœ€é©åŒ–: ç·©å’Œã•ã‚ŒãŸé–¾å€¤
                        part_visibility += landmark.visibility
                        valid_count += 1

            if valid_count > 0:
                part_score = part_visibility / valid_count
                total_score += part_score * weights[part_name]
            elif part_name == "lower_body":
                # Week 2æœ€é©åŒ–: ä¸‹åŠèº«ãŒè¦‹ãˆãªãã¦ã‚‚æ¸›ç‚¹ã—ãªã„
                total_score += 0.5 * weights[part_name]  # éƒ¨åˆ†çš„ãªã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸

        return min(1.0, total_score)

    def _count_visible_keypoints_optimized(self, landmarks) -> int:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆWeek 2ï¼šç·©å’Œã•ã‚ŒãŸé–¾å€¤ï¼‰"""
        if not landmarks or not landmarks.landmark:
            return 0

        visible_count = 0
        for landmark in landmarks.landmark:
            if landmark.visibility > 0.2:  # Week 2æœ€é©åŒ–: 0.3â†’0.2ã«ç·©å’Œ
                visible_count += 1

        return visible_count

    def _calculate_optimized_confidence(
        self, visibility_score: float, completeness_score: float, keypoints_detected: int
    ) -> float:
        """æœ€é©åŒ–ã•ã‚ŒãŸä¿¡é ¼åº¦è¨ˆç®—ï¼ˆWeek 2ï¼šéƒ¨åˆ†ãƒãƒ¼ã‚ºã§ã‚‚é«˜ä¿¡é ¼åº¦ï¼‰"""

        # Week 2æœ€é©åŒ–: 3ç‚¹ä»¥ä¸Šæ¤œå‡ºã§åŸºæœ¬ä¿¡é ¼åº¦ã‚’ç¢ºä¿
        if keypoints_detected >= 3:
            base_confidence = 0.6  # æœ€ä½ä¿¡é ¼åº¦ã‚’å¼•ãä¸Šã’
        else:
            base_confidence = 0.3

        # å¯è¦–æ€§ãƒ»å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è£œæ­£
        combined_score = (visibility_score + completeness_score) / 2

        # Week 2æœ€é©åŒ–: ã‚ˆã‚Šå¯›å®¹ãªä¿¡é ¼åº¦è¨ˆç®—
        final_confidence = base_confidence + (combined_score * 0.4)

        return min(1.0, final_confidence)


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="å¼·åŒ–æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    parser.add_argument("--image", "-i", required=True, help="ãƒ†ã‚¹ãƒˆç”»åƒãƒ‘ã‚¹")
    parser.add_argument("--face-only", action="store_true", help="é¡”æ¤œå‡ºã®ã¿å®Ÿè¡Œ")
    parser.add_argument("--pose-only", action="store_true", help="ãƒãƒ¼ã‚ºæ¤œå‡ºã®ã¿å®Ÿè¡Œ")

    args = parser.parse_args()

    # ç”»åƒèª­ã¿è¾¼ã¿
    image = cv2.imread(args.image)
    if image is None:
        print(f"ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {args.image}")
        return 1

    print(f"ğŸ” å¼·åŒ–æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ: {args.image}")
    print("=" * 60)

    # é¡”æ¤œå‡ºãƒ†ã‚¹ãƒˆ
    if not args.pose_only:
        print("ğŸ‘¤ é¡”æ¤œå‡ºãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        face_detector = EnhancedFaceDetector()
        face_detections = face_detector.detect_faces_comprehensive(image)

        print(f"é¡”æ¤œå‡ºçµæœ: {len(face_detections)}ä»¶")
        for i, detection in enumerate(face_detections):
            print(
                f"  é¡”{i+1}: æ‰‹æ³•={detection.method}, "
                f"ä¿¡é ¼åº¦={detection.confidence:.3f}, "
                f"ä½ç½®={detection.bbox}"
            )

    # ãƒãƒ¼ã‚ºæ¤œå‡ºãƒ†ã‚¹ãƒˆ
    if not args.face_only:
        print("\nğŸ¤¸ ãƒãƒ¼ã‚ºæ¤œå‡ºãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        pose_detector = EnhancedPoseDetector()
        pose_result = pose_detector.detect_pose_comprehensive(image)

        print(f"ãƒãƒ¼ã‚ºæ¤œå‡ºçµæœ:")
        print(f"  æ¤œå‡ºæˆåŠŸ: {pose_result.detected}")
        if pose_result.detected:
            print(f"  ã‚«ãƒ†ã‚´ãƒª: {pose_result.pose_category}")
            print(f"  å¯è¦–æ€§ã‚¹ã‚³ã‚¢: {pose_result.visibility_score:.3f}")
            print(f"  å®Œå…¨æ€§ã‚¹ã‚³ã‚¢: {pose_result.completeness_score:.3f}")
            print(f"  ç·åˆä¿¡é ¼åº¦: {pose_result.confidence:.3f}")
            print(f"  å¯è¦–ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ: {pose_result.keypoints_detected}/33")

    return 0


if __name__ == "__main__":
    exit(main())
