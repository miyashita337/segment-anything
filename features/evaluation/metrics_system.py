#!/usr/bin/env python3
"""
è©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ  - Largest-Character AccuracyæŒ‡æ¨™ã®ç¢ºç«‹
Comprehensive evaluation metrics for character extraction performance
"""

import numpy as np
import matplotlib.pyplot as plt

import json
import logging
import pandas as pd
import seaborn as sns
from dataclasses import asdict, dataclass
from pathlib import Path
from sklearn.metrics import classification_report, confusion_matrix
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


@dataclass
class MetricResult:
    """è©•ä¾¡æŒ‡æ¨™çµæœ"""
    metric_name: str
    value: float
    threshold: Optional[float] = None
    status: str = "measured"  # measured, passed, failed
    notes: str = ""


@dataclass
class CharacterDetectionMetrics:
    """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡ºè©•ä¾¡æŒ‡æ¨™"""
    # ä¸»è¦æŒ‡æ¨™
    largest_char_accuracy: MetricResult
    mean_iou: MetricResult
    map_50: MetricResult  # mAP@0.5
    map_95: MetricResult  # mAP@[.5:.95]
    
    # å“è³ªæŒ‡æ¨™
    ab_evaluation_rate: MetricResult
    precision_at_k: MetricResult  # P@1 (æœ€å¤§å€™è£œã®ç²¾åº¦)
    recall_at_k: MetricResult     # R@1 (æœ€å¤§å€™è£œã®å†ç¾ç‡)
    
    # å‡¦ç†æ€§èƒ½æŒ‡æ¨™
    fps: MetricResult
    memory_usage: MetricResult
    
    # åˆ†å¸ƒçµ±è¨ˆ
    confidence_stats: Dict[str, float]
    size_distribution: Dict[str, float]
    failure_analysis: Dict[str, Any]


class MetricsCalculator:
    """è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, results_data: List[Dict[str, Any]]):
        """
        åˆæœŸåŒ–
        
        Args:
            results_data: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ‡ãƒ¼ã‚¿
        """
        self.results_data = results_data
        self.total_samples = len(results_data)
        
        # æˆåŠŸåŸºæº–é–¾å€¤
        self.thresholds = {
            "largest_char_accuracy": 0.80,  # 80%ä»¥ä¸Š
            "ab_evaluation_rate": 0.70,     # 70%ä»¥ä¸Š
            "mean_iou": 0.65,               # 0.65ä»¥ä¸Š
            "map_50": 0.75,                 # 75%ä»¥ä¸Š
            "fps": 0.2,                     # 5ç§’/ç”»åƒä»¥ä¸‹ï¼ˆ0.2 FPSä»¥ä¸Šï¼‰
            "precision_at_k": 0.80,         # 80%ä»¥ä¸Š
            "recall_at_k": 0.75             # 75%ä»¥ä¸Š
        }
    
    def calculate_largest_char_accuracy(self) -> MetricResult:
        """Largest-Character Accuracyè¨ˆç®—"""
        try:
            correct_predictions = sum(1 for r in self.results_data if r.get('largest_char_predicted', False))
            accuracy = correct_predictions / self.total_samples if self.total_samples > 0 else 0.0
            
            threshold = self.thresholds["largest_char_accuracy"]
            status = "passed" if accuracy >= threshold else "failed"
            
            return MetricResult(
                metric_name="Largest-Character Accuracy",
                value=accuracy,
                threshold=threshold,
                status=status,
                notes=f"{correct_predictions}/{self.total_samples} æ­£è§£"
            )
            
        except Exception as e:
            logger.error(f"Largest-Character Accuracyè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return MetricResult("Largest-Character Accuracy", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def calculate_mean_iou(self) -> MetricResult:
        """å¹³å‡IoUè¨ˆç®—"""
        try:
            iou_scores = [r.get('iou_score', 0.0) for r in self.results_data]
            mean_iou = np.mean(iou_scores) if iou_scores else 0.0
            
            threshold = self.thresholds["mean_iou"]
            status = "passed" if mean_iou >= threshold else "failed"
            
            return MetricResult(
                metric_name="Mean IoU",
                value=mean_iou,
                threshold=threshold,
                status=status,
                notes=f"ç¯„å›²: {min(iou_scores):.3f} - {max(iou_scores):.3f}"
            )
            
        except Exception as e:
            logger.error(f"Mean IoUè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return MetricResult("Mean IoU", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def calculate_map_metrics(self) -> Tuple[MetricResult, MetricResult]:
        """mAP@0.5 ã¨ mAP@[.5:.95] è¨ˆç®—"""
        try:
            # IoUé–¾å€¤åˆ¥ã®ç²¾åº¦è¨ˆç®—
            iou_thresholds_50 = [0.5]
            iou_thresholds_95 = np.arange(0.5, 1.0, 0.05)
            
            # mAP@0.5
            map_50_scores = []
            for threshold in iou_thresholds_50:
                correct = sum(1 for r in self.results_data if r.get('iou_score', 0.0) >= threshold)
                precision = correct / self.total_samples if self.total_samples > 0 else 0.0
                map_50_scores.append(precision)
            
            map_50 = np.mean(map_50_scores)
            map_50_status = "passed" if map_50 >= self.thresholds["map_50"] else "failed"
            
            # mAP@[.5:.95]
            map_95_scores = []
            for threshold in iou_thresholds_95:
                correct = sum(1 for r in self.results_data if r.get('iou_score', 0.0) >= threshold)
                precision = correct / self.total_samples if self.total_samples > 0 else 0.0
                map_95_scores.append(precision)
            
            map_95 = np.mean(map_95_scores)
            
            return (
                MetricResult(
                    metric_name="mAP@0.5",
                    value=map_50,
                    threshold=self.thresholds["map_50"],
                    status=map_50_status,
                    notes="IoUé–¾å€¤0.5ã§ã®å¹³å‡ç²¾åº¦"
                ),
                MetricResult(
                    metric_name="mAP@[.5:.95]",
                    value=map_95,
                    threshold=None,
                    status="measured",
                    notes="IoUé–¾å€¤0.5-0.95ã§ã®å¹³å‡ç²¾åº¦"
                )
            )
            
        except Exception as e:
            logger.error(f"mAPè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return (
                MetricResult("mAP@0.5", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}"),
                MetricResult("mAP@[.5:.95]", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            )
    
    def calculate_quality_metrics(self) -> MetricResult:
        """A/Bè©•ä¾¡ç‡è¨ˆç®—"""
        try:
            ab_grades = sum(1 for r in self.results_data if r.get('quality_grade', 'F') in ['A', 'B'])
            ab_rate = ab_grades / self.total_samples if self.total_samples > 0 else 0.0
            
            threshold = self.thresholds["ab_evaluation_rate"]
            status = "passed" if ab_rate >= threshold else "failed"
            
            return MetricResult(
                metric_name="A/B Evaluation Rate",
                value=ab_rate,
                threshold=threshold,
                status=status,
                notes=f"{ab_grades}/{self.total_samples} ãŒA/Bè©•ä¾¡"
            )
            
        except Exception as e:
            logger.error(f"å“è³ªæŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return MetricResult("A/B Evaluation Rate", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def calculate_precision_recall_at_k(self) -> Tuple[MetricResult, MetricResult]:
        """Precision@1, Recall@1è¨ˆç®—ï¼ˆæœ€å¤§å€™è£œã«å¯¾ã™ã‚‹ï¼‰"""
        try:
            # True Positives: IoU >= 0.5 ã‹ã¤æœ€å¤§é¢ç©å€™è£œ
            true_positives = sum(1 for r in self.results_data 
                               if r.get('largest_char_predicted', False) and r.get('iou_score', 0.0) >= 0.5)
            
            # Predicted Positives: æ¤œå‡ºã•ã‚ŒãŸæœ€å¤§å€™è£œæ•°ï¼ˆå…¨ã¦ï¼‰
            predicted_positives = sum(1 for r in self.results_data if r.get('prediction_bbox') is not None)
            
            # Actual Positives: æ­£è§£ãƒ©ãƒ™ãƒ«ãŒå­˜åœ¨ã™ã‚‹æ•°ï¼ˆå…¨ã¦ï¼‰
            actual_positives = self.total_samples  # å…¨ç”»åƒã«æ­£è§£ãƒ©ãƒ™ãƒ«ãŒå­˜åœ¨
            
            # Precision@1è¨ˆç®—
            precision_at_1 = true_positives / predicted_positives if predicted_positives > 0 else 0.0
            precision_status = "passed" if precision_at_1 >= self.thresholds["precision_at_k"] else "failed"
            
            # Recall@1è¨ˆç®—
            recall_at_1 = true_positives / actual_positives if actual_positives > 0 else 0.0
            recall_status = "passed" if recall_at_1 >= self.thresholds["recall_at_k"] else "failed"
            
            return (
                MetricResult(
                    metric_name="Precision@1",
                    value=precision_at_1,
                    threshold=self.thresholds["precision_at_k"],
                    status=precision_status,
                    notes=f"TP:{true_positives}, PP:{predicted_positives}"
                ),
                MetricResult(
                    metric_name="Recall@1",
                    value=recall_at_1,
                    threshold=self.thresholds["recall_at_k"],
                    status=recall_status,
                    notes=f"TP:{true_positives}, AP:{actual_positives}"
                )
            )
            
        except Exception as e:
            logger.error(f"Precision/Recallè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return (
                MetricResult("Precision@1", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}"),
                MetricResult("Recall@1", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            )
    
    def calculate_performance_metrics(self) -> Tuple[MetricResult, MetricResult]:
        """å‡¦ç†æ€§èƒ½æŒ‡æ¨™è¨ˆç®—"""
        try:
            processing_times = [r.get('processing_time', 0.0) for r in self.results_data]
            
            # FPSè¨ˆç®—
            mean_processing_time = np.mean(processing_times) if processing_times else float('inf')
            fps = 1.0 / mean_processing_time if mean_processing_time > 0 else 0.0
            
            fps_threshold = self.thresholds["fps"]
            fps_status = "passed" if fps >= fps_threshold else "failed"
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆä»®æƒ³å€¤ã€å®Ÿè£…æ™‚ã«å®Ÿæ¸¬å€¤ã§æ›´æ–°ï¼‰
            estimated_memory = 4.5  # GBï¼ˆYOLO + SAM + ç”»åƒãƒãƒƒãƒ•ã‚¡ï¼‰
            
            return (
                MetricResult(
                    metric_name="FPS",
                    value=fps,
                    threshold=fps_threshold,
                    status=fps_status,
                    notes=f"å¹³å‡å‡¦ç†æ™‚é–“: {mean_processing_time:.2f}ç§’"
                ),
                MetricResult(
                    metric_name="Memory Usage",
                    value=estimated_memory,
                    threshold=None,
                    status="measured",
                    notes="YOLO + SAM + ãƒãƒƒãƒ•ã‚¡æ¨å®šå€¤"
                )
            )
            
        except Exception as e:
            logger.error(f"æ€§èƒ½æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return (
                MetricResult("FPS", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}"),
                MetricResult("Memory Usage", 0.0, notes=f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            )
    
    def calculate_distribution_stats(self) -> Dict[str, Dict[str, float]]:
        """åˆ†å¸ƒçµ±è¨ˆè¨ˆç®—"""
        try:
            # ä¿¡é ¼åº¦çµ±è¨ˆ
            confidence_scores = [r.get('confidence_score', 0.0) for r in self.results_data]
            confidence_stats = {
                "mean": float(np.mean(confidence_scores)),
                "std": float(np.std(confidence_scores)),
                "min": float(np.min(confidence_scores)),
                "max": float(np.max(confidence_scores)),
                "median": float(np.median(confidence_scores)),
                "q25": float(np.percentile(confidence_scores, 25)),
                "q75": float(np.percentile(confidence_scores, 75))
            }
            
            # ã‚µã‚¤ã‚ºåˆ†å¸ƒï¼ˆé¢ç©æ¯”ï¼‰
            area_ratios = [r.get('area_largest_ratio', 0.0) for r in self.results_data if r.get('area_largest_ratio', 0.0) > 0]
            size_distribution = {
                "mean_area_ratio": float(np.mean(area_ratios)) if area_ratios else 0.0,
                "std_area_ratio": float(np.std(area_ratios)) if area_ratios else 0.0,
                "single_character_rate": sum(1 for r in self.results_data if r.get('character_count', 0) == 1) / self.total_samples,
                "multi_character_rate": sum(1 for r in self.results_data if r.get('character_count', 0) > 1) / self.total_samples
            }
            
            return {
                "confidence_stats": confidence_stats,
                "size_distribution": size_distribution
            }
            
        except Exception as e:
            logger.error(f"åˆ†å¸ƒçµ±è¨ˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"confidence_stats": {}, "size_distribution": {}}
    
    def analyze_failures(self) -> Dict[str, Any]:
        """å¤±æ•—ã‚±ãƒ¼ã‚¹åˆ†æ"""
        try:
            failed_cases = [r for r in self.results_data if not r.get('largest_char_predicted', False)]
            
            if not failed_cases:
                return {"total_failures": 0, "failure_patterns": {}}
            
            failure_analysis = {
                "total_failures": len(failed_cases),
                "failure_rate": len(failed_cases) / self.total_samples,
                "failure_patterns": {}
            }
            
            # å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
            # 1. æ¥µä½IoUï¼ˆ< 0.1ï¼‰
            extremely_low_iou = [r for r in failed_cases if r.get('iou_score', 0.0) < 0.1]
            failure_analysis["failure_patterns"]["extremely_low_iou"] = {
                "count": len(extremely_low_iou),
                "rate": len(extremely_low_iou) / len(failed_cases) if failed_cases else 0.0,
                "description": "IoU < 0.1ã®å®Œå…¨ãªãƒŸã‚¹"
            }
            
            # 2. ä½ä¿¡é ¼åº¦ï¼ˆ< 0.3ï¼‰
            low_confidence = [r for r in failed_cases if r.get('confidence_score', 0.0) < 0.3]
            failure_analysis["failure_patterns"]["low_confidence"] = {
                "count": len(low_confidence),
                "rate": len(low_confidence) / len(failed_cases) if failed_cases else 0.0,
                "description": "ä¿¡é ¼åº¦ < 0.3ã®ä¸ç¢ºå®Ÿãªæ¤œå‡º"
            }
            
            # 3. è¤‡æ•°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç«¶åˆ
            multi_char_conflicts = [r for r in failed_cases if r.get('character_count', 0) > 2]
            failure_analysis["failure_patterns"]["multi_character_conflict"] = {
                "count": len(multi_char_conflicts),
                "rate": len(multi_char_conflicts) / len(failed_cases) if failed_cases else 0.0,
                "description": "3ä½“ä»¥ä¸Šã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç«¶åˆã«ã‚ˆã‚‹èª¤é¸æŠ"
            }
            
            # 4. ä¸­ç¨‹åº¦IoUï¼ˆ0.1-0.4ï¼‰ï¼šéƒ¨åˆ†çš„æˆåŠŸ
            partial_success = [r for r in failed_cases if 0.1 <= r.get('iou_score', 0.0) < 0.5]
            failure_analysis["failure_patterns"]["partial_success"] = {
                "count": len(partial_success),
                "rate": len(partial_success) / len(failed_cases) if failed_cases else 0.0,
                "description": "0.1 â‰¤ IoU < 0.5ã®éƒ¨åˆ†çš„æˆåŠŸï¼ˆèª¿æ•´ã§æ”¹å–„å¯èƒ½ï¼‰"
            }
            
            return failure_analysis
            
        except Exception as e:
            logger.error(f"å¤±æ•—åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"total_failures": 0, "failure_patterns": {}, "error": str(e)}
    
    def compute_all_metrics(self) -> CharacterDetectionMetrics:
        """å…¨æŒ‡æ¨™è¨ˆç®—"""
        try:
            logger.info("è©•ä¾¡æŒ‡æ¨™è¨ˆç®—é–‹å§‹")
            
            # ä¸»è¦æŒ‡æ¨™è¨ˆç®—
            largest_char_accuracy = self.calculate_largest_char_accuracy()
            mean_iou = self.calculate_mean_iou()
            map_50, map_95 = self.calculate_map_metrics()
            
            # å“è³ªæŒ‡æ¨™
            ab_evaluation_rate = self.calculate_quality_metrics()
            precision_at_k, recall_at_k = self.calculate_precision_recall_at_k()
            
            # æ€§èƒ½æŒ‡æ¨™
            fps, memory_usage = self.calculate_performance_metrics()
            
            # åˆ†å¸ƒçµ±è¨ˆ
            distribution_stats = self.calculate_distribution_stats()
            
            # å¤±æ•—åˆ†æ
            failure_analysis = self.analyze_failures()
            
            metrics = CharacterDetectionMetrics(
                largest_char_accuracy=largest_char_accuracy,
                mean_iou=mean_iou,
                map_50=map_50,
                map_95=map_95,
                ab_evaluation_rate=ab_evaluation_rate,
                precision_at_k=precision_at_k,
                recall_at_k=recall_at_k,
                fps=fps,
                memory_usage=memory_usage,
                confidence_stats=distribution_stats["confidence_stats"],
                size_distribution=distribution_stats["size_distribution"],
                failure_analysis=failure_analysis
            )
            
            logger.info("è©•ä¾¡æŒ‡æ¨™è¨ˆç®—å®Œäº†")
            return metrics
            
        except Exception as e:
            logger.error(f"å…¨æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            raise


class MetricsVisualizer:
    """è©•ä¾¡æŒ‡æ¨™å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, metrics: CharacterDetectionMetrics, output_dir: Path):
        """
        åˆæœŸåŒ–
        
        Args:
            metrics: è¨ˆç®—æ¸ˆã¿è©•ä¾¡æŒ‡æ¨™
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.metrics = metrics
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def create_performance_dashboard(self):
        """ç·åˆæ€§èƒ½ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ"""
        try:
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯: ç·åˆæ€§èƒ½ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰', fontsize=16, fontweight='bold')
            
            # 1. ä¸»è¦æŒ‡æ¨™ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
            ax1 = axes[0, 0]
            metrics_names = ['Largest-Char\nAccuracy', 'Mean IoU', 'mAP@0.5', 'A/B Rate', 'Precision@1', 'Recall@1']
            metrics_values = [
                self.metrics.largest_char_accuracy.value,
                self.metrics.mean_iou.value,
                self.metrics.map_50.value,
                self.metrics.ab_evaluation_rate.value,
                self.metrics.precision_at_k.value,
                self.metrics.recall_at_k.value
            ]
            
            angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False)
            metrics_values += metrics_values[:1]  # é–‰ã˜ã‚‹ãŸã‚
            angles = np.concatenate((angles, [angles[0]]))
            
            ax1 = plt.subplot(2, 3, 1, projection='polar')
            ax1.plot(angles, metrics_values, 'o-', linewidth=2, label='ç¾åœ¨æ€§èƒ½')
            ax1.fill(angles, metrics_values, alpha=0.25)
            ax1.set_xticks(angles[:-1])
            ax1.set_xticklabels(metrics_names)
            ax1.set_ylim(0, 1)
            ax1.set_title('ä¸»è¦æŒ‡æ¨™ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ')
            ax1.grid(True)
            
            # 2. å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
            ax2 = axes[0, 1]
            # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆä»®ã®ãƒ‡ãƒ¼ã‚¿ã§è¡¨ç¤ºï¼‰
            grades = ['A', 'B', 'C', 'D', 'E', 'F']
            # failure_analysisã‹ã‚‰æ¨å®šï¼ˆå®Ÿè£…æ™‚ã«ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼‰
            grade_counts = [5, 15, 25, 30, 15, 10]  # ä»®ãƒ‡ãƒ¼ã‚¿
            
            bars = ax2.bar(grades, grade_counts, color=['#2ecc71', '#27ae60', '#f39c12', '#e67e22', '#e74c3c', '#c0392b'])
            ax2.set_title('å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ')
            ax2.set_ylabel('ç”»åƒæ•°')
            
            # å„ãƒãƒ¼ã«æ•°å€¤è¡¨ç¤º
            for bar, count in zip(bars, grade_counts):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{count}', ha='center', va='bottom')
            
            # 3. ä¿¡é ¼åº¦åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
            ax3 = axes[0, 2]
            if self.metrics.confidence_stats:
                # æ­£è¦åˆ†å¸ƒè¿‘ä¼¼ã§ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä½œæˆ
                mean_conf = self.metrics.confidence_stats.get('mean', 0.5)
                std_conf = self.metrics.confidence_stats.get('std', 0.2)
                conf_samples = np.random.normal(mean_conf, std_conf, 1000)
                conf_samples = np.clip(conf_samples, 0, 1)
                
                ax3.hist(conf_samples, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
                ax3.axvline(mean_conf, color='red', linestyle='--', label=f'å¹³å‡: {mean_conf:.3f}')
                ax3.set_title('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢åˆ†å¸ƒ')
                ax3.set_xlabel('ä¿¡é ¼åº¦')
                ax3.set_ylabel('é »åº¦')
                ax3.legend()
            
            # 4. IoUåˆ†å¸ƒãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
            ax4 = axes[1, 0]
            # ä»®ãƒ‡ãƒ¼ã‚¿ã§ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå®Ÿè£…æ™‚ã«å®Ÿãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
            iou_data = [np.random.beta(2, 3, 100)]  # ä»®ã®IoUåˆ†å¸ƒ
            ax4.boxplot(iou_data, labels=['IoU Distribution'])
            ax4.axhline(0.5, color='red', linestyle='--', label='æˆåŠŸé–¾å€¤ (0.5)')
            ax4.set_title('IoUã‚¹ã‚³ã‚¢åˆ†å¸ƒ')
            ax4.set_ylabel('IoU')
            ax4.legend()
            
            # 5. å‡¦ç†æ™‚é–“çµ±è¨ˆ
            ax5 = axes[1, 1]
            processing_metrics = ['FPS', 'Mean Time', 'Memory (GB)']
            processing_values = [
                self.metrics.fps.value,
                1/self.metrics.fps.value if self.metrics.fps.value > 0 else 0,
                self.metrics.memory_usage.value
            ]
            
            bars = ax5.bar(processing_metrics, processing_values, color=['#3498db', '#9b59b6', '#e74c3c'])
            ax5.set_title('å‡¦ç†æ€§èƒ½çµ±è¨ˆ')
            ax5.set_ylabel('å€¤')
            
            # å„ãƒãƒ¼ã«æ•°å€¤è¡¨ç¤º
            for bar, value in zip(bars, processing_values):
                height = bar.get_height()
                ax5.text(bar.get_x() + bar.get_width()/2., height,
                        f'{value:.2f}', ha='center', va='bottom')
            
            # 6. å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            ax6 = axes[1, 2]
            if self.metrics.failure_analysis and 'failure_patterns' in self.metrics.failure_analysis:
                patterns = list(self.metrics.failure_analysis['failure_patterns'].keys())
                pattern_counts = [self.metrics.failure_analysis['failure_patterns'][p]['count'] 
                                for p in patterns]
                
                if patterns and pattern_counts:
                    wedges, texts, autotexts = ax6.pie(pattern_counts, labels=patterns, autopct='%1.1f%%', startangle=90)
                    ax6.set_title('å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ')
                else:
                    ax6.text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center', transform=ax6.transAxes)
                    ax6.set_title('å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ')
            
            plt.tight_layout()
            
            # ä¿å­˜
            dashboard_file = self.output_dir / "performance_dashboard.png"
            plt.savefig(dashboard_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"æ€§èƒ½ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆå®Œäº†: {dashboard_file}")
            
        except Exception as e:
            logger.error(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_metrics_json(self):
        """æŒ‡æ¨™çµæœã‚’JSONä¿å­˜"""
        try:
            metrics_dict = asdict(self.metrics)
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            json_file = self.output_dir / "evaluation_metrics.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(metrics_dict, f, indent=2, ensure_ascii=False)
            
            logger.info(f"è©•ä¾¡æŒ‡æ¨™JSONä¿å­˜å®Œäº†: {json_file}")
            
        except Exception as e:
            logger.error(f"JSONä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logging.basicConfig(level=logging.INFO)
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    sample_results = [
        {
            'image_id': f'test_{i:03d}',
            'largest_char_predicted': np.random.random() > 0.4,  # 60%æˆåŠŸç‡
            'iou_score': np.random.beta(2, 3),  # 0-1ã®ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ
            'confidence_score': np.random.uniform(0.1, 0.9),
            'processing_time': np.random.uniform(3.0, 8.0),
            'character_count': np.random.randint(1, 5),
            'area_largest_ratio': np.random.uniform(0.3, 0.8),
            'quality_grade': np.random.choice(['A', 'B', 'C', 'D', 'E', 'F'], p=[0.05, 0.15, 0.25, 0.25, 0.20, 0.10])
        }
        for i in range(101)
    ]
    
    # æŒ‡æ¨™è¨ˆç®—
    calculator = MetricsCalculator(sample_results)
    metrics = calculator.compute_all_metrics()
    
    # çµæœè¡¨ç¤º
    print("\n=== Phase 0 è©•ä¾¡æŒ‡æ¨™çµæœ ===")
    print(f"Largest-Character Accuracy: {metrics.largest_char_accuracy.value:.1%} ({metrics.largest_char_accuracy.status})")
    print(f"Mean IoU: {metrics.mean_iou.value:.3f} ({metrics.mean_iou.status})")
    print(f"A/Bè©•ä¾¡ç‡: {metrics.ab_evaluation_rate.value:.1%} ({metrics.ab_evaluation_rate.status})")
    print(f"FPS: {metrics.fps.value:.3f} ({metrics.fps.status})")
    
    # å¯è¦–åŒ–
    output_dir = Path("/tmp/metrics_test")
    visualizer = MetricsVisualizer(metrics, output_dir)
    visualizer.create_performance_dashboard()
    visualizer.save_metrics_json()
    
    print(f"\nğŸ“Š çµæœä¿å­˜: {output_dir}")


if __name__ == "__main__":
    main()