#!/usr/bin/env python3
"""
ãƒ†ã‚¹ãƒˆãƒãƒƒãƒç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
Claudeã®è‡ªå·±è©•ä¾¡ã«åŸºã¥ããƒ™ã‚¹ãƒˆ5ãƒ»ãƒ¯ãƒ¼ã‚¹ãƒˆ5æŠ½å‡ºæ©Ÿèƒ½
"""

import numpy as np
import cv2
import matplotlib.patches as patches
import matplotlib.pyplot as plt

import json
import logging
import shutil
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


@dataclass
class TestBatchItem:
    """ãƒ†ã‚¹ãƒˆãƒãƒƒãƒé …ç›®"""
    image_id: str
    image_path: str
    rank: int  # 1-10ã®ãƒ©ãƒ³ã‚¯
    category: str  # "best" or "worst"
    claude_score: float  # Claudeã®è‡ªå·±è©•ä¾¡ã‚¹ã‚³ã‚¢
    ground_truth_bbox: Tuple[int, int, int, int]  # æ­£è§£bbox
    predicted_bbox: Optional[Tuple[int, int, int, int]]  # äºˆæ¸¬bbox
    iou_score: float
    confidence: float
    quality_grade: str
    issues: List[str]  # ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œç‚¹
    notes: str


@dataclass
class TestBatchSummary:
    """ãƒ†ã‚¹ãƒˆãƒãƒƒãƒã‚µãƒãƒªãƒ¼"""
    timestamp: str
    phase: str
    total_items: int
    best_items: List[TestBatchItem]
    worst_items: List[TestBatchItem]
    score_range: Tuple[float, float]  # (min, max)
    avg_score_best: float
    avg_score_worst: float
    key_insights: List[str]


class TestBatchGenerator:
    """ãƒ†ã‚¹ãƒˆãƒãƒƒãƒç”Ÿæˆå™¨"""
    
    def __init__(self, project_root: Path):
        """
        åˆæœŸåŒ–
        
        Args:
            project_root: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.project_root = project_root
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸå‰‡: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ã¸ã®ç”»åƒå‡ºåŠ›ç¦æ­¢
        self.output_dir = Path("/mnt/c/AItools/lora/train/yado/test_batches")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.labels_file = project_root / "extracted_labels.json"
        self.ground_truth_labels = self.load_ground_truth_labels()
        
    def load_ground_truth_labels(self) -> Dict[str, Any]:
        """äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            if not self.labels_file.exists():
                logger.warning(f"ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.labels_file}")
                return {}
            
            with open(self.labels_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(data)}ãƒ•ã‚¡ã‚¤ãƒ«")
            return data
            
        except Exception as e:
            logger.error(f"ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def calculate_claude_score(self, result: Dict[str, Any]) -> float:
        """
        Claudeã®è‡ªå·±è©•ä¾¡ã‚¹ã‚³ã‚¢è¨ˆç®—
        
        Args:
            result: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            
        Returns:
            0.0-1.0ã®è‡ªå·±è©•ä¾¡ã‚¹ã‚³ã‚¢
        """
        try:
            if not isinstance(result, dict):
                logger.error(f"calculate_claude_score: äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿å‹ {type(result)}")
                return 0.0
            # è¤‡åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            weights = {
                'iou_weight': 0.4,        # IoUé‡è¦åº¦
                'confidence_weight': 0.25, # ä¿¡é ¼åº¦é‡è¦åº¦
                'quality_weight': 0.20,   # å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰é‡è¦åº¦
                'stability_weight': 0.15  # å‡¦ç†å®‰å®šæ€§é‡è¦åº¦
            }
            
            # IoUã‚¹ã‚³ã‚¢ (0.0-1.0)
            iou_score = result.get('iou_score', 0.0)
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ (0.0-1.0)
            confidence_score = result.get('confidence_score', 0.0)
            
            # å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢
            grade_mapping = {'A': 1.0, 'B': 0.8, 'C': 0.6, 'D': 0.4, 'E': 0.2, 'F': 0.0}
            quality_score = grade_mapping.get(result.get('quality_grade', 'F'), 0.0)
            
            # å‡¦ç†å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆå‡¦ç†æ™‚é–“ã®é€†æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
            processing_time = result.get('processing_time', 10.0)
            stability_score = min(1.0, 5.0 / max(processing_time, 1.0))  # 5ç§’ä»¥ä¸‹ã§1.0
            
            # é‡ã¿ä»˜ãç·åˆã‚¹ã‚³ã‚¢
            claude_score = (
                iou_score * weights['iou_weight'] +
                confidence_score * weights['confidence_weight'] +
                quality_score * weights['quality_weight'] +
                stability_score * weights['stability_weight']
            )
            
            return min(1.0, max(0.0, claude_score))
            
        except Exception as e:
            logger.error(f"Claudeã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def analyze_issues(self, result: Dict[str, Any]) -> List[str]:
        """
        å•é¡Œç‚¹åˆ†æ
        
        Args:
            result: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            
        Returns:
            ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œç‚¹ã®ãƒªã‚¹ãƒˆ
        """
        issues = []
        
        try:
            if not isinstance(result, dict):
                logger.error(f"analyze_issues: äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿å‹ {type(result)}")
                return ["ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚¨ãƒ©ãƒ¼"]
            iou_score = result.get('iou_score', 0.0)
            confidence = result.get('confidence_score', 0.0)
            processing_time = result.get('processing_time', 0.0)
            character_count = result.get('character_count', 0)
            
            # IoUé–¢é€£ã®å•é¡Œ
            if iou_score < 0.1:
                issues.append("å®Œå…¨ãªãƒŸã‚¹æŠ½å‡ºï¼ˆIoU < 0.1ï¼‰")
            elif iou_score < 0.3:
                issues.append("å¤§å¹…ãªä½ç½®ãšã‚Œï¼ˆIoU < 0.3ï¼‰")
            elif iou_score < 0.5:
                issues.append("éƒ¨åˆ†çš„ãªä½ç½®ãšã‚Œï¼ˆIoU < 0.5ï¼‰")
            
            # ä¿¡é ¼åº¦é–¢é€£ã®å•é¡Œ
            if confidence < 0.2:
                issues.append("æ¥µä½ä¿¡é ¼åº¦æ¤œå‡ºï¼ˆ< 0.2ï¼‰")
            elif confidence < 0.4:
                issues.append("ä½ä¿¡é ¼åº¦æ¤œå‡ºï¼ˆ< 0.4ï¼‰")
            
            # å‡¦ç†æ€§èƒ½é–¢é€£ã®å•é¡Œ
            if processing_time > 15.0:
                issues.append("å‡¦ç†æ™‚é–“éé•·ï¼ˆ> 15ç§’ï¼‰")
            elif processing_time > 10.0:
                issues.append("å‡¦ç†æ™‚é–“é•·ã„ï¼ˆ> 10ç§’ï¼‰")
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡ºé–¢é€£ã®å•é¡Œ
            if character_count == 0:
                issues.append("ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æœªæ¤œå‡º")
            elif character_count > 5:
                issues.append(f"éå‰°æ¤œå‡ºï¼ˆ{character_count}ä½“æ¤œå‡ºï¼‰")
            
            # äºˆæ¸¬bboxé–¢é€£ã®å•é¡Œ
            pred_bbox = result.get('prediction_bbox')
            if pred_bbox is None:
                issues.append("äºˆæ¸¬bboxç”Ÿæˆå¤±æ•—")
            elif pred_bbox == (0, 0, 0, 0):
                issues.append("ç„¡åŠ¹ãªbboxåº§æ¨™")
            
            # å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰é–¢é€£ã®è­¦å‘Š
            quality_grade = result.get('quality_grade', 'F')
            if quality_grade == 'F':
                issues.append("å“è³ªè©•ä¾¡Fï¼ˆè¦å¤§å¹…æ”¹å–„ï¼‰")
            elif quality_grade in ['D', 'E']:
                issues.append(f"ä½å“è³ªè©•ä¾¡ï¼ˆ{quality_grade}è©•ä¾¡ï¼‰")
            
            return issues
            
        except Exception as e:
            logger.error(f"å•é¡Œåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return ["åˆ†æã‚¨ãƒ©ãƒ¼"]
    
    def generate_test_batch(self, benchmark_results: List[Dict[str, Any]], 
                          phase: str = "phase0") -> TestBatchSummary:
        """
        ãƒ†ã‚¹ãƒˆãƒãƒƒãƒç”Ÿæˆ
        
        Args:
            benchmark_results: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒªã‚¹ãƒˆ
            phase: ç¾åœ¨ã®Phase
            
        Returns:
            ãƒ†ã‚¹ãƒˆãƒãƒƒãƒã‚µãƒãƒªãƒ¼
        """
        try:
            logger.info("ãƒ†ã‚¹ãƒˆãƒãƒƒãƒç”Ÿæˆé–‹å§‹")
            
            # Claudeã‚¹ã‚³ã‚¢è¨ˆç®—
            scored_results = []
            for i, result in enumerate(benchmark_results):
                if i == 0:  # ãƒ‡ãƒãƒƒã‚°ç”¨
                    logger.info(f"çµæœãƒ‡ãƒ¼ã‚¿å‹: {type(result)}, å†…å®¹ã‚µãƒ³ãƒ—ãƒ«: {str(result)[:200]}")
                
                try:
                    claude_score = self.calculate_claude_score(result)
                    issues = self.analyze_issues(result)
                    
                    scored_result = {
                        **result,
                        'claude_score': claude_score,
                        'issues': issues
                    }
                    scored_results.append(scored_result)
                except Exception as e:
                    logger.error(f"çµæœå‡¦ç†ã‚¨ãƒ©ãƒ¼ (é …ç›®{i}): {e}")
                    continue
            
            # ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
            scored_results.sort(key=lambda x: x['claude_score'], reverse=True)
            
            # ãƒ™ã‚¹ãƒˆ5ãƒ»ãƒ¯ãƒ¼ã‚¹ãƒˆ5é¸æŠ
            best_5 = scored_results[:5]
            worst_5 = scored_results[-5:]
            
            # TestBatchItemä½œæˆ
            best_items = []
            for i, result in enumerate(best_5, 1):
                try:
                    item = self.create_test_batch_item(result, i, "best")
                    best_items.append(item)
                except Exception as e:
                    logger.error(f"ãƒ™ã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ä½œæˆã‚¨ãƒ©ãƒ¼ (é …ç›®{i}): {e}")
                    continue
            
            worst_items = []
            for i, result in enumerate(worst_5, 1):
                try:
                    item = self.create_test_batch_item(result, i + 5, "worst")
                    worst_items.append(item)
                except Exception as e:
                    logger.error(f"ãƒ¯ãƒ¼ã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ä½œæˆã‚¨ãƒ©ãƒ¼ (é …ç›®{i}): {e}")
                    continue
            
            # çµ±è¨ˆè¨ˆç®—
            all_scores = [r['claude_score'] for r in scored_results]
            score_range = (min(all_scores), max(all_scores))
            avg_score_best = np.mean([item.claude_score for item in best_items])
            avg_score_worst = np.mean([item.claude_score for item in worst_items])
            
            # ä¸»è¦ãªæ´å¯ŸæŠ½å‡º
            key_insights = self.extract_key_insights(best_items, worst_items, scored_results)
            
            # ã‚µãƒãƒªãƒ¼ä½œæˆ
            summary = TestBatchSummary(
                timestamp=time.strftime("%Y%m%d_%H%M%S"),
                phase=phase,
                total_items=10,
                best_items=best_items,
                worst_items=worst_items,
                score_range=score_range,
                avg_score_best=avg_score_best,
                avg_score_worst=avg_score_worst,
                key_insights=key_insights
            )
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
            self.save_test_batch(summary)
            self.create_visual_batch(summary)
            
            logger.info("ãƒ†ã‚¹ãƒˆãƒãƒƒãƒç”Ÿæˆå®Œäº†")
            return summary
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆãƒãƒƒãƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def create_test_batch_item(self, result: Dict[str, Any], 
                              rank: int, category: str) -> TestBatchItem:
        """TestBatchItemä½œæˆ"""
        
        # Ground truthãƒ‡ãƒ¼ã‚¿å–å¾—
        image_id = result['image_id']
        
        # ground_truth_bboxãŒçµæœã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        if 'ground_truth_bbox' in result and result['ground_truth_bbox']:
            gt_bbox = tuple(result['ground_truth_bbox'])
        else:
            # ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
            gt_data = self.ground_truth_labels.get(image_id, {})
            if isinstance(gt_data, dict):
                gt_bbox = tuple(gt_data.get('red_box_coords', [0, 0, 0, 0]))
            else:
                gt_bbox = (0, 0, 0, 0)
        
        return TestBatchItem(
            image_id=image_id,
            image_path=result.get('image_path', ''),
            rank=rank,
            category=category,
            claude_score=result['claude_score'],
            ground_truth_bbox=gt_bbox,
            predicted_bbox=tuple(result['prediction_bbox']) if result.get('prediction_bbox') else None,
            iou_score=result.get('iou_score', 0.0),
            confidence=result.get('confidence_score', 0.0),
            quality_grade=result.get('quality_grade', 'F'),
            issues=result.get('issues', []),
            notes=f"Claudeè‡ªå·±è©•ä¾¡: {result['claude_score']:.3f}, "
                  f"å•é¡Œæ•°: {len(result.get('issues', []))}"
        )
    
    def extract_key_insights(self, best_items: List[TestBatchItem], 
                           worst_items: List[TestBatchItem],
                           all_results: List[Dict[str, Any]]) -> List[str]:
        """ä¸»è¦æ´å¯Ÿã®æŠ½å‡º"""
        insights = []
        
        try:
            # ãƒ™ã‚¹ãƒˆ5ã®å…±é€šç‰¹å¾´
            best_avg_iou = np.mean([item.iou_score for item in best_items])
            best_avg_confidence = np.mean([item.confidence for item in best_items])
            insights.append(f"ãƒ™ã‚¹ãƒˆ5å¹³å‡: IoU {best_avg_iou:.3f}, ä¿¡é ¼åº¦ {best_avg_confidence:.3f}")
            
            # ãƒ¯ãƒ¼ã‚¹ãƒˆ5ã®å…±é€šå•é¡Œ
            worst_avg_iou = np.mean([item.iou_score for item in worst_items])
            worst_avg_confidence = np.mean([item.confidence for item in worst_items])
            insights.append(f"ãƒ¯ãƒ¼ã‚¹ãƒˆ5å¹³å‡: IoU {worst_avg_iou:.3f}, ä¿¡é ¼åº¦ {worst_avg_confidence:.3f}")
            
            # æœ€ã‚‚é »ç¹ãªå•é¡Œã®ç‰¹å®š
            all_issues = []
            for result in all_results:
                all_issues.extend(result.get('issues', []))
            
            if all_issues:
                from collections import Counter
                issue_counts = Counter(all_issues)
                top_issue = issue_counts.most_common(1)[0]
                insights.append(f"æœ€é »å•é¡Œ: {top_issue[0]} ({top_issue[1]}ä»¶)")
            
            # ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
            scores = [r['claude_score'] for r in all_results]
            high_score_count = sum(1 for s in scores if s >= 0.7)
            insights.append(f"é«˜ã‚¹ã‚³ã‚¢(â‰¥0.7): {high_score_count}/{len(scores)}ä»¶ ({high_score_count/len(scores):.1%})")
            
            # å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†æ
            grade_counts = {}
            for result in all_results:
                grade = result.get('quality_grade', 'F')
                grade_counts[grade] = grade_counts.get(grade, 0) + 1
            
            ab_count = grade_counts.get('A', 0) + grade_counts.get('B', 0)
            insights.append(f"A/Bè©•ä¾¡ç‡: {ab_count}/{len(all_results)}ä»¶ ({ab_count/len(all_results):.1%})")
            
        except Exception as e:
            logger.error(f"æ´å¯ŸæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            insights.append("æ´å¯ŸæŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return insights
    
    def save_test_batch(self, summary: TestBatchSummary):
        """ãƒ†ã‚¹ãƒˆãƒãƒƒãƒJSONä¿å­˜"""
        try:
            # JSONä¿å­˜
            json_file = self.output_dir / f"test_batch_{summary.phase}_{summary.timestamp}.json"
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(summary), f, indent=2, ensure_ascii=False)
            
            # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ã‚³ãƒ”ãƒ¼
            latest_file = self.output_dir / f"latest_test_batch_{summary.phase}.json"
            shutil.copy2(json_file, latest_file)
            
            logger.info(f"ãƒ†ã‚¹ãƒˆãƒãƒƒãƒJSONä¿å­˜: {json_file}")
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆãƒãƒƒãƒä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def create_visual_batch(self, summary: TestBatchSummary):
        """å¯è¦–åŒ–ãƒ†ã‚¹ãƒˆãƒãƒƒãƒä½œæˆ"""
        try:
            timestamp = summary.timestamp
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            visual_dir = self.output_dir / f"visual_batch_{summary.phase}_{timestamp}"
            visual_dir.mkdir(exist_ok=True)
            
            # ãƒ™ã‚¹ãƒˆ5ç”»åƒã‚³ãƒ”ãƒ¼ãƒ»ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            best_dir = visual_dir / "best_5"
            best_dir.mkdir(exist_ok=True)
            
            for item in summary.best_items:
                self.create_annotated_image(item, best_dir)
            
            # ãƒ¯ãƒ¼ã‚¹ãƒˆ5ç”»åƒã‚³ãƒ”ãƒ¼ãƒ»ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            worst_dir = visual_dir / "worst_5"
            worst_dir.mkdir(exist_ok=True)
            
            for item in summary.worst_items:
                self.create_annotated_image(item, worst_dir)
            
            # ã‚µãƒãƒªãƒ¼ç”»åƒä½œæˆ
            self.create_summary_image(summary, visual_dir)
            
            # READMEä½œæˆ
            self.create_batch_readme(summary, visual_dir)
            
            logger.info(f"å¯è¦–åŒ–ãƒ†ã‚¹ãƒˆãƒãƒƒãƒä½œæˆ: {visual_dir}")
            
        except Exception as e:
            logger.error(f"å¯è¦–åŒ–ãƒãƒƒãƒä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def create_annotated_image(self, item: TestBatchItem, output_dir: Path):
        """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãç”»åƒä½œæˆ"""
        try:
            # å…ƒç”»åƒèª­ã¿è¾¼ã¿
            if not Path(item.image_path).exists():
                # test_smallãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¤œç´¢
                possible_paths = [
                    self.project_root / "test_small" / f"{item.image_id}.png",
                    self.project_root / "test_small" / f"{item.image_id}.jpg",
                    self.project_root / "test_small" / f"{item.image_id}.jpeg"
                ]
                
                image_path = None
                for path in possible_paths:
                    if path.exists():
                        image_path = path
                        break
                
                if image_path is None:
                    logger.warning(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {item.image_id}")
                    return
            else:
                image_path = Path(item.image_path)
            
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(str(image_path))
            if image is None:
                logger.warning(f"ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {image_path}")
                return
            
            # BGR -> RGBå¤‰æ›
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # matplotlibå›³ä½œæˆ
            fig, ax = plt.subplots(1, 1, figsize=(12, 8))
            ax.imshow(image_rgb)
            
            # Ground Truth bbox (ç·‘è‰²)
            gt_bbox = item.ground_truth_bbox
            if gt_bbox != (0, 0, 0, 0):
                gt_rect = patches.Rectangle(
                    (gt_bbox[0], gt_bbox[1]), gt_bbox[2], gt_bbox[3],
                    linewidth=3, edgecolor='green', facecolor='none',
                    label='Ground Truth'
                )
                ax.add_patch(gt_rect)
            
            # Predicted bbox (èµ¤è‰²)
            if item.predicted_bbox:
                pred_bbox = item.predicted_bbox
                pred_rect = patches.Rectangle(
                    (pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3],
                    linewidth=3, edgecolor='red', facecolor='none',
                    label='Prediction'
                )
                ax.add_patch(pred_rect)
            
            # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æƒ…å ±è¿½åŠ 
            title = (f"Rank {item.rank} ({item.category.upper()}) - {item.image_id}\n"
                    f"Claude Score: {item.claude_score:.3f}, IoU: {item.iou_score:.3f}, "
                    f"Grade: {item.quality_grade}")
            ax.set_title(title, fontsize=12, fontweight='bold')
            
            # å•é¡Œç‚¹ãƒ†ã‚­ã‚¹ãƒˆè¿½åŠ 
            if item.issues:
                issues_text = "Issues: " + "; ".join(item.issues[:3])  # æœ€å¤§3ä»¶è¡¨ç¤º
                ax.text(10, image_rgb.shape[0] - 30, issues_text, 
                       fontsize=10, color='red', bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))
            
            # å‡¡ä¾‹è¿½åŠ 
            ax.legend(loc='upper right')
            ax.axis('off')
            
            # ä¿å­˜
            output_file = output_dir / f"{item.rank:02d}_{item.image_id}_score{item.claude_score:.3f}.png"
            plt.savefig(output_file, dpi=150, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”»åƒä½œæˆã‚¨ãƒ©ãƒ¼ ({item.image_id}): {e}")
    
    def create_summary_image(self, summary: TestBatchSummary, output_dir: Path):
        """ã‚µãƒãƒªãƒ¼ç”»åƒä½œæˆ"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle(f'Test Batch Summary - {summary.phase.upper()} ({summary.timestamp})', 
                        fontsize=16, fontweight='bold')
            
            # 1. ã‚¹ã‚³ã‚¢åˆ†å¸ƒæ¯”è¼ƒ
            ax1 = axes[0, 0]
            best_scores = [item.claude_score for item in summary.best_items]
            worst_scores = [item.claude_score for item in summary.worst_items]
            
            x_pos = np.arange(5)
            width = 0.35
            
            bars1 = ax1.bar(x_pos - width/2, best_scores, width, label='Best 5', color='green', alpha=0.7)
            bars2 = ax1.bar(x_pos + width/2, worst_scores, width, label='Worst 5', color='red', alpha=0.7)
            
            ax1.set_title('Claude Score Comparison')
            ax1.set_xlabel('Rank')
            ax1.set_ylabel('Claude Score')
            ax1.set_xticks(x_pos)
            ax1.set_xticklabels([f'#{i+1}' for i in range(5)])
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # ãƒãƒ¼ã«æ•°å€¤è¡¨ç¤º
            for bar in bars1:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{height:.2f}', ha='center', va='bottom', fontsize=8)
            
            for bar in bars2:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{height:.2f}', ha='center', va='bottom', fontsize=8)
            
            # 2. IoU vs Confidenceæ•£å¸ƒå›³
            ax2 = axes[0, 1]
            
            best_iou = [item.iou_score for item in summary.best_items]
            best_conf = [item.confidence for item in summary.best_items]
            worst_iou = [item.iou_score for item in summary.worst_items]
            worst_conf = [item.confidence for item in summary.worst_items]
            
            ax2.scatter(best_iou, best_conf, c='green', s=100, alpha=0.7, label='Best 5')
            ax2.scatter(worst_iou, worst_conf, c='red', s=100, alpha=0.7, label='Worst 5')
            
            ax2.set_title('IoU vs Confidence')
            ax2.set_xlabel('IoU Score')
            ax2.set_ylabel('Confidence')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # 3. å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
            ax3 = axes[1, 0]
            
            all_items = summary.best_items + summary.worst_items
            grades = [item.quality_grade for item in all_items]
            grade_counts = {grade: grades.count(grade) for grade in ['A', 'B', 'C', 'D', 'E', 'F']}
            
            colors = ['#2ecc71', '#27ae60', '#f39c12', '#e67e22', '#e74c3c', '#c0392b']
            wedges, texts, autotexts = ax3.pie(
                list(grade_counts.values()), 
                labels=list(grade_counts.keys()),
                colors=colors,
                autopct='%1.0f%%',
                startangle=90
            )
            ax3.set_title('Quality Grade Distribution')
            
            # 4. ä¸»è¦æ´å¯Ÿãƒ†ã‚­ã‚¹ãƒˆ
            ax4 = axes[1, 1]
            ax4.axis('off')
            
            insights_text = "Key Insights:\n\n"
            for i, insight in enumerate(summary.key_insights, 1):
                insights_text += f"{i}. {insight}\n\n"
            
            insights_text += f"\nScore Range: {summary.score_range[0]:.3f} - {summary.score_range[1]:.3f}\n"
            insights_text += f"Best 5 Avg: {summary.avg_score_best:.3f}\n"
            insights_text += f"Worst 5 Avg: {summary.avg_score_worst:.3f}"
            
            ax4.text(0.05, 0.95, insights_text, transform=ax4.transAxes,
                    fontsize=11, verticalalignment='top',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor='lightgray', alpha=0.8))
            
            plt.tight_layout()
            
            # ä¿å­˜
            summary_file = output_dir / "test_batch_summary.png"
            plt.savefig(summary_file, dpi=150, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ã‚µãƒãƒªãƒ¼ç”»åƒä½œæˆ: {summary_file}")
            
        except Exception as e:
            logger.error(f"ã‚µãƒãƒªãƒ¼ç”»åƒä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def create_batch_readme(self, summary: TestBatchSummary, output_dir: Path):
        """ãƒ†ã‚¹ãƒˆãƒãƒƒãƒREADMEä½œæˆ"""
        try:
            readme_content = f"""# Test Batch Report - {summary.phase.upper()}

**ç”Ÿæˆæ—¥æ™‚**: {summary.timestamp}  
**Phase**: {summary.phase}  
**ç·ã‚¢ã‚¤ãƒ†ãƒ æ•°**: {summary.total_items}

---

## ğŸ“Š æ¦‚è¦

### ã‚¹ã‚³ã‚¢çµ±è¨ˆ
- **ã‚¹ã‚³ã‚¢ç¯„å›²**: {summary.score_range[0]:.3f} - {summary.score_range[1]:.3f}
- **ãƒ™ã‚¹ãƒˆ5å¹³å‡**: {summary.avg_score_best:.3f}
- **ãƒ¯ãƒ¼ã‚¹ãƒˆ5å¹³å‡**: {summary.avg_score_worst:.3f}

### ä¸»è¦æ´å¯Ÿ
"""
            
            for i, insight in enumerate(summary.key_insights, 1):
                readme_content += f"{i}. {insight}\n"
            
            readme_content += f"""

---

## ğŸ† ãƒ™ã‚¹ãƒˆ5ï¼ˆClaudeãŒé«˜è©•ä¾¡ï¼‰

"""
            
            for item in summary.best_items:
                readme_content += f"""### #{item.rank} - {item.image_id}
- **Claudeã‚¹ã‚³ã‚¢**: {item.claude_score:.3f}
- **IoU**: {item.iou_score:.3f}
- **ä¿¡é ¼åº¦**: {item.confidence:.3f}
- **å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰**: {item.quality_grade}
- **å•é¡Œç‚¹**: {', '.join(item.issues) if item.issues else 'ãªã—'}

"""
            
            readme_content += f"""---

## ğŸ’¥ ãƒ¯ãƒ¼ã‚¹ãƒˆ5ï¼ˆClaudeãŒä½è©•ä¾¡ï¼‰

"""
            
            for item in summary.worst_items:
                readme_content += f"""### #{item.rank} - {item.image_id}
- **Claudeã‚¹ã‚³ã‚¢**: {item.claude_score:.3f}
- **IoU**: {item.iou_score:.3f}
- **ä¿¡é ¼åº¦**: {item.confidence:.3f}
- **å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰**: {item.quality_grade}
- **å•é¡Œç‚¹**: {', '.join(item.issues) if item.issues else 'ãªã—'}

"""
            
            readme_content += f"""---

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
{output_dir.name}/
â”œâ”€â”€ best_5/          # ãƒ™ã‚¹ãƒˆ5ç”»åƒï¼ˆã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
â”œâ”€â”€ worst_5/         # ãƒ¯ãƒ¼ã‚¹ãƒˆ5ç”»åƒï¼ˆã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
â”œâ”€â”€ test_batch_summary.png  # çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”»åƒ
â””â”€â”€ README.md        # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

---

## ğŸ¯ äººé–“è©•ä¾¡ã¨ã®æ¯”è¼ƒè¦³ç‚¹

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
1. **ãƒ™ã‚¹ãƒˆ5**: ClaudeãŒé«˜è©•ä¾¡ã—ãŸç”»åƒã¯å®Ÿéš›ã«è‰¯ã„æŠ½å‡ºçµæœã‹ï¼Ÿ
2. **ãƒ¯ãƒ¼ã‚¹ãƒˆ5**: ClaudeãŒä½è©•ä¾¡ã—ãŸç”»åƒã¯å®Ÿéš›ã«å•é¡ŒãŒã‚ã‚‹ã‹ï¼Ÿ
3. **èªè­˜ã®ä¹–é›¢**: äººé–“ã®è©•ä¾¡ã¨Claudeè©•ä¾¡ã§å¤§ããç•°ãªã‚‹ã‚±ãƒ¼ã‚¹ã¯ï¼Ÿ

### æ”¹å–„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
- ãƒ™ã‚¹ãƒˆ5ã§å•é¡ŒãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ â†’ è©•ä¾¡åŸºæº–ã®èª¿æ•´ãŒå¿…è¦
- ãƒ¯ãƒ¼ã‚¹ãƒˆ5ã§è‰¯ã„ã‚±ãƒ¼ã‚¹ â†’ ã‚¹ã‚³ã‚¢ç®—å‡ºæ–¹æ³•ã®æ”¹å–„ãŒå¿…è¦
- ä¸€è²«ã—ã¦å•é¡ŒãŒã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ â†’ ã‚·ã‚¹ãƒ†ãƒ è‡ªä½“ã®æ”¹å–„ãŒå¿…è¦

---

*Generated by Test Batch Generator v1.0*
"""
            
            readme_file = output_dir / "README.md"
            with open(readme_file, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            
            logger.info(f"READMEä½œæˆ: {readme_file}")
            
        except Exception as e:
            logger.error(f"READMEä½œæˆã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    logging.basicConfig(level=logging.INFO)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    project_root = Path("/mnt/c/AItools/segment-anything")
    generator = TestBatchGenerator(project_root)
    
    # ã‚µãƒ³ãƒ—ãƒ«çµæœãƒ‡ãƒ¼ã‚¿
    sample_results = []
    for i in range(20):
        result = {
            'image_id': f'kana08_{i:04d}',
            'image_path': f'/test_small/kana08_{i:04d}.png',
            'largest_char_predicted': np.random.random() > 0.4,
            'iou_score': np.random.uniform(0.0, 1.0),
            'confidence_score': np.random.uniform(0.1, 0.9),
            'processing_time': np.random.uniform(3.0, 12.0),
            'character_count': np.random.randint(1, 6),
            'area_largest_ratio': np.random.uniform(0.2, 0.8),
            'quality_grade': np.random.choice(['A', 'B', 'C', 'D', 'E', 'F'], 
                                            p=[0.05, 0.15, 0.25, 0.25, 0.20, 0.10]),
            'prediction_bbox': (
                int(np.random.uniform(50, 200)),
                int(np.random.uniform(50, 200)),
                int(np.random.uniform(100, 300)),
                int(np.random.uniform(150, 400))
            ) if np.random.random() > 0.2 else None
        }
        sample_results.append(result)
    
    # ãƒ†ã‚¹ãƒˆãƒãƒƒãƒç”Ÿæˆ
    summary = generator.generate_test_batch(sample_results, "phase0")
    
    print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒãƒƒãƒç”Ÿæˆå®Œäº†")
    print(f"ãƒ™ã‚¹ãƒˆ5å¹³å‡ã‚¹ã‚³ã‚¢: {summary.avg_score_best:.3f}")
    print(f"ãƒ¯ãƒ¼ã‚¹ãƒˆ5å¹³å‡ã‚¹ã‚³ã‚¢: {summary.avg_score_worst:.3f}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {generator.output_dir}")


if __name__ == "__main__":
    main()