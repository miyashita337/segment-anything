#!/usr/bin/env python3
"""
Character Extraction Command
Main command for extracting characters from manga images using SAM + YOLO
"""

import os
import sys
import argparse
import time
from pathlib import Path
from typing import Optional, Dict, Any, List

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

import cv2
import numpy as np

from features.common.hooks.start import get_sam_model, get_yolo_model, get_performance_monitor
from features.evaluation.utils.difficult_pose import (
    DifficultPoseProcessor, 
    detect_difficult_pose, 
    get_difficult_pose_config,
    process_with_retry
)
from features.processing.preprocessing.preprocessing import preprocess_image_pipeline
from features.processing.postprocessing.postprocessing import (
    enhance_character_mask, 
    extract_character_from_image, 
    crop_to_content,
    save_character_result,
    calculate_mask_quality_metrics
)
from features.evaluation.utils.text_detection import TextDetector
from features.evaluation.utils.learned_quality_assessment import assess_image_quality, LearnedQualityAssessment
from features.evaluation.utils.partial_extraction_detector import PartialExtractionDetector, analyze_extraction_completeness


class CharacterExtractor:
    """
    Character Extraction Wrapper Class
    Provides class-based interface for character extraction functionality
    Phase 0ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œ: ä¾å­˜é–¢ä¿‚å•é¡Œã®è§£æ±º
    """
    
    def __init__(self):
        """Initialize character extractor with default settings"""
        self.default_settings = {
            'enhance_contrast': False,
            'filter_text': True,
            'save_mask': False,
            'save_transparent': False,
            'min_yolo_score': 0.1,
            'verbose': True,
            'difficult_pose': False,
            'low_threshold': False,
            'auto_retry': False,
            'high_quality': False
        }
    
    def extract(self, image_path: str, output_path: str = None, **kwargs):
        """
        Extract character from image
        
        Args:
            image_path: Path to input image
            output_path: Path for output (optional)
            **kwargs: Additional extraction parameters
            
        Returns:
            Result dictionary with success status and paths
        """
        # Merge default settings with provided kwargs
        settings = {**self.default_settings, **kwargs}
        
        # Call the main extraction function
        return extract_character_from_path(
            image_path=image_path,
            output_path=output_path,
            **settings
        )
    
    def batch_extract(self, input_dir: str, output_dir: str, **kwargs):
        """
        Batch extract characters from directory
        
        Args:
            input_dir: Input directory path
            output_dir: Output directory path
            **kwargs: Additional extraction parameters
            
        Returns:
            Batch processing results
        """
        settings = {**self.default_settings, **kwargs}
        
        return batch_extract_characters(
            input_dir=input_dir,
            output_dir=output_dir,
            **settings
        )


def extract_character_from_path(image_path: str,
                               output_path: Optional[str] = None,
                               enhance_contrast: bool = False,
                               filter_text: bool = True,
                               save_mask: bool = False,
                               save_transparent: bool = False,
                               min_yolo_score: float = 0.1,
                               verbose: bool = True,
                               difficult_pose: bool = False,
                               low_threshold: bool = False,
                               auto_retry: bool = False,
                               high_quality: bool = False,
                               manga_mode: bool = False,
                               effect_removal: bool = False,
                               panel_split: bool = False,
                               multi_character_criteria: str = 'balanced',
                               adaptive_learning: bool = False,
                               use_box_expansion: bool = False,
                               expansion_strategy: str = 'balanced',
                               **kwargs) -> Dict[str, Any]:
    """
    ç”»åƒãƒ‘ã‚¹ã‹ã‚‰ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’æŠ½å‡º (Phase Aå¯¾å¿œç‰ˆ)
    
    Args:
        image_path: å…¥åŠ›ç”»åƒãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ‘ã‚¹ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        enhance_contrast: ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–
        filter_text: ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        save_mask: ãƒã‚¹ã‚¯ã‚’ä¿å­˜
        save_transparent: é€æ˜èƒŒæ™¯ç‰ˆã‚’ä¿å­˜
        min_yolo_score: YOLOæœ€å°ã‚¹ã‚³ã‚¢
        verbose: è©³ç´°å‡ºåŠ›
        difficult_pose: è¤‡é›‘ãƒãƒ¼ã‚ºãƒ¢ãƒ¼ãƒ‰
        low_threshold: ä½é–¾å€¤ãƒ¢ãƒ¼ãƒ‰ï¼ˆYOLO 0.02ï¼‰
        auto_retry: è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ¢ãƒ¼ãƒ‰
        high_quality: é«˜å“è³ªSAMå‡¦ç†
        manga_mode: æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ (Phase 2)
        effect_removal: ã‚¨ãƒ•ã‚§ã‚¯ãƒˆç·šé™¤å»ã‚’æœ‰åŠ¹åŒ– (Phase 2)
        panel_split: ãƒãƒ«ãƒã‚³ãƒåˆ†å‰²ã‚’æœ‰åŠ¹åŒ– (Phase 2)
        multi_character_criteria: è¤‡æ•°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é¸æŠåŸºæº– ('balanced', 'size_priority', 'fullbody_priority', 'fullbody_priority_enhanced', 'central_priority', 'confidence_priority')
        adaptive_learning: é©å¿œå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆ281è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€é©æ‰‹æ³•é¸æŠï¼‰
        use_box_expansion: GPT-4Oæ¨å¥¨ãƒœãƒƒã‚¯ã‚¹æ‹¡å¼µã‚’æœ‰åŠ¹åŒ– (Phase A)
        expansion_strategy: æ‹¡å¼µæˆ¦ç•¥ ('conservative', 'balanced', 'aggressive') (Phase A)
        
    Returns:
        æŠ½å‡ºçµæœã®è¾æ›¸
    """
    result = {
        'success': False,
        'input_path': image_path,
        'output_path': None,
        'processing_time': 0.0,
        'mask_quality': {},
        'error': None,
        'adaptive_learning_info': None
    }
    
    start_time = time.time()
    
    # è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ process_with_retry ã‚’ä½¿ç”¨
    if auto_retry:
        if verbose:
            print(f"ğŸ”„ è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ¢ãƒ¼ãƒ‰ã§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºé–‹å§‹: {image_path}")
        
        def extract_function(img_path, **config):
            # å…ƒã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—ï¼ˆãƒªãƒˆãƒ©ã‚¤ç”¨ï¼‰
            return extract_character_from_path(
                img_path, output_path, enhance_contrast, filter_text,
                save_mask, save_transparent, config.get('min_yolo_score', min_yolo_score),
                verbose=False,  # ãƒªãƒˆãƒ©ã‚¤ä¸­ã¯è©³ç´°å‡ºåŠ›ã‚’æŠ‘åˆ¶
                difficult_pose=False, low_threshold=False, auto_retry=False,  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
                high_quality=config.get('enable_enhanced_processing', high_quality),
                manga_mode=config.get('enable_manga_preprocessing', manga_mode),
                effect_removal=config.get('enable_effect_removal', effect_removal),
                panel_split=config.get('enable_panel_split', panel_split),
                multi_character_criteria=multi_character_criteria,
                adaptive_learning=adaptive_learning,
                use_box_expansion=use_box_expansion,  # Phase A
                expansion_strategy=expansion_strategy,  # Phase A
                **{k: v for k, v in config.items() if k not in [
                    'min_yolo_score', 'enable_enhanced_processing', 'enable_manga_preprocessing',
                    'enable_effect_removal', 'enable_panel_split'
                ]}
            )
        
        return process_with_retry(image_path, extract_function, max_retries=4)
    
    try:
        # Phase 3: é©å¿œå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ - 281è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€é©æ‰‹æ³•é¸æŠ
        if adaptive_learning:
            if verbose:
                print(f"ğŸ§  é©å¿œå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: 281è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€é©æ‰‹æ³•é¸æŠã‚’å®Ÿè¡Œä¸­...")
            
            try:
                # å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã§ç”»åƒç‰¹æ€§ã‚’åˆ†æã—æœ€é©æ‰‹æ³•ã‚’äºˆæ¸¬
                quality_prediction = assess_image_quality(image_path)
                result['adaptive_learning_info'] = {
                    'predicted_quality': quality_prediction.predicted_quality,
                    'confidence': quality_prediction.confidence,
                    'recommended_method': quality_prediction.recommended_method,
                    'fallback_method': quality_prediction.fallback_method,
                    'reasoning': quality_prediction.reasoning,
                    'image_characteristics': quality_prediction.image_characteristics
                }
                
                # æ¨å¥¨æ‰‹æ³•ã‚’multi_character_criteriaã«é©ç”¨
                multi_character_criteria = quality_prediction.recommended_method
                
                # ImageCharacteristicsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                from utils.learned_quality_assessment import ImageCharacteristics
                img_chars_dict = quality_prediction.image_characteristics
                img_chars = ImageCharacteristics(**img_chars_dict) if isinstance(img_chars_dict, dict) else img_chars_dict
                
                # ç”»åƒç‰¹æ€§ã«åŸºã¥ãæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
                assessor = LearnedQualityAssessment()
                optimized_params = assessor.get_method_parameters(
                    quality_prediction.recommended_method,
                    img_chars
                )
                
                # æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©ç”¨
                if optimized_params.get('score_threshold'):
                    min_yolo_score = optimized_params['score_threshold']
                
                # å¢ƒç•Œå•é¡ŒãŒã‚ã‚‹å ´åˆã¯æ¼«ç”»å‰å‡¦ç†ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–ï¼ˆä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
                if img_chars.has_boundary_complexity:
                    # manga_mode = True
                    # effect_removal = True
                    if verbose:
                        print(f"   ğŸ¨ å¢ƒç•Œå•é¡Œæ¤œå‡º: æ¼«ç”»å‰å‡¦ç†ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–ï¼ˆç„¡åŠ¹åŒ–ä¸­ï¼‰")
                
                if verbose:
                    print(f"   ğŸ“Š æ¨å¥¨æ‰‹æ³•: {quality_prediction.recommended_method}")
                    print(f"   ğŸ¯ äºˆæ¸¬å“è³ª: {quality_prediction.predicted_quality:.3f}")
                    print(f"   ğŸ”§ ä¿¡é ¼åº¦: {quality_prediction.confidence:.3f}")
                    print(f"   ğŸ“ ç†ç”±: {quality_prediction.reasoning}")
                    print(f"   âš™ï¸  æœ€é©YOLOé–¾å€¤: {min_yolo_score}")
                    
                    # ç”»åƒç‰¹æ€§ã®è©³ç´°è¡¨ç¤º
                    if img_chars.has_complex_pose:
                        print(f"   ğŸ¤¸ è¤‡é›‘å§¿å‹¢æ¤œå‡º")
                    if img_chars.has_multiple_characters:
                        print(f"   ğŸ‘¥ è¤‡æ•°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼")
                    if img_chars.has_screentone_issues:
                        print(f"   ğŸ“° ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ãƒˆãƒ¼ãƒ³å¢ƒç•Œå•é¡Œ")
                    if img_chars.has_mosaic_issues:
                        print(f"   ğŸ”² ãƒ¢ã‚¶ã‚¤ã‚¯å¢ƒç•Œå•é¡Œ")
                
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ é©å¿œå­¦ç¿’ã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‰‹æ³•ã§ç¶™ç¶š: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‰‹æ³•ã‚’ç¶™ç¶šä½¿ç”¨
                result['adaptive_learning_info'] = {
                    'error': str(e),
                    'fallback_to_default': True
                }
        
        # Get models
        sam_model = get_sam_model()
        yolo_model = get_yolo_model()
        performance_monitor = get_performance_monitor()
        
        if not sam_model or not yolo_model:
            if verbose:
                print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«æœªåˆæœŸåŒ–ã€è‡ªå‹•åˆæœŸåŒ–ã‚’å®Ÿè¡Œä¸­...")
            
            # æ–°æ§‹é€ å¯¾å¿œã®è‡ªå‹•åˆæœŸåŒ–
            try:
                # Phase 0å¾Œã®æ–°ãƒ‘ã‚¹ã§ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
                from features.common.hooks.start import initialize_models
                initialize_models()
                
                # å†åº¦ãƒ¢ãƒ‡ãƒ«å–å¾—ã‚’è©¦è¡Œ
                sam_model = get_sam_model()
                yolo_model = get_yolo_model()
                performance_monitor = get_performance_monitor()
                
                if verbose:
                    print("âœ… ãƒ¢ãƒ‡ãƒ«è‡ªå‹•åˆæœŸåŒ–å®Œäº†ï¼ˆæ–°æ§‹é€ å¯¾å¿œï¼‰")
                
                if not sam_model or not yolo_model:
                    raise RuntimeError("Auto initialization failed. Models still not available.")
                    
            except ImportError as e:
                # Phase 0æ–°æ§‹é€ ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if verbose:
                    print(f"âš ï¸ è‡ªå‹•åˆæœŸåŒ–å¤±æ•—: {e}")
                raise RuntimeError(f"Models not initialized. Please run: python3 features/common/hooks/start.py\nError: {e}")
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ åˆæœŸåŒ–ä¾‹å¤–: {e}")
                raise RuntimeError(f"Failed to auto-initialize models: {e}")
        
        # è¤‡é›‘ãƒãƒ¼ã‚ºåˆ¤å®šã¨è¨­å®šèª¿æ•´ (Phase 2å¯¾å¿œç‰ˆ)
        if difficult_pose or low_threshold or manga_mode:
            processor = DifficultPoseProcessor()
            
            if difficult_pose:
                # è¤‡é›‘ãƒãƒ¼ã‚ºãƒ¢ãƒ¼ãƒ‰: è‡ªå‹•åˆ¤å®šã«ã‚ˆã‚‹è¨­å®š
                complexity_info = processor.detect_pose_complexity(image_path)
                recommended_config = processor.get_recommended_config(complexity_info)
                
                if verbose:
                    print(f"ğŸ” ãƒãƒ¼ã‚ºè¤‡é›‘åº¦: {complexity_info['complexity']} (ã‚¹ã‚³ã‚¢: {complexity_info['score']:.1f})")
                    print(f"ğŸ”§ æ¨å¥¨è¨­å®šé©ç”¨: {recommended_config['description']}")
                
                # æ¨å¥¨è¨­å®šã‚’é©ç”¨
                min_yolo_score = min(min_yolo_score, recommended_config['min_yolo_score'])
                if 'sam_points_per_side' in recommended_config:
                    # SAMè¨­å®šã‚’kwargsã«è¿½åŠ 
                    kwargs.update({
                        'sam_points_per_side': recommended_config['sam_points_per_side'],
                        'sam_pred_iou_thresh': recommended_config['sam_pred_iou_thresh'],
                        'sam_stability_score_thresh': recommended_config['sam_stability_score_thresh']
                    })
            
            if low_threshold:
                min_yolo_score = 0.02
                if verbose:
                    print(f"ğŸ”§ ä½é–¾å€¤ãƒ¢ãƒ¼ãƒ‰: YOLOé–¾å€¤ã‚’{min_yolo_score}ã«è¨­å®š")
            
            # Phase 2: æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
            if manga_mode or effect_removal or panel_split:
                if verbose:
                    print(f"ğŸ¨ æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
                    print(f"   ã‚¨ãƒ•ã‚§ã‚¯ãƒˆç·šé™¤å»: {'âœ…' if effect_removal else 'âŒ'}")
                    print(f"   ãƒãƒ«ãƒã‚³ãƒåˆ†å‰²: {'âœ…' if panel_split else 'âŒ'}")
                
                # å‰å‡¦ç†ã‚’é©ç”¨
                processed_image_path = processor.preprocess_for_difficult_pose(
                    image_path,
                    enable_manga_preprocessing=True,
                    enable_effect_removal=effect_removal,
                    enable_panel_split=panel_split
                )
                
                # å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ä½¿ç”¨
                image_path = processed_image_path
        
        if high_quality:
            # é«˜å“è³ªSAMè¨­å®š
            kwargs.update({
                'sam_points_per_side': kwargs.get('sam_points_per_side', 64),
                'sam_pred_iou_thresh': kwargs.get('sam_pred_iou_thresh', 0.88),
                'sam_stability_score_thresh': kwargs.get('sam_stability_score_thresh', 0.92)
            })
            if verbose:
                print(f"ğŸ”§ é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰: SAMå¯†åº¦ {kwargs.get('sam_points_per_side', 64)} ãƒã‚¤ãƒ³ãƒˆ/ã‚µã‚¤ãƒ‰")
        
        if verbose:
            print(f"ğŸ¯ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºé–‹å§‹: {image_path}")
            print(f"ğŸ“Š YOLOé–¾å€¤: {min_yolo_score}")
        
        # Step 1: Image preprocessing
        performance_monitor.start_stage("Image Preprocessing")
        bgr_image, rgb_image, scale = preprocess_image_pipeline(
            image_path, 
            enhance_contrast=enhance_contrast
        )
        
        if rgb_image is None:
            raise ValueError(f"Failed to load image: {image_path}")
        
        performance_monitor.end_stage()
        
        # Step 2: SAM mask generation
        performance_monitor.start_stage("SAM Mask Generation")
        
        # é«˜å“è³ª/è¤‡é›‘ãƒãƒ¼ã‚ºãƒ¢ãƒ¼ãƒ‰ã§SAMè¨­å®šã‚’å‹•çš„ã«é©ç”¨
        if any(key.startswith('sam_') for key in kwargs.keys()) or high_quality:
            # SAMGeneratorã‚’ä¸€æ™‚çš„ã«å†æ§‹ç¯‰
            try:
                from segment_anything import SamAutomaticMaskGenerator
                
                sam_params = {
                    'model': sam_model.sam,
                    'points_per_side': kwargs.get('sam_points_per_side', 32),
                    'pred_iou_thresh': kwargs.get('sam_pred_iou_thresh', 0.8),
                    'stability_score_thresh': kwargs.get('sam_stability_score_thresh', 0.85),
                    'crop_n_layers': 1,
                    'crop_n_points_downscale_factor': 2,
                    'min_mask_region_area': 100,
                }
                
                if verbose:
                    print(f"ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ SAMè¨­å®šé©ç”¨:")
                    print(f"   ãƒã‚¤ãƒ³ãƒˆå¯†åº¦: {sam_params['points_per_side']}")
                    print(f"   IoUé–¾å€¤: {sam_params['pred_iou_thresh']}")
                    print(f"   å®‰å®šæ€§é–¾å€¤: {sam_params['stability_score_thresh']}")
                
                # ä¸€æ™‚çš„ãªãƒã‚¹ã‚¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã§å‡¦ç†
                temp_generator = SamAutomaticMaskGenerator(**sam_params)
                all_masks = temp_generator.generate(rgb_image)
                
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ ã‚«ã‚¹ã‚¿ãƒ SAMè¨­å®šå¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ç¶™ç¶š: {e}")
                all_masks = sam_model.generate_masks(rgb_image)
        else:
            all_masks = sam_model.generate_masks(rgb_image)
        
        if not all_masks:
            raise ValueError("No masks generated by SAM")
        
        character_masks = sam_model.filter_character_masks(all_masks)
        
        if verbose:
            print(f"ğŸ“Š ç”Ÿæˆãƒã‚¹ã‚¯: {len(all_masks)} â†’ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å€™è£œ: {len(character_masks)}")
        
        performance_monitor.end_stage()
        
        # Step 3: YOLO scoring (GPT-4Oæ¨å¥¨ãƒœãƒƒã‚¯ã‚¹æ‹¡å¼µå¯¾å¿œ)
        performance_monitor.start_stage("YOLO Scoring")
        
        # GPT-4Oæ¨å¥¨ãƒœãƒƒã‚¯ã‚¹æ‹¡å¼µã‚ªãƒ—ã‚·ãƒ§ãƒ³
        use_box_expansion = kwargs.get('use_box_expansion', False)
        expansion_strategy = kwargs.get('expansion_strategy', 'balanced')
        
        if use_box_expansion:
            if verbose:
                print(f"ğŸ¯ GPT-4Oæ¨å¥¨ãƒœãƒƒã‚¯ã‚¹æ‹¡å¼µã‚’æœ‰åŠ¹åŒ–: æˆ¦ç•¥={expansion_strategy}")
                print(f"   æ°´å¹³æ‹¡å¼µ: 2.5-3å€ã€å‚ç›´æ‹¡å¼µ: 4å€")
        
        scored_masks = yolo_model.score_masks_with_detections(
            character_masks, 
            bgr_image,
            use_expanded_boxes=use_box_expansion,
            expansion_strategy=expansion_strategy
        )
        
        # Phase 1 P1-003: æ”¹è‰¯ç‰ˆå…¨èº«æ¤œå‡ºã®çµ±åˆ
        if multi_character_criteria == 'fullbody_priority_enhanced':
            selection_result = yolo_model.select_best_mask_with_criteria(
                scored_masks, 
                bgr_image,
                criteria=multi_character_criteria
            )
            if selection_result is not None:
                best_mask, quality_score = selection_result
                if verbose:
                    print(f"ğŸ” æ”¹è‰¯ç‰ˆå…¨èº«æ¤œå‡ºä½¿ç”¨: å“è³ªã‚¹ã‚³ã‚¢={quality_score:.3f}")
            else:
                best_mask = None
        else:
            best_mask = yolo_model.get_best_character_mask(
                scored_masks, 
                bgr_image, 
                min_yolo_score=min_yolo_score
            )
        
        if best_mask is None:
            raise ValueError(f"No good character masks found (min YOLO score: {min_yolo_score})")
        
        if verbose:
            print(f"ğŸ¯ æœ€é©ãƒã‚¹ã‚¯é¸æŠ: YOLO score={best_mask['yolo_score']:.3f}, "
                  f"combined score={best_mask['combined_score']:.3f}")
            if adaptive_learning:
                print(f"   ğŸ§  æ¨å¥¨æ‰‹æ³•: {multi_character_criteria} (é©å¿œå­¦ç¿’)")
            else:
                print(f"   ğŸ”§ é¸æŠåŸºæº–: {multi_character_criteria}")
        
        performance_monitor.end_stage()
        
        # Step 4: Text filtering (optional)
        if filter_text:
            performance_monitor.start_stage("Text Filtering")
            text_detector = TextDetector(use_easyocr=True)
            
            text_density = text_detector.calculate_text_density_score(
                bgr_image, 
                best_mask['bbox']
            )
            
            if text_density > 0.5:
                if verbose:
                    print(f"âš ï¸ é«˜ãƒ†ã‚­ã‚¹ãƒˆå¯†åº¦æ¤œå‡º: {text_density:.3f} - å‡¦ç†ç¶šè¡Œ")
            
            # Add text density to result
            best_mask['text_density'] = text_density
            performance_monitor.end_stage()
        
        # Step 5: Mask refinement
        performance_monitor.start_stage("Mask Refinement")
        raw_mask = sam_model.mask_to_binary(best_mask)
        
        # é©å¿œå­¦ç¿’ã«ã‚ˆã‚‹å¢ƒç•Œå•é¡Œå¯¾å¿œ + è¤‡é›‘ãƒãƒ¼ã‚ºç”¨ã®å¼·åŒ–ãƒã‚¹ã‚¯å‡¦ç†
        boundary_complexity = False
        if adaptive_learning and result['adaptive_learning_info'] and 'image_characteristics' in result['adaptive_learning_info']:
            img_chars_dict = result['adaptive_learning_info']['image_characteristics']
            boundary_complexity = img_chars_dict.get('has_boundary_complexity', False)
        
        use_enhanced_processing = (difficult_pose or low_threshold or high_quality or boundary_complexity)
        
        if use_enhanced_processing:
            if verbose:
                enhancement_reason = []
                if difficult_pose:
                    enhancement_reason.append("è¤‡é›‘ãƒãƒ¼ã‚º")
                if low_threshold:
                    enhancement_reason.append("ä½é–¾å€¤")
                if high_quality:
                    enhancement_reason.append("é«˜å“è³ª")
                if boundary_complexity:
                    enhancement_reason.append("å¢ƒç•Œå•é¡Œå¯¾å¿œ")
                
                print(f"ğŸ”§ ãƒã‚¹ã‚¯å¼·åŒ–å‡¦ç†ã‚’é©ç”¨: {'+'.join(enhancement_reason)}")
            
            # DifficultPoseProcessorã‚’ä½¿ç”¨ã—ãŸå¼·åŒ–å‡¦ç†
            if 'processor' not in locals():
                processor = DifficultPoseProcessor()
            
            enhanced_mask = processor.enhance_mask_for_complex_pose(raw_mask, bgr_image)
        else:
            enhanced_mask = enhance_character_mask(
                raw_mask,
                remove_small_area=100,
                smooth_kernel=3,
                fill_holes=True
            )
        
        # Calculate mask quality metrics
        quality_metrics = calculate_mask_quality_metrics(enhanced_mask)
        result['mask_quality'] = quality_metrics
        
        # Phase 1 P1-002: éƒ¨åˆ†æŠ½å‡ºæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹åˆ†æ
        performance_monitor.start_stage("Partial Extraction Analysis")
        try:
            partial_detector = PartialExtractionDetector()
            extraction_analysis = partial_detector.analyze_extraction(bgr_image, enhanced_mask)
            
            result['extraction_analysis'] = {
                'has_face': extraction_analysis.has_face,
                'has_torso': extraction_analysis.has_torso,
                'has_limbs': extraction_analysis.has_limbs,
                'completeness_score': extraction_analysis.completeness_score,
                'quality_assessment': extraction_analysis.quality_assessment,
                'issues_count': len(extraction_analysis.issues),
                'issues': [
                    {
                        'type': issue.issue_type,
                        'confidence': issue.confidence,
                        'severity': issue.severity,
                        'description': issue.description
                    } for issue in extraction_analysis.issues
                ]
            }
            
            if verbose:
                print(f"ğŸ” æŠ½å‡ºå®Œå…¨æ€§åˆ†æ: å®Œå…¨æ€§={extraction_analysis.completeness_score:.3f}, "
                      f"å“è³ª={extraction_analysis.quality_assessment}, å•é¡Œ={len(extraction_analysis.issues)}ä»¶")
                
                # é‡è¦ãªå•é¡Œã‚’è¡¨ç¤º
                high_severity_issues = [issue for issue in extraction_analysis.issues if issue.severity == 'high']
                if high_severity_issues:
                    for issue in high_severity_issues[:2]:  # æœ€å¤§2ä»¶è¡¨ç¤º
                        print(f"  âš ï¸ {issue.issue_type}: {issue.description}")
            
        except Exception as e:
            if verbose:
                print(f"âš ï¸ éƒ¨åˆ†æŠ½å‡ºåˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}")
            result['extraction_analysis'] = {
                'completeness_score': 0.5,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                'quality_assessment': 'unknown',
                'issues_count': 0,
                'error': str(e)
            }
        
        performance_monitor.end_stage()
        
        if verbose:
            print(f"ğŸ“ ãƒã‚¹ã‚¯å“è³ª: coverage={quality_metrics['coverage_ratio']:.3f}, "
                  f"compactness={quality_metrics['compactness']:.3f}")
        
        # Step 6: Character extraction
        performance_monitor.start_stage("Character Extraction")
        character_image = extract_character_from_image(
            bgr_image, 
            enhanced_mask,
            background_color=(0, 0, 0)  # Black background
        )
        
        # Crop to content
        cropped_character, cropped_mask, crop_bbox = crop_to_content(
            character_image,
            enhanced_mask,
            padding=10
        )
        
        performance_monitor.end_stage()
        
        # Step 7: Save results
        performance_monitor.start_stage("Saving Results")
        
        # Generate output path if not provided
        if output_path is None:
            input_path = Path(image_path)
            output_dir = input_path.parent / "character_output"
            output_dir.mkdir(exist_ok=True)
            output_path = output_dir / input_path.stem
        
        # Save results
        save_success = save_character_result(
            cropped_character,
            cropped_mask,
            str(output_path),
            save_mask=save_mask,
            save_transparent=save_transparent
        )
        
        if not save_success:
            raise RuntimeError("Failed to save results")
        
        result['output_path'] = str(output_path)
        performance_monitor.end_stage()
        
        # Success
        result['success'] = True
        result['processing_time'] = time.time() - start_time
        
        # é©å¿œå­¦ç¿’çµæœã®ãƒ­ã‚°è¨˜éŒ²
        if adaptive_learning and result['adaptive_learning_info']:
            try:
                assessor = LearnedQualityAssessment()
                # å®Ÿéš›ã®å“è³ªã‚’è¨ˆç®—ï¼ˆãƒã‚¹ã‚¯å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰æ¨å®šï¼‰
                actual_quality = (quality_metrics['coverage_ratio'] * 2 + 
                                quality_metrics['compactness'] * 2 + 1.0)  # 1-5ã‚¹ã‚±ãƒ¼ãƒ«æ¨å®š
                
                # äºˆæ¸¬çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²ï¼ˆå°†æ¥ã®å­¦ç¿’æ›´æ–°ç”¨ï¼‰
                assessor.log_prediction_result(
                    image_path, 
                    type('QualityPrediction', (), result['adaptive_learning_info'])(),
                    actual_quality=actual_quality
                )
                
                result['adaptive_learning_info']['estimated_actual_quality'] = actual_quality
                
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ é©å¿œå­¦ç¿’ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
        
        if verbose:
            print(f"âœ… ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºå®Œäº†: {result['processing_time']:.2f}ç§’")
            print(f"   å‡ºåŠ›: {result['output_path']}")
            
            # é©å¿œå­¦ç¿’çµæœã®ã‚µãƒãƒªè¡¨ç¤º
            if adaptive_learning and result['adaptive_learning_info'] and not result['adaptive_learning_info'].get('error'):
                adaptive_info = result['adaptive_learning_info']
                print(f"   ğŸ§  é©å¿œå­¦ç¿’çµæœ:")
                print(f"      æ‰‹æ³•: {adaptive_info['recommended_method']}")
                print(f"      äºˆæ¸¬å“è³ª: {adaptive_info['predicted_quality']:.3f}")
                if 'estimated_actual_quality' in adaptive_info:
                    print(f"      å®Ÿéš›å“è³ª: {adaptive_info['estimated_actual_quality']:.3f}")
                    prediction_error = abs(adaptive_info['predicted_quality'] - adaptive_info['estimated_actual_quality'])
                    print(f"      äºˆæ¸¬ç²¾åº¦: Â±{prediction_error:.3f}")
        
        return result
        
    except Exception as e:
        result['error'] = str(e)
        result['processing_time'] = time.time() - start_time
        
        if verbose:
            print(f"âŒ æŠ½å‡ºå¤±æ•—: {e}")
        
        return result


def batch_extract_characters(input_dir: str,
                           output_dir: str,
                           **extract_kwargs) -> Dict[str, Any]:
    """
    ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ç”»åƒã«å¯¾ã—ã¦ãƒãƒƒãƒå‡¦ç† (TDRå®‰å…¨å¯¾ç­–ç‰ˆ)
    
    Args:
        input_dir: å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        **extract_kwargs: extract_character_from_path ã®å¼•æ•°
        
    Returns:
        ãƒãƒƒãƒå‡¦ç†çµæœ
    """
    import torch
    import gc
    
    def gpu_memory_cleanup():
        """GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (TDRå¯¾ç­–)"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gc.collect()
        except Exception as e:
            print(f"âš ï¸ GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
    
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    
    if not input_path.exists():
        return {'success': False, 'error': f'Input directory not found: {input_dir}'}
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    image_files = []
    
    for ext in image_extensions:
        image_files.extend(input_path.glob(f'*{ext}'))
        image_files.extend(input_path.glob(f'*{ext.upper()}'))
    
    if not image_files:
        return {'success': False, 'error': f'No image files found in {input_dir}'}
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
    results = []
    successful = 0
    
    print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(image_files)} ç”»åƒ")
    print(f"ğŸ›¡ï¸ TDRå®‰å…¨å¯¾ç­–: GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æœ‰åŠ¹")
    
    for i, image_file in enumerate(image_files, 1):
        print(f"\nğŸ“ å‡¦ç†ä¸­ [{i}/{len(image_files)}]: {image_file.name}")
        
        try:
            # å‡ºåŠ›ãƒ‘ã‚¹ç”Ÿæˆ
            output_file = output_path / image_file.stem
            
            # æŠ½å‡ºå®Ÿè¡Œ
            batch_kwargs = extract_kwargs.copy()
            batch_kwargs['verbose'] = False
            
            result = extract_character_from_path(
                str(image_file),
                output_path=str(output_file),
                **batch_kwargs
            )
            
            result['filename'] = image_file.name
            results.append(result)
            
            if result['success']:
                successful += 1
                print(f"âœ… æˆåŠŸ: {image_file.name}")
            else:
                print(f"âŒ å¤±æ•—: {image_file.name} - {result.get('error', 'Unknown error')}")
            
            # 5æšã”ã¨ã«GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (TDRå¯¾ç­–)
            if i % 5 == 0:
                print(f"ğŸ§¹ GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ ({i}/{len(image_files)})")
                gpu_memory_cleanup()
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹å‡¦ç†ä¸­æ–­")
            gpu_memory_cleanup()
            break
        except Exception as e:
            print(f"âŒ ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {image_file.name} - {e}")
            # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ãƒãƒƒãƒå‡¦ç†ã¯ç¶™ç¶š
            error_result = {
                'success': False,
                'error': str(e),
                'filename': image_file.name,
                'processing_time': 0.0
            }
            results.append(error_result)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚GPU ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            gpu_memory_cleanup()
    
    # æœ€çµ‚GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    print("\nğŸ§¹ æœ€çµ‚GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—...")
    gpu_memory_cleanup()
    
    # çµæœã‚µãƒãƒª
    batch_result = {
        'success': True,
        'total_files': len(image_files),
        'successful': successful,
        'failed': len(image_files) - successful,
        'success_rate': successful / len(image_files) if len(image_files) > 0 else 0,
        'results': results
    }
    
    print(f"\nğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†:")
    print(f"   æˆåŠŸ: {successful}/{len(image_files)} ({batch_result['success_rate']:.1%})")
    print(f"   ğŸ›¡ï¸ TDRå¯¾ç­–: å®‰å…¨ã«GPUå‡¦ç†å®Œäº†")
    
    # Pushoveré€šçŸ¥é€ä¿¡
    try:
        from utils.notification import send_batch_notification
        print("\nğŸ“± é€šçŸ¥é€ä¿¡ä¸­...")
        notification_sent = send_batch_notification(
            successful=successful,
            total=len(image_files),
            failed=len(image_files) - successful,
            total_time=batch_result['total_time']
        )
        
        if notification_sent:
            print("âœ… Pushoveré€šçŸ¥é€ä¿¡å®Œäº†")
        else:
            print("âš ï¸ Pushoveré€šçŸ¥é€ä¿¡å¤±æ•—ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—")
    except ImportError:
        print("âš ï¸ é€šçŸ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except Exception as e:
        print(f"âš ï¸ é€šçŸ¥é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
    
    return batch_result


def main():
    """Main function for command line interface"""
    parser = argparse.ArgumentParser(description="Character Extraction using SAM + YOLO")
    
    parser.add_argument('input', help='Input image path or directory')
    parser.add_argument('-o', '--output', help='Output path (auto-generated if not specified)')
    parser.add_argument('--batch', action='store_true', help='Batch processing mode')
    parser.add_argument('--enhance-contrast', action='store_true', help='Enhance image contrast')
    parser.add_argument('--filter-text', action='store_true', default=True, help='Filter text regions')
    parser.add_argument('--save-mask', action='store_true', default=False, help='Save mask files')
    parser.add_argument('--save-transparent', action='store_true', default=False, help='Save transparent background')
    parser.add_argument('--min-yolo-score', type=float, default=0.1, help='Minimum YOLO score threshold')
    parser.add_argument('--verbose', action='store_true', default=True, help='Verbose output')
    
    # è¤‡é›‘ãƒãƒ¼ã‚ºãƒ»ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯æ§‹å›³å¯¾å¿œã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--difficult-pose', action='store_true', help='Enable difficult pose processing mode')
    parser.add_argument('--low-threshold', action='store_true', help='Use low threshold settings (YOLO score 0.02)')
    parser.add_argument('--auto-retry', action='store_true', help='Enable automatic retry with progressive settings')
    parser.add_argument('--high-quality', action='store_true', help='Enable high-quality SAM processing')
    
    # Phase 2: æ¼«ç”»å‰å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--manga-mode', action='store_true', help='Enable manga-specific preprocessing (Phase 2)')
    parser.add_argument('--effect-removal', action='store_true', help='Enable effect line removal (Phase 2)')
    parser.add_argument('--panel-split', action='store_true', help='Enable multi-panel splitting (Phase 2)')
    
    # è¤‡æ•°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é¸æŠåŸºæº–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--multi-character-criteria', 
                       choices=['balanced', 'size_priority', 'fullbody_priority', 'fullbody_priority_enhanced', 'central_priority', 'confidence_priority'],
                       default='balanced',
                       help='Character selection criteria for multiple characters (default: balanced)')
    
    # Phase 3: é©å¿œå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆ281è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€é©æ‰‹æ³•é¸æŠï¼‰
    parser.add_argument('--adaptive-learning', action='store_true', 
                       help='Enable adaptive learning mode based on 281 evaluation records (Phase 3)')
    
    # Phase A: GPT-4Oæ¨å¥¨ãƒœãƒƒã‚¯ã‚¹æ‹¡å¼µï¼ˆé¡”æ¤œå‡ºãƒœãƒƒã‚¯ã‚¹ã‚’2.5-3å€æ°´å¹³ã€4å€å‚ç›´ã«æ‹¡å¼µï¼‰
    parser.add_argument('--use-box-expansion', action='store_true', 
                       help='Enable GPT-4O recommended box expansion (2.5-3x horizontal, 4x vertical) (Phase A)')
    parser.add_argument('--expansion-strategy', 
                       choices=['conservative', 'balanced', 'aggressive'],
                       default='balanced',
                       help='Box expansion strategy: conservative(2.5x3.5), balanced(2.75x4.0), aggressive(3.0x4.5) (default: balanced)')
    
    args = parser.parse_args()
    
    # Extract common arguments (Phase Aå¯¾å¿œç‰ˆ)
    extract_args = {
        'enhance_contrast': args.enhance_contrast,
        'filter_text': args.filter_text,
        'save_mask': args.save_mask,
        'save_transparent': args.save_transparent,
        'min_yolo_score': args.min_yolo_score,
        'verbose': args.verbose,
        'difficult_pose': args.difficult_pose,
        'low_threshold': args.low_threshold,
        'auto_retry': args.auto_retry,
        'high_quality': args.high_quality,
        'manga_mode': args.manga_mode,
        'effect_removal': args.effect_removal,
        'panel_split': args.panel_split,
        'multi_character_criteria': args.multi_character_criteria,
        'adaptive_learning': args.adaptive_learning,
        'use_box_expansion': args.use_box_expansion,      # Phase A
        'expansion_strategy': args.expansion_strategy     # Phase A
    }
    
    # è¤‡é›‘ãƒãƒ¼ã‚ºãƒ¢ãƒ¼ãƒ‰ç”¨ã®è¨­å®šèª¿æ•´
    if args.low_threshold:
        extract_args['min_yolo_score'] = 0.02
        print("ğŸ”§ ä½é–¾å€¤ãƒ¢ãƒ¼ãƒ‰: YOLOé–¾å€¤ã‚’0.02ã«è¨­å®š")
    
    if args.high_quality:
        print("ğŸ”§ é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰: SAMé«˜å¯†åº¦å‡¦ç†ã‚’æœ‰åŠ¹åŒ–")
    
    # Phase 2: æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
    if args.manga_mode or args.effect_removal or args.panel_split:
        print("ğŸ¨ Phase 2: æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
        if args.effect_removal:
            print("   ğŸ“ ã‚¨ãƒ•ã‚§ã‚¯ãƒˆç·šé™¤å»: æœ‰åŠ¹")
        if args.panel_split:
            print("   ğŸ“Š ãƒãƒ«ãƒã‚³ãƒåˆ†å‰²: æœ‰åŠ¹")
    
    # Phase 3: é©å¿œå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰
    if args.adaptive_learning:
        print("ğŸ§  Phase 3: é©å¿œå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
        print("   ğŸ“Š 281è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€é©æ‰‹æ³•è‡ªå‹•é¸æŠ")
        print("   ğŸ¯ å¢ƒç•Œå•é¡Œè‡ªå‹•æ¤œå‡ºãƒ»å¯¾å¿œ")
        print("   âš™ï¸  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–")
    
    # Phase A: GPT-4Oæ¨å¥¨ãƒœãƒƒã‚¯ã‚¹æ‹¡å¼µ
    if args.use_box_expansion:
        print("ğŸ¯ Phase A: GPT-4Oæ¨å¥¨ãƒœãƒƒã‚¯ã‚¹æ‹¡å¼µæœ‰åŠ¹")
        print(f"   ğŸ“ æ‹¡å¼µæˆ¦ç•¥: {args.expansion_strategy}")
        strategy_details = {
            'conservative': "æ°´å¹³2.5å€ Ã— å‚ç›´3.5å€",
            'balanced': "æ°´å¹³2.75å€ Ã— å‚ç›´4.0å€ (æ¨å¥¨)",
            'aggressive': "æ°´å¹³3.0å€ Ã— å‚ç›´4.5å€"
        }
        print(f"   ğŸ“ æ‹¡å¼µå€ç‡: {strategy_details.get(args.expansion_strategy, 'ä¸æ˜')}")
        print("   ğŸª é¡”æ¤œå‡ºãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰å…¨èº«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºã‚’å¼·åŒ–")
    
    if args.batch:
        # Batch processing
        output_dir = args.output or f"{args.input}_character_output"
        result = batch_extract_characters(args.input, output_dir, **extract_args)
    else:
        # Single file processing
        result = extract_character_from_path(args.input, args.output, **extract_args)
    
    # Exit with appropriate code
    sys.exit(0 if result['success'] else 1)


if __name__ == "__main__":
    main()