#!/usr/bin/env python3
"""
YOLO Model Wrapper
YOLOv8 wrapper for character detection and scoring
"""

import numpy as np
import cv2
import torch

import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    print("WARNING: ultralytics not available, YOLO functionality disabled")


class YOLOModelWrapper:
    """
    Wrapper class for YOLOv8 model
    Provides character detection and scoring capabilities
    """
    
    def __init__(self, 
                 model_path: str = "yolov8n.pt",
                 confidence_threshold: float = 0.25,
                 device: Optional[str] = None):
        """
        Initialize YOLO wrapper
        
        Args:
            model_path: Path to YOLO model file
            confidence_threshold: Minimum confidence for detections
            device: Device to run on (cuda/cpu), auto-detect if None
        """
        self.model_path = model_path
        self.confidence_threshold = confidence_threshold
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        self.model = None
        self.is_loaded = False
        
        # COCO class names for person detection
        self.person_class_id = 0  # 'person' class in COCO dataset
        
    def load_model(self) -> bool:
        """
        Load YOLO model
        
        Returns:
            True if successful, False otherwise
        """
        if not YOLO_AVAILABLE:
            print("‚ùå YOLO unavailable - ultralytics not installed")
            return False
        
        try:
            # Check if model file exists, download if needed
            if not os.path.exists(self.model_path):
                print(f"üì• Downloading YOLO model: {self.model_path}")
            
            # Load YOLO model
            self.model = YOLO(self.model_path)
            self.model.to(self.device)
            
            self.is_loaded = True
            print(f"‚úÖ YOLO {self.model_path} loaded on {self.device}")
            return True
            
        except Exception as e:
            print(f"‚ùå YOLO model loading failed: {e}")
            return False
    
    def detect_persons(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        Detect person objects in the image
        
        Args:
            image: Input image (BGR or RGB format)
            
        Returns:
            List of detection dictionaries with bbox, confidence, etc.
        """
        if not self.is_loaded:
            raise RuntimeError("YOLO model not loaded. Call load_model() first.")
        
        try:
            # Run inference
            results = self.model(image, verbose=False)
            
            persons = []
            
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for i, box in enumerate(boxes):
                        # Check if detection is a person
                        class_id = int(box.cls[0])
                        if class_id == self.person_class_id:
                            confidence = float(box.conf[0])
                            
                            # Filter by confidence threshold
                            if confidence >= self.confidence_threshold:
                                # Get bounding box coordinates
                                x1, y1, x2, y2 = box.xyxy[0].tolist()
                                
                                persons.append({
                                    'bbox': [int(x1), int(y1), int(x2-x1), int(y2-y1)],  # [x, y, w, h]
                                    'bbox_xyxy': [int(x1), int(y1), int(x2), int(y2)],   # [x1, y1, x2, y2]
                                    'confidence': confidence,
                                    'area': int((x2-x1) * (y2-y1)),
                                    'class_id': class_id,
                                    'class_name': 'person'
                                })
            
            # Sort by confidence (highest first)
            persons.sort(key=lambda x: x['confidence'], reverse=True)
            
            return persons
            
        except Exception as e:
            print(f"‚ùå Person detection failed: {e}")
            return []
    
    def calculate_overlap_score(self, 
                              mask_bbox: Tuple[int, int, int, int], 
                              person_bbox: Tuple[int, int, int, int]) -> float:
        """
        Calculate overlap score between mask and person detection
        
        Args:
            mask_bbox: Mask bounding box [x, y, w, h]
            person_bbox: Person detection bounding box [x, y, w, h]
            
        Returns:
            Overlap score (IoU - Intersection over Union)
        """
        # Convert to [x1, y1, x2, y2] format
        mask_x1, mask_y1 = mask_bbox[0], mask_bbox[1]
        mask_x2, mask_y2 = mask_x1 + mask_bbox[2], mask_y1 + mask_bbox[3]
        
        person_x1, person_y1 = person_bbox[0], person_bbox[1]
        person_x2, person_y2 = person_x1 + person_bbox[2], person_y1 + person_bbox[3]
        
        # Calculate intersection
        intersection_x1 = max(mask_x1, person_x1)
        intersection_y1 = max(mask_y1, person_y1)
        intersection_x2 = min(mask_x2, person_x2)
        intersection_y2 = min(mask_y2, person_y2)
        
        # Check if there's an intersection
        if intersection_x1 >= intersection_x2 or intersection_y1 >= intersection_y2:
            return 0.0
        
        # Calculate areas
        intersection_area = (intersection_x2 - intersection_x1) * (intersection_y2 - intersection_y1)
        mask_area = mask_bbox[2] * mask_bbox[3]
        person_area = person_bbox[2] * person_bbox[3]
        union_area = mask_area + person_area - intersection_area
        
        # Calculate IoU
        if union_area == 0:
            return 0.0
        
        iou = intersection_area / union_area
        return iou
    
    def score_masks_with_detections(self, 
                                   masks: List[Dict[str, Any]], 
                                   image: np.ndarray,
                                   use_expanded_boxes: bool = False,
                                   expansion_strategy: str = 'balanced') -> List[Dict[str, Any]]:
        """
        Score SAM masks based on YOLO person detections
        
        Args:
            masks: List of SAM mask dictionaries
            image: Input image for person detection
            use_expanded_boxes: GPT-4OÊé®Â•®„ÅÆ„Éú„ÉÉ„ÇØ„ÇπÊã°Âºµ„Çí‰ΩøÁî®
            expansion_strategy: Êã°ÂºµÊà¶Áï• ('conservative', 'balanced', 'aggressive')
            
        Returns:
            List of masks with added YOLO scores
        """
        if not masks:
            return []
        
        # Get person detections
        persons = self.detect_persons(image)
        
        # GPT-4OÊé®Â•®: „Éú„ÉÉ„ÇØ„ÇπÊã°Âºµ„Ç™„Éó„Ç∑„Éß„É≥
        if use_expanded_boxes and persons:
            try:
                from features.extraction.utils.box_expansion import apply_gpt4o_expansion_strategy
                image_shape = image.shape[:2]  # (height, width)
                expanded_persons = apply_gpt4o_expansion_strategy(persons, image_shape, expansion_strategy)
                
                if expanded_persons:
                    print(f"üéØ GPT-4OÊé®Â•®„Éú„ÉÉ„ÇØ„ÇπÊã°ÂºµÈÅ©Áî®: {len(persons)}‚Üí{len(expanded_persons)} (Êà¶Áï•: {expansion_strategy})")
                    for i, person in enumerate(expanded_persons[:3]):  # ÊúÄÂ§ß3‰ª∂Ë°®Á§∫
                        exp_info = person.get('expansion_info', {})
                        print(f"   Ê§úÂá∫{i+1}: H{exp_info.get('horizontal_factor', 0):.1f}x V{exp_info.get('vertical_factor', 0):.1f}x "
                              f"({'Â¢ÉÁïåÂà∂Èôê' if exp_info.get('clipped_to_bounds') else 'Âà∂Èôê„Å™„Åó'})")
                    persons = expanded_persons
                    
            except ImportError as e:
                print(f"‚ö†Ô∏è „Éú„ÉÉ„ÇØ„ÇπÊã°Âºµ„É¢„Ç∏„É•„Éº„É´Êú™Âà©Áî®ÂèØËÉΩ: {e}")
            except Exception as e:
                print(f"‚ö†Ô∏è „Éú„ÉÉ„ÇØ„ÇπÊã°Âºµ„Ç®„É©„Éº: {e}")
                # „Ç®„É©„ÉºÊôÇ„ÅØÂÖÉ„ÅÆÊ§úÂá∫ÁµêÊûú„Çí‰ΩøÁî®
        
        # Score each mask
        scored_masks = []
        
        for mask in masks:
            mask_bbox = mask['bbox']  # [x, y, w, h]
            best_overlap = 0.0
            best_confidence = 0.0
            
            # Find best matching person detection
            for person in persons:
                overlap = self.calculate_overlap_score(mask_bbox, person['bbox'])
                if overlap > best_overlap:
                    best_overlap = overlap
                    best_confidence = person['confidence']
            
            # Calculate combined score
            yolo_score = best_overlap * best_confidence
            
            # Add YOLO scoring information to mask
            mask_with_score = mask.copy()
            mask_with_score.update({
                'yolo_overlap': best_overlap,
                'yolo_confidence': best_confidence,
                'yolo_score': yolo_score,
                'combined_score': mask['stability_score'] * 0.6 + yolo_score * 0.4
            })
            
            scored_masks.append(mask_with_score)
        
        # Sort by combined score (highest first)
        scored_masks.sort(key=lambda x: x['combined_score'], reverse=True)
        
        return scored_masks
    
    def get_best_character_mask(self, 
                               masks: List[Dict[str, Any]], 
                               image: np.ndarray,
                               min_yolo_score: float = 0.1) -> Optional[Dict[str, Any]]:
        """
        Get the best character mask based on YOLO scoring
        
        Args:
            masks: List of SAM mask dictionaries
            image: Input image for person detection
            min_yolo_score: Minimum YOLO score threshold
            
        Returns:
            Best character mask or None if no good matches
        """
        scored_masks = self.score_masks_with_detections(masks, image)
        
        # Filter by minimum YOLO score
        good_masks = [m for m in scored_masks if m['yolo_score'] >= min_yolo_score]
        
        if good_masks:
            return good_masks[0]  # Highest scored mask
        
        return None
    
    def _select_best_character_with_criteria(self, 
                                           masks: List[Dict[str, Any]], 
                                           image_shape: tuple,
                                           criteria: str = 'balanced',
                                           original_image: Optional[np.ndarray] = None) -> Optional[Tuple[Dict[str, Any], float]]:
        """
        Ë§áÂêà„Çπ„Ç≥„Ç¢„Å´„Çà„ÇãÊúÄÈÅ©„Ç≠„É£„É©„ÇØ„Çø„ÉºÈÅ∏Êäû (GeminiÊèêÊ°àÂÆüË£Ö)
        
        Args:
            masks: „Éû„Çπ„ÇØÂÄôË£ú„É™„Çπ„Éà
            image_shape: ÁîªÂÉè„Çµ„Ç§„Ç∫ (height, width, channels)
            criteria: ÈÅ∏ÊäûÂü∫Ê∫ñ ('balanced', 'size_priority', 'fullbody_priority', 'fullbody_priority_enhanced', 'central_priority', 'confidence_priority')
            original_image: ÂÖÉÁîªÂÉèÔºàÊîπËâØÁâàÂÖ®Ë∫´Ê§úÂá∫„Å´ÂøÖË¶ÅÔºâ
            
        Returns:
            (ÊúÄÈÅ©„Éû„Çπ„ÇØ, ÂìÅË≥™„Çπ„Ç≥„Ç¢) „Åæ„Åü„ÅØ None
        """
        if not masks:
            return None
        
        h, w = image_shape[:2]
        image_center_x, image_center_y = w / 2, h / 2
        
        def calculate_composite_score(mask_data: Dict[str, Any]) -> Dict[str, float]:
            """Ë§áÂêà„Çπ„Ç≥„Ç¢Ë®àÁÆó"""
            scores = {}
            
            # 1. Èù¢Á©ç„Çπ„Ç≥„Ç¢ (30%): ÈÅ©Â∫¶„Å™Â§ß„Åç„Åï„ÇíË©ï‰æ°
            area_ratio = mask_data['area'] / (h * w)
            if 0.05 <= area_ratio <= 0.4:  # ÁîªÂÉè„ÅÆ5-40%„ÅåÁêÜÊÉ≥ÁöÑ
                scores['area'] = min(area_ratio / 0.4, 1.0)
            else:
                scores['area'] = max(0, 1.0 - abs(area_ratio - 0.2) / 0.2)
            
            # 2. „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„Çπ„Ç≥„Ç¢ (25%): ÂÖ®Ë∫´„Ç≠„É£„É©„ÇØ„Çø„Éº„ÇíÂÑ™ÂÖà
            bbox = mask_data['bbox']
            aspect_ratio = bbox[3] / max(bbox[2], 1)  # height / width
            
            # Phase 1 P1-007: Enhanced Aspect Ratio AnalyzerÁµ±Âêà
            if criteria in ['fullbody_priority_enhanced', 'aspect_ratio_enhanced'] and original_image is not None:
                try:
                    # P1-007: Enhanced Aspect Ratio Analysis
                    from features.evaluation.utils.enhanced_aspect_ratio_analyzer import (
                        evaluate_enhanced_aspect_ratio,
                    )

                    # Enhanced aspect ratio analysis
                    enhanced_fullbody_score, aspect_analysis = evaluate_enhanced_aspect_ratio(
                        original_image, mask_data, aspect_ratio
                    )
                    
                    scores['fullbody'] = enhanced_fullbody_score
                    print(f"   P1-007 Enhanced aspect ratio: {enhanced_fullbody_score:.3f} "
                          f"(style: {aspect_analysis.style_category.value}, "
                          f"ratio: {aspect_analysis.adjusted_ratio:.2f})")
                    
                    # Fallback to P1-003 if P1-007 gives low confidence
                    if aspect_analysis.confidence_score < 0.4:
                        try:
                            from features.evaluation.utils.enhanced_fullbody_detector import (
                                evaluate_fullbody_enhanced,
                            )
                            
                            fullbody_result = evaluate_fullbody_enhanced(original_image, mask_data)
                            # Blend P1-007 and P1-003 results based on confidence
                            blend_factor = aspect_analysis.confidence_score
                            scores['fullbody'] = (
                                enhanced_fullbody_score * blend_factor + 
                                fullbody_result.total_score * (1 - blend_factor)
                            )
                            print(f"   Blended with P1-003: {scores['fullbody']:.3f}")
                        except Exception as e:
                            print(f"   P1-003 fallback failed: {e}")
                
                except Exception as e:
                    print(f"   P1-007 Enhanced aspect ratio error: {e}, using traditional fallback")
                    # ÂæìÊù•„ÅÆ„Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„Éô„Éº„ÇπÂà§ÂÆö„Å´„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
                    if 1.2 <= aspect_ratio <= 2.5:
                        scores['fullbody'] = min((aspect_ratio - 0.5) / 2.0, 1.0)
                    else:
                        scores['fullbody'] = max(0, 1.0 - abs(aspect_ratio - 1.8) / 1.0)
            
            elif criteria == 'fullbody_priority_enhanced' and original_image is None:
                print("   Enhanced fullbody: no image provided, using P1-003 fallback")
                # P1-003„Ç∑„Çπ„ÉÜ„É†„ÅÆ‰ΩøÁî®„ÇíË©¶Ë°å
                try:
                    from features.evaluation.utils.enhanced_fullbody_detector import (
                        evaluate_fullbody_enhanced,
                    )

                    # ÁîªÂÉè„Å™„Åó„Åß„ÇÇ‰ΩøÁî®ÂèØËÉΩ„Å™Â†¥Âêà
                    enhanced_score = evaluate_fullbody_enhanced(None, mask_data)
                    scores['fullbody'] = enhanced_score.total_score
                except Exception:
                    # ÂÆåÂÖ®„Å™„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
                    if 1.2 <= aspect_ratio <= 2.5:
                        scores['fullbody'] = min((aspect_ratio - 0.5) / 2.0, 1.0)
                    else:
                        scores['fullbody'] = max(0, 1.0 - abs(aspect_ratio - 1.8) / 1.0)
            
            else:
                # ÂæìÊù•„ÅÆ„Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„Éô„Éº„ÇπÂà§ÂÆö
                if 1.2 <= aspect_ratio <= 2.5:  # ÂÖ®Ë∫´„Ç≠„É£„É©„ÇØ„Çø„ÉºÁØÑÂõ≤
                    scores['fullbody'] = min((aspect_ratio - 0.5) / 2.0, 1.0)
                else:
                    scores['fullbody'] = max(0, 1.0 - abs(aspect_ratio - 1.8) / 1.0)
            
            # 3. ‰∏≠Â§Æ‰ΩçÁΩÆ„Çπ„Ç≥„Ç¢ (20%): ÁîªÂÉè‰∏≠Â§Æ„Å´Ëøë„ÅÑ„Ç≠„É£„É©„ÇØ„Çø„Éº„ÇíÂÑ™ÂÖà
            mask_center_x = bbox[0] + bbox[2] / 2
            mask_center_y = bbox[1] + bbox[3] / 2
            distance_from_center = np.sqrt(
                ((mask_center_x - image_center_x) / w)**2 + 
                ((mask_center_y - image_center_y) / h)**2
            )
            scores['central'] = max(0, 1.0 - distance_from_center)
            
            # 4. Êé•Âú∞„Çπ„Ç≥„Ç¢ (15%): ÁîªÈù¢‰∏ãÈÉ®„Å´„ÅÑ„Çã„Ç≠„É£„É©„ÇØ„Çø„Éº„ÇíÂÑ™ÂÖà
            bottom_position = (bbox[1] + bbox[3]) / h
            if bottom_position >= 0.6:  # ‰∏ãÈÉ®60%‰ª•Èôç
                scores['grounded'] = min(bottom_position, 1.0)
            else:
                scores['grounded'] = bottom_position / 0.6
            
            # 5. YOLO‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢ (10%)
            scores['confidence'] = mask_data.get('yolo_confidence', 0.0)
            
            return scores
        
        # Âü∫Ê∫ñÂà•„ÅÆÈáç„ÅøË®≠ÂÆö
        weight_configs = {
            'balanced': {'area': 0.30, 'fullbody': 0.25, 'central': 0.20, 'grounded': 0.15, 'confidence': 0.10},
            'size_priority': {'area': 0.50, 'fullbody': 0.15, 'central': 0.15, 'grounded': 0.10, 'confidence': 0.10},
            'fullbody_priority': {'area': 0.20, 'fullbody': 0.40, 'central': 0.15, 'grounded': 0.15, 'confidence': 0.10},
            'fullbody_priority_enhanced': {'area': 0.15, 'fullbody': 0.50, 'central': 0.15, 'grounded': 0.10, 'confidence': 0.10},  # Phase 1 P1-003
            'aspect_ratio_enhanced': {'area': 0.10, 'fullbody': 0.60, 'central': 0.15, 'grounded': 0.10, 'confidence': 0.05},  # Phase 1 P1-007
            'central_priority': {'area': 0.20, 'fullbody': 0.20, 'central': 0.35, 'grounded': 0.15, 'confidence': 0.10},
            'confidence_priority': {'area': 0.25, 'fullbody': 0.20, 'central': 0.15, 'grounded': 0.10, 'confidence': 0.30}
        }
        
        weights = weight_configs.get(criteria, weight_configs['balanced'])
        
        # ÂêÑ„Éû„Çπ„ÇØ„ÅÆ„Çπ„Ç≥„Ç¢Ë®àÁÆó
        best_mask = None
        best_score = 0.0
        
        print(f"üéØ Ë§áÂêà„Çπ„Ç≥„Ç¢Ë©ï‰æ°ÈñãÂßã (Âü∫Ê∫ñ: {criteria})")
        
        for i, mask_data in enumerate(masks):
            scores = calculate_composite_score(mask_data)
            
            # Èáç„Åø‰ªò„ÅçÁ∑èÂêà„Çπ„Ç≥„Ç¢Ë®àÁÆó
            composite_score = sum(scores[key] * weights[key] for key in weights.keys())
            
            print(f"   „Éû„Çπ„ÇØ{i+1}: Á∑èÂêà={composite_score:.3f} "
                  f"(Èù¢Á©ç={scores['area']:.2f}, ÂÖ®Ë∫´={scores['fullbody']:.2f}, "
                  f"‰∏≠Â§Æ={scores['central']:.2f}, Êé•Âú∞={scores['grounded']:.2f}, "
                  f"‰ø°È†ºÂ∫¶={scores['confidence']:.2f})")
            
            if composite_score > best_score:
                best_score = composite_score
                best_mask = mask_data
        
        if best_mask is not None:
            print(f"‚úÖ ÊúÄÈÅ©„Éû„Çπ„ÇØÈÅ∏Êäû: Á∑èÂêà„Çπ„Ç≥„Ç¢ {best_score:.3f}")
            return best_mask, best_score
        
        return None
    
    def select_best_mask_with_criteria(self, 
                                     masks: List[Dict[str, Any]], 
                                     image: np.ndarray,
                                     criteria: str = 'balanced') -> Optional[Tuple[Dict[str, Any], float]]:
        """
        ÂÖ¨Èñã„É°„ÇΩ„ÉÉ„Éâ: Ë§áÂêà„Çπ„Ç≥„Ç¢„Å´„Çà„ÇãÊúÄÈÅ©„Éû„Çπ„ÇØÈÅ∏Êäû
        
        Args:
            masks: „Éû„Çπ„ÇØÂÄôË£ú„É™„Çπ„Éà
            image: ÂÖÉÁîªÂÉè
            criteria: ÈÅ∏ÊäûÂü∫Ê∫ñ
            
        Returns:
            (ÊúÄÈÅ©„Éû„Çπ„ÇØ, ÂìÅË≥™„Çπ„Ç≥„Ç¢) „Åæ„Åü„ÅØ None
        """
        if not masks:
            return None
        
        image_shape = image.shape
        return self._select_best_character_with_criteria(masks, image_shape, criteria, image)
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the loaded model
        
        Returns:
            Dictionary with model information
        """
        return {
            'model_path': self.model_path,
            'device': self.device,
            'is_loaded': self.is_loaded,
            'confidence_threshold': self.confidence_threshold,
            'available': YOLO_AVAILABLE,
            'person_class_id': self.person_class_id
        }
    
    def unload_model(self):
        """
        Unload model and free memory
        """
        if self.model is not None:
            del self.model
            self.model = None
        
        self.is_loaded = False
        
        # Clear GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("üóëÔ∏è YOLO model unloaded")


if __name__ == "__main__":
    # Test the wrapper
    wrapper = YOLOModelWrapper()
    
    if wrapper.load_model():
        print("‚úÖ YOLO wrapper test successful")
        print(f"Model info: {wrapper.get_model_info()}")
        
        # Test with a sample image if available
        test_image_path = "../assets/masks1.png"
        if os.path.exists(test_image_path):
            image = cv2.imread(test_image_path)
            
            persons = wrapper.detect_persons(image)
            print(f"Detected {len(persons)} persons")
            
            for i, person in enumerate(persons):
                print(f"  Person {i+1}: confidence={person['confidence']:.3f}, bbox={person['bbox']}")
        
        wrapper.unload_model()
    else:
        print("‚ùå YOLO wrapper test failed")