#!/usr/bin/env python3
"""
äººé–“ãƒ©ãƒ™ãƒ« vs AIæŠ½å‡ºç²¾åº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ 
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½œæˆã—ãŸèµ¤æ ãƒ©ãƒ™ãƒ«ã‚’æ­£è§£ã¨ã—ã¦ã€ç¾åœ¨ã®AIã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦ã‚’æ¸¬å®š
"""

import numpy as np
import cv2
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import torch

import json
import logging
# SAM + YOLO ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from PIL import Image
from typing import Dict, List, Optional, Tuple

sys.path.append(str(Path(__file__).parent))
from segment_anything import SamPredictor, sam_model_registry

from ultralytics import YOLO

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    image_id: str
    image_path: str
    human_bbox: Tuple[int, int, int, int]  # x, y, w, h
    ai_bbox: Optional[Tuple[int, int, int, int]]
    iou_score: float
    extraction_success: bool
    processing_time: float
    error_message: str = ""
    

class HumanVsAIBenchmark:
    """äººé–“ãƒ©ãƒ™ãƒ« vs AIæŠ½å‡ºãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.labels_file = project_root / "extracted_labels.json"
        self.output_dir = Path("/mnt/c/AItools/lora/train/yado/benchmark_results")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.human_labels = self.load_human_labels()
        logger.info(f"äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(self.human_labels)}ä»¶")
        
        # SAM + YOLOãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.init_models()
        
    def init_models(self):
        """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        # YOLOåˆæœŸåŒ–
        self.yolo_model = YOLO('yolov8n.pt')
        
        # SAMåˆæœŸåŒ–
        sam_checkpoint = self.project_root / "sam_vit_h_4b8939.pth"
        if not sam_checkpoint.exists():
            logger.error(f"SAMãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sam_checkpoint}")
            raise FileNotFoundError(f"SAM checkpoint not found: {sam_checkpoint}")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        sam = sam_model_registry["vit_h"](checkpoint=str(sam_checkpoint))
        sam.to(device=device)
        self.sam_predictor = SamPredictor(sam)
        
    def load_human_labels(self) -> Dict[str, Dict]:
        """äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            with open(self.labels_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ãƒªã‚¹ãƒˆå½¢å¼ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            labels_dict = {}
            for item in data:
                filename = item['filename']
                image_id = filename.rsplit('.', 1)[0]
                
                # æœ€åˆã®èµ¤æ ã‚’ä½¿ç”¨
                if item.get('red_boxes') and len(item['red_boxes']) > 0:
                    first_box = item['red_boxes'][0]
                    bbox = first_box['bbox']
                    labels_dict[image_id] = {
                        'filename': filename,
                        'bbox': [bbox['x'], bbox['y'], bbox['width'], bbox['height']]
                    }
            
            return labels_dict
            
        except Exception as e:
            logger.error(f"ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def calculate_iou(self, bbox1: Tuple[int, int, int, int], 
                     bbox2: Tuple[int, int, int, int]) -> float:
        """IoU (Intersection over Union) è¨ˆç®—"""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        # äº¤å·®é ˜åŸŸã®è¨ˆç®—
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        # äº¤å·®é ˜åŸŸã®é¢ç©
        intersection_area = (x_right - x_left) * (y_bottom - y_top)
        
        # å’Œé›†åˆã®é¢ç©
        union_area = w1 * h1 + w2 * h2 - intersection_area
        
        # IoU
        iou = intersection_area / max(union_area, 1e-6)
        return iou
    
    def find_image_path(self, image_id: str) -> Optional[Path]:
        """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ¤œç´¢"""
        search_dirs = [
            Path("/mnt/c/AItools/lora/train/yado/org/kana05_cursor"),
            Path("/mnt/c/AItools/lora/train/yado/org/kana07_cursor"),
            Path("/mnt/c/AItools/lora/train/yado/org/kana08_cursor"),
            self.project_root / "test_small"
        ]
        
        extensions = ['.jpg', '.jpeg', '.png']
        
        for dir_path in search_dirs:
            for ext in extensions:
                image_path = dir_path / f"{image_id}{ext}"
                if image_path.exists():
                    return image_path
        
        return None
    
    def extract_with_ai(self, image_path: Path) -> Optional[Tuple[int, int, int, int]]:
        """ç¾åœ¨ã®AIã‚·ã‚¹ãƒ†ãƒ ã§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡º"""
        try:
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(str(image_path))
            if image is None:
                return None
            
            # YOLOæ¤œå‡º
            results = self.yolo_model(image, conf=0.07)  # ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”¨ä½é–¾å€¤
            
            if not results or len(results[0].boxes) == 0:
                return None
            
            # æœ€å¤§ã®æ¤œå‡ºçµæœã‚’é¸æŠ
            boxes = results[0].boxes.xyxy.cpu().numpy()
            areas = []
            for box in boxes:
                x1, y1, x2, y2 = box
                area = (x2 - x1) * (y2 - y1)
                areas.append(area)
            
            largest_idx = np.argmax(areas)
            x1, y1, x2, y2 = boxes[largest_idx]
            
            # SAMã§ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            self.sam_predictor.set_image(image)
            
            # ä¸­å¿ƒç‚¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ä½¿ç”¨
            input_point = np.array([[(x1 + x2) / 2, (y1 + y2) / 2]])
            input_label = np.array([1])
            
            masks, scores, _ = self.sam_predictor.predict(
                point_coords=input_point,
                point_labels=input_label,
                multimask_output=True
            )
            
            if masks is None or len(masks) == 0:
                # SAMå¤±æ•—æ™‚ã¯YOLOãƒœãƒƒã‚¯ã‚¹ã‚’è¿”ã™
                return (int(x1), int(y1), int(x2 - x1), int(y2 - y1))
            
            # æœ€è‰¯ãƒã‚¹ã‚¯ã‹ã‚‰å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹è¨ˆç®—
            best_mask = masks[np.argmax(scores)]
            y_indices, x_indices = np.where(best_mask > 0)
            
            if len(x_indices) == 0 or len(y_indices) == 0:
                return (int(x1), int(y1), int(x2 - x1), int(y2 - y1))
            
            x_min = int(np.min(x_indices))
            x_max = int(np.max(x_indices))
            y_min = int(np.min(y_indices))
            y_max = int(np.max(y_indices))
            
            return (x_min, y_min, x_max - x_min, y_max - y_min)
            
        except Exception as e:
            logger.error(f"AIæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def run_single_benchmark(self, image_id: str, label_data: Dict) -> BenchmarkResult:
        """å˜ä¸€ç”»åƒã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        start_time = time.time()
        
        # ç”»åƒãƒ‘ã‚¹æ¤œç´¢
        image_path = self.find_image_path(image_id)
        if not image_path:
            return BenchmarkResult(
                image_id=image_id,
                image_path="",
                human_bbox=tuple(label_data['bbox']),
                ai_bbox=None,
                iou_score=0.0,
                extraction_success=False,
                processing_time=0.0,
                error_message="ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        # AIæŠ½å‡ºå®Ÿè¡Œ
        ai_bbox = self.extract_with_ai(image_path)
        processing_time = time.time() - start_time
        
        # çµæœè©•ä¾¡
        human_bbox = tuple(label_data['bbox'])
        
        if ai_bbox is None:
            iou_score = 0.0
            extraction_success = False
            error_message = "AIæŠ½å‡ºå¤±æ•—"
        else:
            iou_score = self.calculate_iou(human_bbox, ai_bbox)
            extraction_success = iou_score > 0.5  # IoU > 0.5 ã‚’æˆåŠŸã¨ã™ã‚‹
            error_message = ""
        
        return BenchmarkResult(
            image_id=image_id,
            image_path=str(image_path),
            human_bbox=human_bbox,
            ai_bbox=ai_bbox,
            iou_score=iou_score,
            extraction_success=extraction_success,
            processing_time=processing_time,
            error_message=error_message
        )
    
    def run_full_benchmark(self) -> List[BenchmarkResult]:
        """å…¨ç”»åƒã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        logger.info("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        results = []
        
        total = len(self.human_labels)
        for i, (image_id, label_data) in enumerate(self.human_labels.items(), 1):
            logger.info(f"å‡¦ç†ä¸­ [{i}/{total}]: {image_id}")
            
            result = self.run_single_benchmark(image_id, label_data)
            results.append(result)
            
            # é€²æ—è¡¨ç¤º
            if i % 10 == 0:
                success_count = sum(1 for r in results if r.extraction_success)
                logger.info(f"é€²æ—: {i}/{total} - æˆåŠŸç‡: {success_count/i*100:.1f}%")
        
        return results
    
    def generate_comparison_image(self, result: BenchmarkResult, output_path: Path):
        """æ¯”è¼ƒç”»åƒç”Ÿæˆï¼ˆäººé–“ãƒ©ãƒ™ãƒ« vs AIæŠ½å‡ºï¼‰"""
        try:
            if not result.image_path:
                return
            
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(result.image_path)
            if image is None:
                return
            
            # BGR -> RGBå¤‰æ›
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # å›³ä½œæˆ
            fig, ax = plt.subplots(1, 1, figsize=(12, 8))
            ax.imshow(image_rgb)
            
            # äººé–“ãƒ©ãƒ™ãƒ«ï¼ˆç·‘è‰²ï¼‰
            human_rect = patches.Rectangle(
                (result.human_bbox[0], result.human_bbox[1]),
                result.human_bbox[2], result.human_bbox[3],
                linewidth=3, edgecolor='green', facecolor='none',
                label='Human Label'
            )
            ax.add_patch(human_rect)
            
            # AIæŠ½å‡ºçµæœï¼ˆèµ¤è‰²ï¼‰
            if result.ai_bbox:
                ai_rect = patches.Rectangle(
                    (result.ai_bbox[0], result.ai_bbox[1]),
                    result.ai_bbox[2], result.ai_bbox[3],
                    linewidth=3, edgecolor='red', facecolor='none',
                    label='AI Extraction'
                )
                ax.add_patch(ai_rect)
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨æƒ…å ±
            title = f"{result.image_id} - IoU: {result.iou_score:.3f}"
            if result.extraction_success:
                title += " âœ…"
            else:
                title += " âŒ"
            
            ax.set_title(title, fontsize=14, fontweight='bold')
            ax.legend(loc='upper right')
            ax.axis('off')
            
            # ä¿å­˜
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"æ¯”è¼ƒç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_report(self, results: List[BenchmarkResult]):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        # çµ±è¨ˆè¨ˆç®—
        total = len(results)
        success_count = sum(1 for r in results if r.extraction_success)
        success_rate = success_count / total * 100
        
        iou_scores = [r.iou_score for r in results]
        avg_iou = np.mean(iou_scores)
        
        processing_times = [r.processing_time for r in results]
        avg_time = np.mean(processing_times)
        
        # ãƒ™ã‚¹ãƒˆ5ãƒ»ãƒ¯ãƒ¼ã‚¹ãƒˆ5
        sorted_results = sorted(results, key=lambda r: r.iou_score, reverse=True)
        best_5 = sorted_results[:5]
        worst_5 = sorted_results[-5:]
        
        # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = f"""# äººé–“ãƒ©ãƒ™ãƒ« vs AIæŠ½å‡º ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**ç·ç”»åƒæ•°**: {total}æš

---

## ğŸ“Š å…¨ä½“çµ±è¨ˆ

### æˆåŠŸç‡
- **æŠ½å‡ºæˆåŠŸæ•°**: {success_count}æš
- **æˆåŠŸç‡**: {success_rate:.1f}%
- **åŸºæº–**: IoU > 0.5

### IoUã‚¹ã‚³ã‚¢
- **å¹³å‡IoU**: {avg_iou:.3f}
- **æœ€é«˜IoU**: {max(iou_scores):.3f}
- **æœ€ä½IoU**: {min(iou_scores):.3f}

### å‡¦ç†æ€§èƒ½
- **å¹³å‡å‡¦ç†æ™‚é–“**: {avg_time:.2f}ç§’/ç”»åƒ

---

## ğŸ† ãƒ™ã‚¹ãƒˆ5ï¼ˆAIãŒæœ€ã‚‚æ­£ç¢ºï¼‰

"""
        
        for i, result in enumerate(best_5, 1):
            report += f"{i}. **{result.image_id}** - IoU: {result.iou_score:.3f}\n"
        
        report += """
---

## ğŸ’¥ ãƒ¯ãƒ¼ã‚¹ãƒˆ5ï¼ˆAIãŒæœ€ã‚‚ä¸æ­£ç¢ºï¼‰

"""
        
        for i, result in enumerate(worst_5, 1):
            report += f"{i}. **{result.image_id}** - IoU: {result.iou_score:.3f}"
            if result.error_message:
                report += f" ({result.error_message})"
            report += "\n"
        
        report += """
---

## ğŸ¯ æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ

"""
        
        # å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        extraction_failures = [r for r in results if not r.ai_bbox]
        low_iou_cases = [r for r in results if r.ai_bbox and r.iou_score < 0.3]
        
        report += f"""### å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³
- **å®Œå…¨æŠ½å‡ºå¤±æ•—**: {len(extraction_failures)}ä»¶ ({len(extraction_failures)/total*100:.1f}%)
- **ä½ç²¾åº¦æŠ½å‡º (IoU < 0.3)**: {len(low_iou_cases)}ä»¶ ({len(low_iou_cases)/total*100:.1f}%)

### æ¨å¥¨æ”¹å–„ç­–
1. YOLOæ¤œå‡ºé–¾å€¤ã®èª¿æ•´ï¼ˆç¾åœ¨0.07ï¼‰
2. SAMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã®æ”¹å–„
3. å›°é›£ãªãƒãƒ¼ã‚ºã«å¯¾ã™ã‚‹ç‰¹åˆ¥å‡¦ç†
4. Phase 1ä»¥é™ã®æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š

---

*Generated by Human vs AI Benchmark System*
"""
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = self.output_dir / f"benchmark_report_{time.strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        
        # çµæœJSONä¿å­˜
        results_data = [asdict(r) for r in results]
        json_path = self.output_dir / f"benchmark_results_{time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        return report_path, best_5, worst_5


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    project_root = Path("/mnt/c/AItools/segment-anything")
    
    benchmark = HumanVsAIBenchmark(project_root)
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    results = benchmark.run_full_benchmark()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_path, best_5, worst_5 = benchmark.generate_report(results)
    
    # æ¯”è¼ƒç”»åƒç”Ÿæˆï¼ˆãƒ™ã‚¹ãƒˆ5ãƒ»ãƒ¯ãƒ¼ã‚¹ãƒˆ5ï¼‰
    comparison_dir = benchmark.output_dir / "comparisons"
    comparison_dir.mkdir(exist_ok=True)
    
    logger.info("æ¯”è¼ƒç”»åƒç”Ÿæˆä¸­...")
    
    # ãƒ™ã‚¹ãƒˆ5
    best_dir = comparison_dir / "best_5"
    best_dir.mkdir(exist_ok=True)
    for i, result in enumerate(best_5, 1):
        output_path = best_dir / f"{i:02d}_{result.image_id}_iou{result.iou_score:.3f}.png"
        benchmark.generate_comparison_image(result, output_path)
    
    # ãƒ¯ãƒ¼ã‚¹ãƒˆ5
    worst_dir = comparison_dir / "worst_5"
    worst_dir.mkdir(exist_ok=True)
    for i, result in enumerate(worst_5, 1):
        output_path = worst_dir / f"{i:02d}_{result.image_id}_iou{result.iou_score:.3f}.png"
        benchmark.generate_comparison_image(result, output_path)
    
    # æœ€çµ‚çµ±è¨ˆ
    success_count = sum(1 for r in results if r.extraction_success)
    print(f"\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
    print(f"æˆåŠŸç‡: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")
    print(f"ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    print(f"æ¯”è¼ƒç”»åƒ: {comparison_dir}")


if __name__ == "__main__":
    main()