#!/usr/bin/env python3
"""
Performance Monitoring Utilities
Extracted from original sam_yolo_character_segment.py
"""

import time
import psutil
import torch
import gc
from typing import Dict, Optional


class PerformanceMonitor:
    """
    å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç›£è¦–ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self):
        self.start_time = None
        self.stage_times = {}
        self.memory_usage = {}
        self.current_stage = None
        
    def start_monitoring(self):
        """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹"""
        self.start_time = time.time()
        self.log_system_info()
        
    def start_stage(self, stage_name: str):
        """å‡¦ç†æ®µéšã®é–‹å§‹"""
        if self.current_stage:
            self.end_stage()
        
        self.current_stage = stage_name
        self.stage_times[stage_name] = time.time()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨˜éŒ²
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        gpu_memory = self.get_gpu_memory() if torch.cuda.is_available() else 0
        
        print(f"ğŸ”„ é–‹å§‹: {stage_name} (RAM: {memory_mb:.1f}MB, GPU: {gpu_memory:.1f}MB)")
        
    def end_stage(self):
        """å‡¦ç†æ®µéšã®çµ‚äº†"""
        if not self.current_stage:
            return
            
        elapsed = time.time() - self.stage_times[self.current_stage]
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨˜éŒ²
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        gpu_memory = self.get_gpu_memory() if torch.cuda.is_available() else 0
        
        print(f"âœ… å®Œäº†: {self.current_stage} ({elapsed:.2f}ç§’, RAM: {memory_mb:.1f}MB, GPU: {gpu_memory:.1f}MB)")
        
        self.stage_times[self.current_stage] = elapsed
        self.current_stage = None
        
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def get_gpu_memory(self) -> float:
        """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        try:
            if torch.cuda.is_available():
                return torch.cuda.memory_allocated() / 1024 / 1024
            return 0.0
        except Exception:
            return 0.0
    
    def log_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        print("ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        
        # CPUæƒ…å ±
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"   CPU: {cpu_count}ã‚³ã‚¢, ä½¿ç”¨ç‡: {cpu_percent}%")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory = psutil.virtual_memory()
        memory_gb = memory.total / 1024 / 1024 / 1024
        memory_available_gb = memory.available / 1024 / 1024 / 1024
        print(f"   ãƒ¡ãƒ¢ãƒª: {memory_available_gb:.1f}GB / {memory_gb:.1f}GB åˆ©ç”¨å¯èƒ½")
        
        # GPUæƒ…å ±
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            gpu_name = torch.cuda.get_device_name(current_device)
            gpu_memory_total = torch.cuda.get_device_properties(current_device).total_memory / 1024 / 1024 / 1024
            print(f"   GPU: {gpu_name} ({gpu_count}åŸº), VRAM: {gpu_memory_total:.1f}GB")
        else:
            print("   GPU: åˆ©ç”¨ä¸å¯ (CPUãƒ¢ãƒ¼ãƒ‰)")
    
    def get_total_time(self) -> float:
        """ç·å‡¦ç†æ™‚é–“ã‚’å–å¾—"""
        if self.start_time:
            return time.time() - self.start_time
        return 0.0
    
    def get_stage_summary(self) -> Dict[str, float]:
        """å„æ®µéšã®å‡¦ç†æ™‚é–“ã‚µãƒãƒªã‚’å–å¾—"""
        return self.stage_times.copy()
    
    def print_summary(self):
        """å‡¦ç†æ™‚é–“ã‚µãƒãƒªã‚’å‡ºåŠ›"""
        total_time = self.get_total_time()
        
        print("\nğŸ“Š å‡¦ç†æ™‚é–“ã‚µãƒãƒª:")
        print(f"   ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        
        for stage, duration in self.stage_times.items():
            percentage = (duration / total_time * 100) if total_time > 0 else 0
            print(f"   {stage}: {duration:.2f}ç§’ ({percentage:.1f}%)")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        gpu_memory = self.get_gpu_memory() if torch.cuda.is_available() else 0
        
        print(f"\nğŸ’¾ æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
        print(f"   RAM: {memory_mb:.1f}MB")
        if torch.cuda.is_available():
            print(f"   GPU: {gpu_memory:.1f}MB")


class ResourceManager:
    """
    ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹
    """
    
    @staticmethod
    def check_available_memory() -> Dict[str, float]:
        """åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã‚’ãƒã‚§ãƒƒã‚¯"""
        memory = psutil.virtual_memory()
        
        result = {
            'total_gb': memory.total / 1024 / 1024 / 1024,
            'available_gb': memory.available / 1024 / 1024 / 1024,
            'percent': memory.percent
        }
        
        if torch.cuda.is_available():
            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024
            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024 / 1024 / 1024
            gpu_memory_free = gpu_memory_total - gpu_memory_allocated
            
            result.update({
                'gpu_total_gb': gpu_memory_total,
                'gpu_allocated_gb': gpu_memory_allocated,
                'gpu_free_gb': gpu_memory_free
            })
        
        return result
    
    @staticmethod
    def cleanup_memory():
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    @staticmethod
    def check_system_requirements() -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        memory = psutil.virtual_memory()
        available_gb = memory.available / 1024 / 1024 / 1024
        
        # æœ€ä½4GBå¿…è¦
        if available_gb < 4.0:
            print(f"âŒ ãƒ¡ãƒ¢ãƒªä¸è¶³: {available_gb:.1f}GB (æœ€ä½4GBå¿…è¦)")
            return False
        
        print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶OK: {available_gb:.1f}GBåˆ©ç”¨å¯èƒ½")
        return True


if __name__ == "__main__":
    # Test performance monitoring
    monitor = PerformanceMonitor()
    monitor.start_monitoring()
    
    monitor.start_stage("Test Stage 1")
    time.sleep(1)
    monitor.end_stage()
    
    monitor.start_stage("Test Stage 2")
    time.sleep(0.5)
    monitor.end_stage()
    
    monitor.print_summary()
    
    # Test resource manager
    print("\nğŸ” Resource Check:")
    resources = ResourceManager.check_available_memory()
    for key, value in resources.items():
        print(f"   {key}: {value}")
    
    print(f"âœ… Performance module test completed")