#!/usr/bin/env python3
"""
æ¤œå‡ºå¤±æ•—åˆ†æã‚·ã‚¹ãƒ†ãƒ 
é¡”æ¤œå‡ºãƒ»ãƒãƒ¼ã‚ºæ¤œå‡ºã®å¤±æ•—åŸå› ã‚’è©³ç´°åˆ†æã—æ”¹å–„ç­–ã‚’ææ¡ˆ
"""

import numpy as np
import cv2

import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from features.evaluation.enhanced_detection_systems import (
    EnhancedFaceDetector,
    EnhancedPoseDetector,
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DetectionFailureAnalyzer:
    """æ¤œå‡ºå¤±æ•—åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.face_detector = EnhancedFaceDetector()
        self.pose_detector = EnhancedPoseDetector()
        self.logger = logging.getLogger(f"{__name__}.DetectionFailureAnalyzer")
    
    def analyze_image_characteristics(self, image_path: str) -> Dict:
        """ç”»åƒç‰¹æ€§åˆ†æ"""
        image = cv2.imread(image_path)
        if image is None:
            return {"error": "ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—"}
        
        # åŸºæœ¬ç”»åƒç‰¹æ€§
        height, width = image.shape[:2]
        area = height * width
        aspect_ratio = width / height
        
        # è‰²å½©ç‰¹æ€§
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray)
        contrast = np.std(gray)
        
        # ã‚¨ãƒƒã‚¸å¯†åº¦
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / area
        
        # è‰²åˆ†å¸ƒ
        color_hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
        color_entropy = -np.sum(color_hist * np.log2(color_hist + 1e-10))
        
        return {
            "dimensions": {"width": width, "height": height, "area": area},
            "aspect_ratio": aspect_ratio,
            "brightness": brightness,
            "contrast": contrast,
            "edge_density": edge_density,
            "color_entropy": color_entropy
        }
    
    def analyze_face_detection_failure(self, image_path: str) -> Dict:
        """é¡”æ¤œå‡ºå¤±æ•—åˆ†æ"""
        image = cv2.imread(image_path)
        if image is None:
            return {"error": "ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—"}
        
        # å„æ‰‹æ³•ã§ã®æ¤œå‡ºè©¦è¡Œ
        detections = self.face_detector.detect_faces_comprehensive(image)
        
        # MediaPipeè©³ç´°åˆ†æ
        mp_analysis = self._analyze_mediapipe_face_failure(image)
        
        # OpenCVè©³ç´°åˆ†æ
        cv_analysis = self._analyze_opencv_face_failure(image)
        
        # æ¨å®šå¤±æ•—è¦å› 
        failure_factors = self._identify_face_failure_factors(image, detections)
        
        return {
            "detections_found": len(detections),
            "detection_details": [
                {
                    "method": d.method,
                    "confidence": d.confidence,
                    "bbox": d.bbox
                } for d in detections
            ],
            "mediapipe_analysis": mp_analysis,
            "opencv_analysis": cv_analysis,
            "failure_factors": failure_factors
        }
    
    def _analyze_mediapipe_face_failure(self, image: np.ndarray) -> Dict:
        """MediaPipeé¡”æ¤œå‡ºå¤±æ•—åˆ†æ"""
        try:
            import mediapipe as mp

            # è¤‡æ•°ã®é–¾å€¤ã§ãƒ†ã‚¹ãƒˆ
            thresholds = [0.1, 0.3, 0.5, 0.7]
            results = {}
            
            for threshold in thresholds:
                detector = mp.solutions.face_detection.FaceDetection(
                    model_selection=1,
                    min_detection_confidence=threshold
                )
                
                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                detection_results = detector.process(rgb_image)
                
                face_count = len(detection_results.detections) if detection_results.detections else 0
                results[f"threshold_{threshold}"] = face_count
            
            return {
                "threshold_analysis": results,
                "recommended_threshold": self._find_optimal_threshold(results)
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def _analyze_opencv_face_failure(self, image: np.ndarray) -> Dict:
        """OpenCVé¡”æ¤œå‡ºå¤±æ•—åˆ†æ"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # è¤‡æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
        scale_factors = [1.05, 1.1, 1.2, 1.3]
        min_neighbors = [2, 3, 4, 5]
        min_sizes = [(15, 15), (20, 20), (30, 30), (40, 40)]
        
        results = {}
        best_config = None
        max_detections = 0
        
        for cascade_name, cascade in self.face_detector.cascade_detectors.items():
            cascade_results = {}
            
            for scale in scale_factors:
                for neighbors in min_neighbors:
                    for min_size in min_sizes:
                        try:
                            faces = cascade.detectMultiScale(
                                gray,
                                scaleFactor=scale,
                                minNeighbors=neighbors,
                                minSize=min_size
                            )
                            
                            face_count = len(faces)
                            config_key = f"scale_{scale}_neighbors_{neighbors}_size_{min_size[0]}"
                            cascade_results[config_key] = face_count
                            
                            if face_count > max_detections:
                                max_detections = face_count
                                best_config = {
                                    "cascade": cascade_name,
                                    "scale_factor": scale,
                                    "min_neighbors": neighbors,
                                    "min_size": min_size,
                                    "detections": face_count
                                }
                                
                        except Exception as e:
                            continue
            
            results[cascade_name] = cascade_results
        
        return {
            "parameter_analysis": results,
            "best_configuration": best_config,
            "max_detections": max_detections
        }
    
    def _identify_face_failure_factors(self, image: np.ndarray, detections: List) -> List[str]:
        """é¡”æ¤œå‡ºå¤±æ•—è¦å› ç‰¹å®š"""
        factors = []
        
        height, width = image.shape[:2]
        area = height * width
        
        # ç”»åƒã‚µã‚¤ã‚ºè¦å› 
        if area < 100000:  # 100K pixelsæœªæº€
            factors.append("ç”»åƒè§£åƒåº¦ãŒä½ã„ï¼ˆ<100K pixelsï¼‰")
        
        # æ˜åº¦è¦å› 
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray)
        if brightness < 50:
            factors.append("ç”»åƒãŒæš—ã™ãã‚‹ï¼ˆå¹³å‡æ˜åº¦<50ï¼‰")
        elif brightness > 200:
            factors.append("ç”»åƒãŒæ˜ã‚‹ã™ãã‚‹ï¼ˆå¹³å‡æ˜åº¦>200ï¼‰")
        
        # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆè¦å› 
        contrast = np.std(gray)
        if contrast < 20:
            factors.append("ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒä½ã„ï¼ˆæ¨™æº–åå·®<20ï¼‰")
        
        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”è¦å› 
        aspect_ratio = width / height
        if aspect_ratio > 2.0 or aspect_ratio < 0.5:
            factors.append(f"æ¥µç«¯ãªã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼ˆ{aspect_ratio:.2f}ï¼‰")
        
        # é¡”ã‚µã‚¤ã‚ºæ¨å®šï¼ˆå¤§ã¾ã‹ãªæ¨å®šï¼‰
        expected_face_size = min(width, height) * 0.1  # ç”»åƒã®10%ç¨‹åº¦
        if expected_face_size < 30:
            factors.append("æ¨å®šé¡”ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹ï¼ˆ<30pxï¼‰")
        
        # æ¤œå‡ºçµæœã«ã‚ˆã‚‹åˆ†æ
        if len(detections) == 0:
            factors.append("å…¨æ‰‹æ³•ã§æ¤œå‡ºå¤±æ•—")
        elif len(detections) == 1:
            factors.append("å˜ä¸€æ‰‹æ³•ã®ã¿æ¤œå‡ºæˆåŠŸ")
        
        return factors
    
    def _find_optimal_threshold(self, threshold_results: Dict) -> float:
        """æœ€é©é–¾å€¤ã®æ¨å®š"""
        best_threshold = 0.3
        max_detections = 0
        
        for threshold_key, detection_count in threshold_results.items():
            if detection_count > max_detections:
                max_detections = detection_count
                best_threshold = float(threshold_key.split('_')[1])
        
        return best_threshold
    
    def analyze_pose_detection_failure(self, image_path: str) -> Dict:
        """ãƒãƒ¼ã‚ºæ¤œå‡ºå¤±æ•—åˆ†æ"""
        image = cv2.imread(image_path)
        if image is None:
            return {"error": "ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—"}
        
        # ãƒãƒ¼ã‚ºæ¤œå‡ºå®Ÿè¡Œ
        pose_result = self.pose_detector.detect_pose_comprehensive(image)
        
        # å¤±æ•—è¦å› åˆ†æ
        failure_factors = self._identify_pose_failure_factors(image, pose_result)
        
        # é–¾å€¤åˆ†æ
        threshold_analysis = self._analyze_pose_thresholds(image)
        
        return {
            "pose_detected": pose_result.detected,
            "pose_details": {
                "category": pose_result.pose_category,
                "visibility_score": pose_result.visibility_score,
                "completeness_score": pose_result.completeness_score,
                "confidence": pose_result.confidence,
                "keypoints_detected": pose_result.keypoints_detected
            } if pose_result.detected else None,
            "failure_factors": failure_factors,
            "threshold_analysis": threshold_analysis
        }
    
    def _identify_pose_failure_factors(self, image: np.ndarray, pose_result) -> List[str]:
        """ãƒãƒ¼ã‚ºæ¤œå‡ºå¤±æ•—è¦å› ç‰¹å®š"""
        factors = []
        
        if not pose_result.detected:
            factors.append("MediaPipeãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡ºå¤±æ•—")
        else:
            if pose_result.visibility_score < 0.3:
                factors.append(f"å¯è¦–æ€§ã‚¹ã‚³ã‚¢ä½ä¸‹ï¼ˆ{pose_result.visibility_score:.3f}ï¼‰")
            
            if pose_result.completeness_score < 0.5:
                factors.append(f"å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ä½ä¸‹ï¼ˆ{pose_result.completeness_score:.3f}ï¼‰")
            
            if pose_result.keypoints_detected < 10:
                factors.append(f"æ¤œå‡ºã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°ä¸è¶³ï¼ˆ{pose_result.keypoints_detected}/33ï¼‰")
        
        # ç”»åƒç‰¹æ€§ã«ã‚ˆã‚‹è¦å› æ¨å®š
        height, width = image.shape[:2]
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # äººä½“ã‚µã‚¤ã‚ºæ¨å®š
        estimated_body_height = height * 0.7  # ç”»åƒã®70%ç¨‹åº¦
        if estimated_body_height < 200:
            factors.append("æ¨å®šäººä½“ã‚µã‚¤ã‚ºãŒå°ã•ã„ï¼ˆ<200pxé«˜ï¼‰")
        
        # èƒŒæ™¯è¤‡é›‘åº¦
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (width * height)
        if edge_density > 0.3:
            factors.append(f"èƒŒæ™¯ãŒè¤‡é›‘ï¼ˆã‚¨ãƒƒã‚¸å¯†åº¦: {edge_density:.3f}ï¼‰")
        
        return factors
    
    def _analyze_pose_thresholds(self, image: np.ndarray) -> Dict:
        """ãƒãƒ¼ã‚ºæ¤œå‡ºé–¾å€¤åˆ†æ"""
        try:
            import mediapipe as mp
            
            thresholds = [0.1, 0.3, 0.5, 0.7]
            results = {}
            
            for threshold in thresholds:
                pose_detector = mp.solutions.pose.Pose(
                    static_image_mode=True,
                    model_complexity=2,
                    enable_segmentation=True,
                    min_detection_confidence=threshold
                )
                
                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                pose_results = pose_detector.process(rgb_image)
                
                detected = pose_results.pose_landmarks is not None
                keypoint_count = len(pose_results.pose_landmarks.landmark) if detected else 0
                
                results[f"threshold_{threshold}"] = {
                    "detected": detected,
                    "keypoints": keypoint_count
                }
            
            return results
            
        except Exception as e:
            return {"error": str(e)}


def analyze_dataset_failures():
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
    logger.info("ğŸ” æ¤œå‡ºå¤±æ•—åˆ†æé–‹å§‹")
    logger.info("=" * 70)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    test_directories = [
        "/mnt/c/AItools/lora/train/yado/org/kana05_cursor_fix",
        "/mnt/c/AItools/lora/train/yado/org/kana07_cursor_fix",
        "/mnt/c/AItools/lora/train/yado/org/kana08_cursor_fix"
    ]
    
    analyzer = DetectionFailureAnalyzer()
    
    all_results = []
    face_failure_patterns = {}
    pose_failure_patterns = {}
    
    for test_dir in test_directories:
        dir_path = Path(test_dir)
        
        if not dir_path.exists():
            continue
        
        logger.info(f"\nğŸ“ åˆ†æãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {dir_path.name}")
        
        # JPGãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡ï¼ˆå…ƒç”»åƒï¼‰
        jpg_files = list(dir_path.glob("*.jpg"))
        
        for jpg_file in jpg_files:
            logger.info(f"  ğŸ” åˆ†æä¸­: {jpg_file.name}")
            
            try:
                # ç”»åƒç‰¹æ€§åˆ†æ
                image_chars = analyzer.analyze_image_characteristics(str(jpg_file))
                
                # é¡”æ¤œå‡ºå¤±æ•—åˆ†æ
                face_analysis = analyzer.analyze_face_detection_failure(str(jpg_file))
                
                # ãƒãƒ¼ã‚ºæ¤œå‡ºå¤±æ•—åˆ†æ
                pose_analysis = analyzer.analyze_pose_detection_failure(str(jpg_file))
                
                result = {
                    "filename": jpg_file.name,
                    "directory": dir_path.name,
                    "image_characteristics": image_chars,
                    "face_analysis": face_analysis,
                    "pose_analysis": pose_analysis
                }
                
                all_results.append(result)
                
                # å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åé›†
                if face_analysis.get("detections_found", 0) == 0:
                    for factor in face_analysis.get("failure_factors", []):
                        face_failure_patterns[factor] = face_failure_patterns.get(factor, 0) + 1
                
                if not pose_analysis.get("pose_detected", False):
                    for factor in pose_analysis.get("failure_factors", []):
                        pose_failure_patterns[factor] = pose_failure_patterns.get(factor, 0) + 1
                
            except Exception as e:
                logger.error(f"    âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                continue
    
    # åˆ†æçµæœã‚µãƒãƒªãƒ¼
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š æ¤œå‡ºå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœ")
    logger.info("=" * 70)
    
    logger.info(f"\nğŸ‘¤ é¡”æ¤œå‡ºå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¸Šä½5ä½ï¼‰:")
    sorted_face_patterns = sorted(face_failure_patterns.items(), key=lambda x: x[1], reverse=True)
    for i, (pattern, count) in enumerate(sorted_face_patterns[:5]):
        logger.info(f"  {i+1}. {pattern}: {count}ä»¶")
    
    logger.info(f"\nğŸ¤¸ ãƒãƒ¼ã‚ºæ¤œå‡ºå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¸Šä½5ä½ï¼‰:")
    sorted_pose_patterns = sorted(pose_failure_patterns.items(), key=lambda x: x[1], reverse=True)
    for i, (pattern, count) in enumerate(sorted_pose_patterns[:5]):
        logger.info(f"  {i+1}. {pattern}: {count}ä»¶")
    
    # æ”¹å–„ææ¡ˆ
    logger.info(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
    
    # é¡”æ¤œå‡ºæ”¹å–„ææ¡ˆ
    logger.info(f"  ğŸ‘¤ é¡”æ¤œå‡ºæ”¹å–„ç­–:")
    if "å…¨æ‰‹æ³•ã§æ¤œå‡ºå¤±æ•—" in face_failure_patterns:
        logger.info(f"    - MediaPipeé–¾å€¤ã‚’0.1ã«ä¸‹ã’ã‚‹")
        logger.info(f"    - OpenCV minNeighbors ã‚’2ã«ä¸‹ã’ã‚‹")
        logger.info(f"    - ã‚¢ãƒ‹ãƒ¡é¡”å°‚ç”¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®è¿½åŠ ")
    
    if "ç”»åƒãŒæš—ã™ãã‚‹" in face_failure_patterns or "ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒä½ã„" in face_failure_patterns:
        logger.info(f"    - å‰å‡¦ç†ã§ã®æ˜åº¦ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´")
        logger.info(f"    - ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å‡åŒ–ã®é©ç”¨")
    
    # ãƒãƒ¼ã‚ºæ¤œå‡ºæ”¹å–„ææ¡ˆ
    logger.info(f"  ğŸ¤¸ ãƒãƒ¼ã‚ºæ¤œå‡ºæ”¹å–„ç­–:")
    if "MediaPipeãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡ºå¤±æ•—" in pose_failure_patterns:
        logger.info(f"    - MediaPipe Poseé–¾å€¤ã‚’0.1ã«ä¸‹ã’ã‚‹")
        logger.info(f"    - ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦ã‚’1ï¼ˆé«˜é€Ÿï¼‰ã§ã‚‚è©¦è¡Œ")
    
    if "æ¤œå‡ºã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°ä¸è¶³" in pose_failure_patterns:
        logger.info(f"    - éƒ¨åˆ†çš„ãƒãƒ¼ã‚ºã§ã‚‚è©•ä¾¡å¯¾è±¡ã¨ã™ã‚‹")
        logger.info(f"    - ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆé–¾å€¤ã‚’10â†’5ã«ä¸‹ã’ã‚‹")
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_data = {
        "timestamp": datetime.now().isoformat(),
        "analysis_type": "detection_failure_analysis",
        "total_analyzed": len(all_results),
        "face_failure_patterns": face_failure_patterns,
        "pose_failure_patterns": pose_failure_patterns,
        "detailed_results": all_results
    }
    
    report_file = f"evaluation_reports/detection_failure_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    os.makedirs("evaluation_reports", exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
    logger.info("âœ… æ¤œå‡ºå¤±æ•—åˆ†æå®Œäº†")
    
    return all_results


if __name__ == "__main__":
    try:
        results = analyze_dataset_failures()
        print(f"\nğŸ” åˆ†æçµæœ: {len(results)}ä»¶ã®ç”»åƒã‚’åˆ†æå®Œäº†")
    except Exception as e:
        logger.error(f"âŒ åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)