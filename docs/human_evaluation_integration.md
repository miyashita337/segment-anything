# äººé–“è©•ä¾¡çµ±åˆæ–¹é‡

**ä½œæˆæ—¥**: 2025-07-24  
**ç›®çš„**: å®¢è¦³çš„3æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ ã¨äººé–“è©•ä¾¡ã®é©åˆ‡ãªçµ±åˆ

## ğŸ¯ çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### åŸºæœ¬æ–¹é‡ï¼šã€Œå®¢è¦³è©•ä¾¡ï¼‹äººé–“æ¤œè¨¼ã€

```yaml
evaluation_hierarchy:
  primary: "å®¢è¦³çš„3æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ ï¼ˆPLA/SCI/PLEï¼‰"
  secondary: "äººé–“è©•ä¾¡ï¼ˆæœ€çµ‚å“è³ªæ¤œè¨¼ï¼‰"
  
  ratio: "å®¢è¦³90% : äººé–“10%"
  
  human_role: "æœ€çµ‚å“è³ªä¿è¨¼ãƒ»å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§ç¢ºèª"
  objective_role: "æ—¥å¸¸å“è³ªç›£è¦–ãƒ»ç¶™ç¶šæ”¹å–„è¿½è·¡"
```

## ğŸ“Š å®¢è¦³è©•ä¾¡ã¨äººé–“è©•ä¾¡ã®åˆ†æ‹…

### ğŸ¤– å®¢è¦³çš„è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®æ‹…å½“é ˜åŸŸ

#### æ—¥å¸¸é‹ç”¨ãƒ»ç›£è¦–
```yaml
objective_evaluation_scope:
  daily_monitoring:
    - "PLAï¼ˆIoUï¼‰ã«ã‚ˆã‚‹æŠ½å‡ºç²¾åº¦æ¸¬å®š"
    - "SCIï¼ˆMediaPipeï¼‰ã«ã‚ˆã‚‹æ§‹é€ å®Œå…¨æ€§è©•ä¾¡"
    - "PLEï¼ˆæ™‚ç³»åˆ—ï¼‰ã«ã‚ˆã‚‹å­¦ç¿’åŠ¹ç‡è¿½è·¡"
    
  continuous_improvement:
    - "æ€§èƒ½ãƒˆãƒ¬ãƒ³ãƒ‰ã®è‡ªå‹•æ¤œå‡º"
    - "é€€è¡Œãƒ»åœæ»ã‚¢ãƒ©ãƒ¼ãƒˆã®è‡ªå‹•ç™ºè¡Œ"
    - "ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³é”æˆåº¦ã®å®šé‡è¿½è·¡"
    
  batch_processing:
    - "å¤§é‡ç”»åƒã®ä¸€æ‹¬å“è³ªè©•ä¾¡"
    - "çµ±è¨ˆçš„å‚¾å‘åˆ†æ"
    - "ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"
```

### ğŸ‘¤ äººé–“è©•ä¾¡ã®æ‹…å½“é ˜åŸŸ

#### LoRAå­¦ç¿’å“è³ªä¿è¨¼
```yaml
human_evaluation_scope:
  final_quality_assurance:
    - "LoRAå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€çµ‚æ¤œè¨¼"
    - "å­¦ç¿’åŠ¹æœã®è¦–è¦šçš„ç¢ºèª"
    - "ç”Ÿæˆç”»åƒå“è³ªã®ä¸»è¦³è©•ä¾¡"
    
  edge_cases:
    - "å®¢è¦³è©•ä¾¡ã§åˆ¤å®šå›°é›£ãªã‚±ãƒ¼ã‚¹"
    - "èŠ¸è¡“çš„ãƒ»ç¾çš„å“è³ªã®è©•ä¾¡"
    - "æ–‡è„ˆçš„å¦¥å½“æ€§ã®ç¢ºèª"
    
  system_validation:
    - "æ–°æ©Ÿèƒ½å°å…¥æ™‚ã®æ„Ÿè¦šçš„å“è³ªç¢ºèª"
    - "å®¢è¦³æŒ‡æ¨™ã¨å®Ÿç”¨å“è³ªã®ç›¸é–¢æ¤œè¨¼"
    - "ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®ç·åˆè©•ä¾¡"
```

## ğŸ”„ çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### Phase 1: å®¢è¦³è©•ä¾¡ä¸»å°ãƒ•ãƒ­ãƒ¼

```mermaid
flowchart TD
    A[ç”»åƒãƒãƒƒãƒå‡¦ç†] --> B[å®¢è¦³çš„3æŒ‡æ¨™è¨ˆç®—]
    B --> C{å“è³ªåŸºæº–åˆ¤å®š}
    
    C -->|PLAâ‰¥0.85<br/>SCIâ‰¥0.80| D[è‡ªå‹•æ‰¿èª]
    C -->|0.75â‰¤PLA<lt>0.85<br/>0.70â‰¤SCI<lt>0.80| E[æ¡ä»¶ä»˜æ‰¿èª]
    C -->|PLA<lt>0.75<br/>SCI<lt>0.70| F[è‡ªå‹•ä¸æ‰¿èª]
    
    D --> G[LoRAå­¦ç¿’ã‚»ãƒƒãƒˆã«è¿½åŠ ]
    E --> H[äººé–“è©•ä¾¡ã‚­ãƒ¥ãƒ¼ã«é€ä¿¡]
    F --> I[æ”¹å–„å‡¦ç†ã¸]
    
    H --> J[äººé–“æœ€çµ‚åˆ¤å®š]
    J -->|æ‰¿èª| G
    J -->|ä¸æ‰¿èª| I
    
    classDef auto fill:#c8e6c9,stroke:#388e3c
    classDef human fill:#fff3e0,stroke:#f57c00
    classDef rejected fill:#ffcdd2,stroke:#d32f2f
    
    class B,C,D,F auto
    class H,J human
    class I rejected
```

### Phase 2: äººé–“è©•ä¾¡ã®åŠ¹ç‡åŒ–

#### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥
```python
def select_human_evaluation_samples(batch_results: List[ObjectiveResult]) -> List[ObjectiveResult]:
    """åŠ¹ç‡çš„ãªäººé–“è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«é¸æŠ"""
    
    # 1. å¿…é ˆè©•ä¾¡å¯¾è±¡ï¼ˆå…¨ã¦äººé–“ç¢ºèªï¼‰
    mandatory_samples = []
    
    # å¢ƒç•Œå€¤ä»˜è¿‘ã®çµæœ
    boundary_samples = [r for r in batch_results 
                       if 0.75 <= r.pla_score < 0.85 or 0.70 <= r.sci_score < 0.80]
    
    # å®¢è¦³è©•ä¾¡ã§é«˜ã‚¹ã‚³ã‚¢ã ãŒä¸è‡ªç„¶ãªçµæœ
    suspicious_high_scores = [r for r in batch_results 
                             if r.pla_score >= 0.90 and r.sci_score >= 0.85 
                             and detect_anomaly(r)]
    
    # 2. çµ±è¨ˆçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå“è³ªä¿è¨¼ç”¨ï¼‰
    # é«˜å“è³ªçµæœã®10%ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    high_quality = [r for r in batch_results if r.pla_score >= 0.85 and r.sci_score >= 0.80]
    random_samples = random.sample(high_quality, max(1, len(high_quality) // 10))
    
    return mandatory_samples + boundary_samples + suspicious_high_scores + random_samples

def detect_anomaly(result: ObjectiveResult) -> bool:
    """å®¢è¦³çš„ã«é«˜ã‚¹ã‚³ã‚¢ã ãŒäººé–“ç¢ºèªãŒå¿…è¦ãªç•°å¸¸ã‚’æ¤œå‡º"""
    # ä¾‹ï¼šPLAé«˜ã„ãŒè¦–è¦šçš„ã«ä¸è‡ªç„¶ï¼ˆèƒŒæ™¯æ··å…¥ç­‰ï¼‰
    return (result.pla_score >= 0.90 and 
            result.background_contamination_score > 0.3)
```

#### äººé–“è©•ä¾¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
```python
class HumanEvaluationInterface:
    """åŠ¹ç‡çš„ãªäººé–“è©•ä¾¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    def __init__(self):
        self.evaluation_queue = deque()
        self.results_cache = {}
    
    def present_for_evaluation(self, sample: ObjectiveResult) -> HumanEvaluationResult:
        """äººé–“è©•ä¾¡ç”¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¡¨ç¤º"""
        
        # å®¢è¦³çš„æŒ‡æ¨™ã®äº‹å‰è¡¨ç¤º
        context = {
            "pla_score": sample.pla_score,
            "sci_score": sample.sci_score,
            "objective_recommendation": self._get_objective_recommendation(sample),
            "similar_cases_history": self._get_similar_evaluations(sample)
        }
        
        # åŠ¹ç‡çš„ãªè©•ä¾¡è³ªå•ï¼ˆYes/Noå½¢å¼ä¸­å¿ƒï¼‰
        questions = [
            "LoRAå­¦ç¿’ã«é©ã—ãŸå“è³ªã§ã™ã‹ï¼Ÿ [Y/n]",
            "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®é‡è¦éƒ¨ä½ã¯å®Œå…¨ã«å«ã¾ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ [Y/n]", 
            "èƒŒæ™¯ã®æ··å…¥ã¯è¨±å®¹ç¯„å›²å†…ã§ã™ã‹ï¼Ÿ [Y/n]"
        ]
        
        return self._collect_human_input(sample, context, questions)
    
    def _get_objective_recommendation(self, sample: ObjectiveResult) -> str:
        """å®¢è¦³è©•ä¾¡ã«åŸºã¥ãæ¨å¥¨åˆ¤å®šã‚’è¡¨ç¤º"""
        if sample.pla_score >= 0.85 and sample.sci_score >= 0.80:
            return "å®¢è¦³è©•ä¾¡ï¼šæ‰¿èªæ¨å¥¨ï¼ˆé«˜å“è³ªï¼‰"
        elif sample.pla_score >= 0.75 and sample.sci_score >= 0.70:
            return "å®¢è¦³è©•ä¾¡ï¼šæ¡ä»¶ä»˜æ‰¿èªï¼ˆæ¨™æº–å“è³ªï¼‰"
        else:
            return "å®¢è¦³è©•ä¾¡ï¼šä¸æ‰¿èªæ¨å¥¨ï¼ˆå“è³ªä¸è¶³ï¼‰"
```

## ğŸ“ˆ äººé–“è©•ä¾¡ã¨å®¢è¦³è©•ä¾¡ã®ç›¸é–¢åˆ†æ

### ç›¸é–¢æ€§ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
```python
class ObjectiveHumanCorrelationAnalyzer:
    """å®¢è¦³è©•ä¾¡ã¨äººé–“è©•ä¾¡ã®ç›¸é–¢åˆ†æ"""
    
    def analyze_correlation(self, evaluation_history: List[CombinedEvaluation]) -> CorrelationReport:
        """ç›¸é–¢åˆ†æã®å®Ÿè¡Œ"""
        
        # PLA vs äººé–“è©•ä¾¡ã®ç›¸é–¢
        pla_scores = [e.objective.pla_score for e in evaluation_history]
        human_scores = [e.human.overall_quality for e in evaluation_history]
        pla_correlation = scipy.stats.pearsonr(pla_scores, human_scores)
        
        # SCI vs äººé–“è©•ä¾¡ã®ç›¸é–¢
        sci_scores = [e.objective.sci_score for e in evaluation_history]
        sci_correlation = scipy.stats.pearsonr(sci_scores, human_scores)
        
        # ä¸ä¸€è‡´ã‚±ãƒ¼ã‚¹ã®åˆ†æ
        disagreement_cases = self._analyze_disagreements(evaluation_history)
        
        return CorrelationReport(
            pla_human_correlation=pla_correlation.statistic,
            sci_human_correlation=sci_correlation.statistic,
            disagreement_analysis=disagreement_cases,
            recommendations=self._generate_calibration_recommendations(pla_correlation, sci_correlation)
        )
    
    def _analyze_disagreements(self, history: List[CombinedEvaluation]) -> List[DisagreementCase]:
        """å®¢è¦³è©•ä¾¡ã¨äººé–“è©•ä¾¡ã®ä¸ä¸€è‡´ã‚±ãƒ¼ã‚¹åˆ†æ"""
        disagreements = []
        
        for eval_case in history:
            obj_quality = self._categorize_objective_quality(eval_case.objective)
            human_quality = eval_case.human.overall_quality
            
            # å¤§ããªä¸ä¸€è‡´ã®æ¤œå‡º
            if abs(obj_quality - human_quality) >= 2:  # 2æ®µéšä»¥ä¸Šã®å·®
                disagreements.append(DisagreementCase(
                    case=eval_case,
                    objective_prediction=obj_quality,
                    human_judgment=human_quality,
                    possible_reasons=self._analyze_disagreement_reasons(eval_case)
                ))
        
        return disagreements
```

## ğŸ¯ LoRAå­¦ç¿’ç‰¹åŒ–ã®äººé–“è©•ä¾¡

### LoRAå­¦ç¿’å“è³ªåŸºæº–
```yaml
lora_specific_criteria:
  character_consistency:
    weight: 40%
    description: "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ä¸€è²«ã—ãŸç‰¹å¾´ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹ã‹"
    evaluation: "ä¸»è¦³çš„åˆ¤å®šãŒå¿…è¦ï¼ˆå®¢è¦³è©•ä¾¡ã§ã¯æ¸¬å®šå›°é›£ï¼‰"
    
  artistic_quality:
    weight: 30%
    description: "èŠ¸è¡“çš„ãƒ»ç¾çš„ãªé­…åŠ›ãŒã‚ã‚‹ã‹"
    evaluation: "å®Œå…¨ã«ä¸»è¦³çš„åˆ¤å®š"
    
  training_suitability:
    weight: 30%
    description: "LoRAå­¦ç¿’ã«é©ã—ãŸãƒ‡ãƒ¼ã‚¿å½¢å¼ãƒ»å“è³ªã‹"
    evaluation: "çµŒé¨“ã«åŸºã¥ãåˆ¤å®š"
```

### LoRAå­¦ç¿’çµæœã®æ¤œè¨¼ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
```python
def validate_lora_training_quality(trained_model, test_prompts: List[str]) -> LoRAValidationReport:
    """LoRAå­¦ç¿’çµæœã®å“è³ªæ¤œè¨¼"""
    
    # 1. å®¢è¦³çš„æ¸¬å®š
    objective_metrics = {
        'feature_preservation': calculate_feature_preservation(trained_model),
        'generation_consistency': calculate_consistency_score(trained_model, test_prompts),
        'overfitting_degree': detect_overfitting_indicators(trained_model)
    }
    
    # 2. äººé–“ã«ã‚ˆã‚‹æœ€çµ‚å“è³ªåˆ¤å®š
    human_evaluation = {
        'character_likeness': human_evaluate_character_similarity(trained_model, test_prompts),
        'artistic_improvement': human_evaluate_artistic_quality(trained_model, test_prompts),
        'practical_usability': human_evaluate_practical_use(trained_model, test_prompts)
    }
    
    # 3. çµ±åˆåˆ¤å®š
    final_quality = integrate_objective_human_evaluation(objective_metrics, human_evaluation)
    
    return LoRAValidationReport(
        objective_metrics=objective_metrics,
        human_evaluation=human_evaluation,
        final_quality_score=final_quality,
        recommendations=generate_improvement_recommendations(objective_metrics, human_evaluation)
    )
```

## ğŸ“‹ çµ±åˆè©•ä¾¡ã®å®Ÿè£…æ‰‹é †

### Step 1: ç¾åœ¨ã®å®¢è¦³è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ç¢ºç«‹ï¼ˆé€²è¡Œä¸­ï¼‰
```bash
# å®¢è¦³çš„3æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ ã®å®Œæˆ
python tools/objective_evaluation_system.py --validate-implementation

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å“è³ªãƒ‡ãƒ¼ã‚¿ã®è“„ç©
python tools/accumulate_baseline_data.py --duration 2weeks
```

### Step 2: äººé–“è©•ä¾¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
```bash
# åŠ¹ç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
python tools/setup_human_evaluation_sampling.py

# äººé–“è©•ä¾¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å®Ÿè£…
python tools/create_human_evaluation_interface.py --mode efficient
```

### Step 3: ç›¸é–¢åˆ†æãƒ»ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
```bash
# å®¢è¦³è©•ä¾¡ã¨äººé–“è©•ä¾¡ã®ç›¸é–¢åˆ†æ
python tools/analyze_objective_human_correlation.py --period 1month

# é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
python tools/calibrate_quality_thresholds.py --based-on-correlation
```

## ğŸ”„ ç¶™ç¶šæ”¹å–„ã‚µã‚¤ã‚¯ãƒ«

### æœˆæ¬¡ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
```python
def monthly_calibration_cycle():
    """æœˆæ¬¡ã§ã®è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    # 1. ç›¸é–¢åˆ†æå®Ÿè¡Œ
    correlation_report = analyze_objective_human_correlation()
    
    # 2. é–¾å€¤èª¿æ•´
    if correlation_report.pla_human_correlation < 0.8:
        adjust_pla_thresholds()
    
    if correlation_report.sci_human_correlation < 0.8:
        adjust_sci_thresholds()
    
    # 3. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥æœ€é©åŒ–
    optimize_sampling_strategy(correlation_report)
    
    # 4. äººé–“è©•ä¾¡è² è·ã®æœ€é©åŒ–
    reduce_human_evaluation_load_if_possible()
```

---

**çµè«–**: å®¢è¦³çš„3æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ ã‚’ä¸»è»¸ã¨ã—ã€äººé–“è©•ä¾¡ã‚’æˆ¦ç•¥çš„ã«çµ±åˆã™ã‚‹ã“ã¨ã§ã€  
åŠ¹ç‡çš„ã‹ã¤é«˜å“è³ªãªLoRAå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã—ã¾ã™ã€‚