#!/usr/bin/env python3
"""
Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµ±åˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Run comprehensive benchmark evaluation for current YOLO+SAM system
"""

import json
import logging
import sys
import time
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from features.common.progress_reporter import ProgressReporter
from features.common.project_tracker import ProjectTracker
from features.evaluation.metrics_system import MetricsCalculator, MetricsVisualizer
from features.evaluation.phase0_benchmark import Phase0Benchmark

logger = logging.getLogger(__name__)


def setup_logging():
    """ãƒ­ã‚®ãƒ³ã‚°è¨­å®š"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler('phase0_benchmark.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )


def run_phase0_complete_benchmark():
    """Phase 0 å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    try:
        logger.info("=== Phase 0 çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹ ===")
        start_time = time.time()
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        tracker = ProjectTracker(project_root)
        
        # Phase 0é–‹å§‹ãƒãƒ¼ã‚¯
        tracker.update_task_status("phase0-start", "in_progress")
        
        # Step 1: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        logger.info("Step 1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®šé–‹å§‹")
        benchmark = Phase0Benchmark(project_root)
        summary = benchmark.run_full_benchmark()
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šå®Œäº†: {summary.total_images}ç”»åƒå‡¦ç†")
        logger.info(f"Largest-Character Accuracy: {summary.largest_char_accuracy:.1%}")
        logger.info(f"A/Bè©•ä¾¡ç‡: {summary.ab_evaluation_rate:.1%}")
        
        # Step 2: è©³ç´°è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        logger.info("Step 2: è©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰")
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ç”¨ã«å¤‰æ›
        results_data = [
            {
                'image_id': r.image_id,
                'largest_char_predicted': r.largest_char_predicted,
                'iou_score': r.iou_score,
                'confidence_score': r.confidence_score,
                'processing_time': r.processing_time,
                'character_count': r.character_count,
                'area_largest_ratio': r.area_largest_ratio,
                'quality_grade': r.quality_grade,
                'prediction_bbox': r.prediction_bbox
            }
            for r in benchmark.benchmark_results
        ]
        
        # è©³ç´°è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        calculator = MetricsCalculator(results_data)
        detailed_metrics = calculator.compute_all_metrics()
        
        logger.info("è©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†")
        
        # Step 3: å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        logger.info("Step 3: ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        
        metrics_output_dir = project_root / "benchmark_results" / "phase0" / "metrics"
        visualizer = MetricsVisualizer(detailed_metrics, metrics_output_dir)
        
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
        visualizer.create_performance_dashboard()
        visualizer.save_metrics_json()
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_content = generate_comprehensive_report(summary, detailed_metrics)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        report_file = project_root / "benchmark_results" / "phase0" / f"comprehensive_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²æ—æ›´æ–°
        tracker.add_task("phase0-metrics", "Largest-Character AccuracyæŒ‡æ¨™ã®ç¢ºç«‹", "phase0", "high", 4.0, ["phase0-benchmark"])
        tracker.update_task_status("phase0-metrics", "completed")
        
        tracker.add_task("phase0-evaluation", "101ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®æ€§èƒ½æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰", "phase0", "high", 6.0, ["phase0-metrics"])
        tracker.update_task_status("phase0-evaluation", "completed")
        
        # Phase 0å®Œäº†
        tracker.update_task_status("phase0-start", "completed")
        
        # é€²æ—ãƒ¬ãƒãƒ¼ãƒˆæ›´æ–°
        reporter = ProgressReporter(project_root)
        progress_report_file = reporter.generate_full_report()
        
        total_time = time.time() - start_time
        
        logger.info("=== Phase 0 çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† ===")
        logger.info(f"ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        logger.info(f"ç·åˆãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
        logger.info(f"é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ: {progress_report_file}")
        logger.info(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: {metrics_output_dir / 'performance_dashboard.png'}")
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "="*60)
        print("ğŸ¯ Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        print(f"ğŸ“Š å‡¦ç†ç”»åƒæ•°: {summary.total_images}æš")
        print(f"ğŸ¯ Largest-Character Accuracy: {detailed_metrics.largest_char_accuracy.value:.1%}")
        print(f"ğŸ“ˆ Mean IoU: {detailed_metrics.mean_iou.value:.3f}")
        print(f"â­ A/Bè©•ä¾¡ç‡: {detailed_metrics.ab_evaluation_rate.value:.1%}")
        print(f"âš¡ å¹³å‡å‡¦ç†æ™‚é–“: {summary.mean_processing_time:.2f}ç§’")
        print(f"ğŸš€ FPS: {detailed_metrics.fps.value:.3f}")
        
        print(f"\nğŸ“‹ è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ:")
        for grade, count in summary.grade_distribution.items():
            percentage = (count / summary.total_images) * 100
            print(f"  {grade}: {count}æš ({percentage:.1f}%)")
        
        print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  ğŸ“ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
        print(f"  ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: {metrics_output_dir / 'performance_dashboard.png'}")
        print(f"  ğŸ“ˆ é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ: {progress_report_file}")
        
        # æ¬¡Phaseæº–å‚™ã®æè¨€
        print(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        if detailed_metrics.largest_char_accuracy.value < 0.6:
            print("  âš ï¸  ç²¾åº¦ãŒä½ã„ãŸã‚ã€Phase 1ã§ã®ã‚³ãƒæ¤œå‡ºæ”¹å–„ãŒæ€¥å‹™")
        print("  ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰é–‹å§‹ã‚’æ¨å¥¨")
        print("  ğŸ”§ Phase 1: ã‚³ãƒæ¤œå‡ºãƒãƒƒãƒˆæ§‹ç¯‰ã¸ã®ç§»è¡Œæº–å‚™")
        
        return True
        
    except Exception as e:
        logger.error(f"Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒˆãƒ©ãƒƒã‚«ãƒ¼æ›´æ–°
        try:
            tracker = ProjectTracker(project_root)
            tracker.update_task_status("phase0-start", "pending")  # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æˆ»ã™
        except:
            pass
            
        return False


def generate_comprehensive_report(summary, detailed_metrics):
    """ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    # æˆåŠŸåŸºæº–ã¨ã®æ¯”è¼ƒ
    targets = {
        "largest_char_accuracy": 0.80,
        "ab_evaluation_rate": 0.70,
        "mean_iou": 0.65,
        "fps": 0.2  # 5ç§’/ç”»åƒä»¥ä¸‹
    }
    
    def get_status_emoji(current, target):
        if current >= target:
            return "âœ…"
        elif current >= target * 0.8:
            return "ğŸŸ¡"
        else:
            return "ğŸ”´"
    
    report = f"""# Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯: ç·åˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {time.strftime('%Y-%m-%d %H:%M:%S')}  
**è©•ä¾¡å¯¾è±¡**: æ—¢å­˜YOLO+SAM+é¢ç©æœ€å¤§é¸æŠã‚·ã‚¹ãƒ†ãƒ   
**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: äººé–“ãƒ©ãƒ™ãƒ«101ãƒ•ã‚¡ã‚¤ãƒ«

---

## ğŸ¯ ç·åˆè©•ä¾¡çµæœ

### ä¸»è¦æŒ‡æ¨™

| æŒ‡æ¨™ | ç¾åœ¨å€¤ | ç›®æ¨™å€¤ | é”æˆç‡ | çŠ¶æ…‹ |
|------|--------|--------|--------|------|
| **Largest-Character Accuracy** | {detailed_metrics.largest_char_accuracy.value:.1%} | {targets['largest_char_accuracy']:.1%} | {(detailed_metrics.largest_char_accuracy.value/targets['largest_char_accuracy']*100):.1f}% | {get_status_emoji(detailed_metrics.largest_char_accuracy.value, targets['largest_char_accuracy'])} |
| **A/Bè©•ä¾¡ç‡** | {detailed_metrics.ab_evaluation_rate.value:.1%} | {targets['ab_evaluation_rate']:.1%} | {(detailed_metrics.ab_evaluation_rate.value/targets['ab_evaluation_rate']*100):.1f}% | {get_status_emoji(detailed_metrics.ab_evaluation_rate.value, targets['ab_evaluation_rate'])} |
| **Mean IoU** | {detailed_metrics.mean_iou.value:.3f} | {targets['mean_iou']:.3f} | {(detailed_metrics.mean_iou.value/targets['mean_iou']*100):.1f}% | {get_status_emoji(detailed_metrics.mean_iou.value, targets['mean_iou'])} |
| **å‡¦ç†é€Ÿåº¦ (FPS)** | {detailed_metrics.fps.value:.3f} | {targets['fps']:.3f} | {(detailed_metrics.fps.value/targets['fps']*100):.1f}% | {get_status_emoji(detailed_metrics.fps.value, targets['fps'])} |

### å“è³ªã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ

"""
    
    for grade, count in summary.grade_distribution.items():
        percentage = (count / summary.total_images) * 100
        bar_length = int(percentage / 2)  # 50%ã§25æ–‡å­—
        bar = "â–ˆ" * bar_length + "â–‘" * (25 - bar_length)
        report += f"**{grade}è©•ä¾¡**: {count:2d}æš ({percentage:4.1f}%) `{bar}`\n"
    
    report += f"""

---

## ğŸ“Š è©³ç´°åˆ†æ

### ç²¾åº¦åˆ†æ
- **æ­£è§£ç”»åƒæ•°**: {sum(1 for r in detailed_metrics.largest_char_accuracy.notes.split('/') if 'correct' in str(r))} / {summary.total_images}
- **å¹³å‡IoU**: {detailed_metrics.mean_iou.value:.3f}
- **mAP@0.5**: {detailed_metrics.map_50.value:.3f}
- **mAP@[.5:.95]**: {detailed_metrics.map_95.value:.3f}

### å‡¦ç†æ€§èƒ½
- **å¹³å‡å‡¦ç†æ™‚é–“**: {summary.mean_processing_time:.2f}ç§’/ç”»åƒ
- **FPS**: {detailed_metrics.fps.value:.3f}
- **æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: {detailed_metrics.memory_usage.value:.1f}GB

### ä¿¡é ¼åº¦çµ±è¨ˆ
"""
    
    if detailed_metrics.confidence_stats:
        report += f"""- **å¹³å‡ä¿¡é ¼åº¦**: {detailed_metrics.confidence_stats.get('mean', 0):.3f}
- **ä¿¡é ¼åº¦ç¯„å›²**: {detailed_metrics.confidence_stats.get('min', 0):.3f} - {detailed_metrics.confidence_stats.get('max', 0):.3f}
- **æ¨™æº–åå·®**: {detailed_metrics.confidence_stats.get('std', 0):.3f}
"""

    report += f"""

---

## ğŸ” å¤±æ•—ã‚±ãƒ¼ã‚¹åˆ†æ

"""
    
    if detailed_metrics.failure_analysis and detailed_metrics.failure_analysis.get('total_failures', 0) > 0:
        total_failures = detailed_metrics.failure_analysis['total_failures']
        failure_rate = detailed_metrics.failure_analysis['failure_rate']
        
        report += f"""**å¤±æ•—ç”»åƒæ•°**: {total_failures}æš ({failure_rate:.1%})

### å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
"""
        
        for pattern, data in detailed_metrics.failure_analysis.get('failure_patterns', {}).items():
            report += f"""
#### {pattern.replace('_', ' ').title()}
- **ä»¶æ•°**: {data['count']}æš ({data['rate']:.1%})
- **èª¬æ˜**: {data['description']}
"""
    else:
        report += "**å…¨ç”»åƒã§æˆåŠŸ** - å¤±æ•—ã‚±ãƒ¼ã‚¹ãªã—"

    report += f"""

---

## ğŸš€ æ”¹å–„æè¨€

### ç·Šæ€¥åº¦: HIGH
"""
    
    # æ”¹å–„æè¨€ã‚’å‹•çš„ç”Ÿæˆ
    recommendations = []
    
    if detailed_metrics.largest_char_accuracy.value < 0.5:
        recommendations.append("ğŸ”´ **Largest-Character Accuracy < 50%** - æ ¹æœ¬çš„ã‚·ã‚¹ãƒ†ãƒ è¦‹ç›´ã—ãŒå¿…è¦")
    elif detailed_metrics.largest_char_accuracy.value < 0.7:
        recommendations.append("ğŸŸ¡ **ç²¾åº¦æ”¹å–„ãŒå¿…è¦** - Phase 1ã§ã®ã‚³ãƒæ¤œå‡ºæ”¹å–„ã‚’å„ªå…ˆ")
        
    if detailed_metrics.ab_evaluation_rate.value < 0.3:
        recommendations.append("ğŸ”´ **å“è³ªè©•ä¾¡ãŒä½ã„** - æŠ½å‡ºå“è³ªã®æ ¹æœ¬çš„æ”¹å–„ãŒå¿…è¦")
        
    if detailed_metrics.fps.value < 0.1:  # 10ç§’/ç”»åƒã‚ˆã‚Šé…ã„
        recommendations.append("ğŸŸ¡ **å‡¦ç†é€Ÿåº¦ãŒé…ã„** - ãƒ¢ãƒ‡ãƒ«è»½é‡åŒ–ã¾ãŸã¯GPUæœ€é©åŒ–ãŒå¿…è¦")
    
    if not recommendations:
        recommendations.append("âœ… **è‰¯å¥½ãªåŸºæº–æ€§èƒ½** - Phase 1ã¸ã®ç§»è¡Œæº–å‚™ã‚’é–‹å§‹")
    
    for rec in recommendations:
        report += f"- {rec}\n"

    report += f"""

### Phase 1 æº–å‚™äº‹é …
1. **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰**
   - ç–‘ä¼¼ãƒ©ãƒ™ãƒ«ç”Ÿæˆ + äººæ‰‹ä¿®æ­£ã«ã‚ˆã‚‹3-5å€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
   - ä½œå“åˆ¥Stratifiedåˆ†å‰²ã§ã®CVæº–å‚™

2. **ã‚³ãƒæ¤œå‡ºãƒãƒƒãƒˆæº–å‚™**
   - Mask R-CNN/YOLOv8-segç’°å¢ƒæ§‹ç¯‰
   - COCOâ†’Manga109â†’è‡ªå‰ãƒ‡ãƒ¼ã‚¿ã®è»¢ç§»å­¦ç¿’æº–å‚™

3. **è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µ**
   - mIoUæ¸¬å®šã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

---

## ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«

- **è©³ç´°çµæœJSON**: `benchmark_results/phase0/latest_benchmark_results.json`
- **è©•ä¾¡æŒ‡æ¨™JSON**: `benchmark_results/phase0/metrics/evaluation_metrics.json` 
- **å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: `benchmark_results/phase0/metrics/performance_dashboard.png`
- **å®Ÿè¡Œãƒ­ã‚°**: `phase0_benchmark.log`

---

## ğŸ“ Next Actions

### å³åº§ã«å®Ÿè¡Œ
1. **Phase 1é–‹å§‹æº–å‚™**: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
2. **èª²é¡Œå ±å‘Š**: å¤±æ•—ã‚±ãƒ¼ã‚¹è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
3. **ãƒªã‚½ãƒ¼ã‚¹ç¢ºä¿**: Phase 1å­¦ç¿’ç”¨GPUç’°å¢ƒç¢ºèª

### Phase 1ç›®æ¨™è¨­å®š
- **Largest-Character Accuracy**: 75%ä»¥ä¸Š
- **ã‚³ãƒæ¤œå‡ºmIoU**: 80%ä»¥ä¸Š
- **å‡¦ç†é€Ÿåº¦**: 2ç§’/ç”»åƒä»¥ä¸‹

---

*Phase 0 Benchmark Report - Generated by Automated Evaluation System*  
*ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²æ—: [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§ç¢ºèª]*
"""

    return report


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    setup_logging()
    
    logger.info("Phase 0 çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œé–‹å§‹")
    
    success = run_phase0_complete_benchmark()
    
    if success:
        logger.info("Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ­£å¸¸å®Œäº†")
        return 0
    else:
        logger.error("Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå¤±æ•—")
        return 1


if __name__ == "__main__":
    exit(main())