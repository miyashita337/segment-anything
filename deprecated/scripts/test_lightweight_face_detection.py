#!/usr/bin/env python3
"""
è»½é‡ç‰ˆé¡”æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆGPT-4Oæœ€é©åŒ–é©ç”¨ï¼‰
MediaPipeç„¡ã—ã€OpenCV+ã‚¢ãƒ‹ãƒ¡ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã®ã¿ã§é«˜é€Ÿãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
"""

import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import numpy as np
import cv2

from features.evaluation.anime_image_preprocessor import AnimeImagePreprocessor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LightweightFaceDetector:
    """è»½é‡ç‰ˆé¡”æ¤œå‡ºï¼ˆMediaPipeç„¡ã—ï¼‰"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.LightweightFaceDetector")
        self.preprocessor = AnimeImagePreprocessor()
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹
        project_root = Path(__file__).parent
        
        # OpenCV Cascadeåˆ†é¡å™¨åˆæœŸåŒ–ï¼ˆã‚¢ãƒ‹ãƒ¡å°‚ç”¨ã®ã¿ï¼‰
        self.cascade_detectors = {}
        cascade_files = {
            'frontal': 'haarcascade_frontalface_default.xml',
            'anime': 'lbpcascade_animeface.xml'
        }
        
        for name, filename in cascade_files.items():
            try:
                if name == 'anime':
                    cascade_path = str(project_root / "models" / "cascades" / filename)
                else:
                    cascade_path = cv2.data.haarcascades + filename
                
                if os.path.exists(cascade_path):
                    self.cascade_detectors[name] = cv2.CascadeClassifier(cascade_path)
                    self.logger.info(f"OpenCV {name} CascadeåˆæœŸåŒ–å®Œäº†")
                else:
                    self.logger.warning(f"Cascadeãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹: {cascade_path}")
            except Exception as e:
                self.logger.warning(f"OpenCV {name} CascadeåˆæœŸåŒ–å¤±æ•—: {e}")
    
    def detect_faces_lightweight(self, image: np.ndarray) -> Dict:
        """è»½é‡ç‰ˆé¡”æ¤œå‡ºï¼ˆå‡¦ç†æ™‚é–“é‡è¦–ï¼‰"""
        start_time = datetime.now()
        results = {
            'detections': [],
            'preprocessing_time': 0.0,
            'detection_time': 0.0,
            'total_time': 0.0,
            'methods_used': []
        }
        
        try:
            # 1. è»½é‡å‰å‡¦ç†
            preprocess_start = datetime.now()
            enhanced_image = self.preprocessor.enhance_for_face_detection(image, lightweight_mode=True)
            results['preprocessing_time'] = (datetime.now() - preprocess_start).total_seconds()
            
            # 2. æ®µéšçš„æ¤œå‡º
            detection_start = datetime.now()
            
            # Step 1: ã‚¢ãƒ‹ãƒ¡å°‚ç”¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ï¼ˆæœ€å„ªå…ˆï¼‰
            if 'anime' in self.cascade_detectors:
                anime_faces = self._detect_anime_cascade(enhanced_image)
                if anime_faces:
                    results['detections'].extend(anime_faces)
                    results['methods_used'].append(f"anime_cascade: {len(anime_faces)}ä»¶")
            
            # Step 2: æ¨™æº–ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ï¼ˆè£œå®Œï¼‰
            if len(results['detections']) == 0 and 'frontal' in self.cascade_detectors:
                frontal_faces = self._detect_frontal_cascade(enhanced_image)
                if frontal_faces:
                    results['detections'].extend(frontal_faces)
                    results['methods_used'].append(f"frontal_cascade: {len(frontal_faces)}ä»¶")
            
            # Step 3: ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ3ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ï¼‰
            if len(results['detections']) == 0 and 'anime' in self.cascade_detectors:
                multiscale_faces = self._detect_multiscale_anime(image)
                if multiscale_faces:
                    results['detections'].extend(multiscale_faces)
                    results['methods_used'].append(f"multiscale_anime: {len(multiscale_faces)}ä»¶")
            
            results['detection_time'] = (datetime.now() - detection_start).total_seconds()
            results['total_time'] = (datetime.now() - start_time).total_seconds()
            
            self.logger.info(f"è»½é‡æ¤œå‡ºå®Œäº†: {len(results['detections'])}ä»¶ "
                           f"ï¼ˆå‰å‡¦ç†: {results['preprocessing_time']:.2f}ç§’, "
                           f"æ¤œå‡º: {results['detection_time']:.2f}ç§’, "
                           f"åˆè¨ˆ: {results['total_time']:.2f}ç§’ï¼‰")
            
        except Exception as e:
            self.logger.error(f"è»½é‡æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            results['error'] = str(e)
        
        return results
    
    def _detect_anime_cascade(self, image: np.ndarray) -> List[Dict]:
        """ã‚¢ãƒ‹ãƒ¡å°‚ç”¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰æ¤œå‡º"""
        detections = []
        
        try:
            detector = self.cascade_detectors['anime']
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            faces = detector.detectMultiScale(
                gray,
                scaleFactor=1.03,
                minNeighbors=1,
                minSize=(10, 10),
                maxSize=(500, 500)
            )
            
            for (x, y, w, h) in faces:
                detections.append({
                    'bbox': (x, y, w, h),
                    'confidence': 0.85,
                    'method': 'anime_cascade'
                })
        
        except Exception as e:
            self.logger.warning(f"ã‚¢ãƒ‹ãƒ¡ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return detections
    
    def _detect_frontal_cascade(self, image: np.ndarray) -> List[Dict]:
        """æ¨™æº–é¡”ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰æ¤œå‡º"""
        detections = []
        
        try:
            detector = self.cascade_detectors['frontal']
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            faces = detector.detectMultiScale(
                gray,
                scaleFactor=1.05,
                minNeighbors=2,
                minSize=(20, 20)
            )
            
            for (x, y, w, h) in faces:
                detections.append({
                    'bbox': (x, y, w, h),
                    'confidence': 0.7,
                    'method': 'frontal_cascade'
                })
        
        except Exception as e:
            self.logger.warning(f"æ¨™æº–ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return detections
    
    def _detect_multiscale_anime(self, image: np.ndarray) -> List[Dict]:
        """ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒ‹ãƒ¡æ¤œå‡ºï¼ˆ3ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ï¼‰"""
        detections = []
        
        try:
            # 3ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ç”Ÿæˆ
            multi_scale_images = self.preprocessor.create_multi_scale_versions(image, lightweight_mode=True)
            detector = self.cascade_detectors['anime']
            
            for scale_data in multi_scale_images:
                scale = scale_data['scale']
                scaled_image = scale_data['image']
                gray = cv2.cvtColor(scaled_image, cv2.COLOR_BGR2GRAY)
                
                faces = detector.detectMultiScale(
                    gray,
                    scaleFactor=1.02,
                    minNeighbors=1,
                    minSize=(8, 8),
                    maxSize=(600, 600)
                )
                
                for (x, y, w, h) in faces:
                    # å…ƒç”»åƒåº§æ¨™ç³»ã«å¤‰æ›
                    orig_x = int(x / scale)
                    orig_y = int(y / scale)
                    orig_w = int(w / scale)
                    orig_h = int(h / scale)
                    
                    detections.append({
                        'bbox': (orig_x, orig_y, orig_w, orig_h),
                        'confidence': 0.8,
                        'method': f'multiscale_anime_{scale:.2f}'
                    })
        
        except Exception as e:
            self.logger.warning(f"ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return detections


class LightweightFaceDetectionTest:
    """è»½é‡ç‰ˆé¡”æ¤œå‡ºçµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.detector = LightweightFaceDetector()
        self.test_datasets = {
            "kana05": "/mnt/c/AItools/lora/train/yado/org/kana05_cursor_fix",
            "kana07": "/mnt/c/AItools/lora/train/yado/org/kana07_cursor_fix", 
            "kana08": "/mnt/c/AItools/lora/train/yado/org/kana08_cursor_fix"
        }
    
    def run_comprehensive_test(self) -> Dict:
        """è»½é‡ç‰ˆåŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸš€ è»½é‡ç‰ˆé¡”æ¤œå‡ºçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        all_results = {}
        total_images = 0
        total_faces_detected = 0
        total_processing_time = 0.0
        processing_times = []
        
        for dataset_name, dataset_path in self.test_datasets.items():
            if not os.path.exists(dataset_path):
                logger.warning(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœªç™ºè¦‹: {dataset_path}")
                continue
            
            logger.info(f"ğŸ“‚ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {dataset_name}")
            dataset_results = self.test_dataset(dataset_path, dataset_name)
            all_results[dataset_name] = dataset_results
            
            total_images += dataset_results['image_count']
            total_faces_detected += dataset_results['total_faces_detected']
            total_processing_time += dataset_results['total_processing_time']
            processing_times.extend(dataset_results['processing_times'])
        
        # ç·åˆçµ±è¨ˆè¨ˆç®—
        overall_stats = {
            'total_images_processed': total_images,
            'total_faces_detected': total_faces_detected,
            'overall_detection_rate': total_faces_detected / total_images if total_images > 0 else 0.0,
            'total_processing_time': total_processing_time,
            'average_processing_time': total_processing_time / total_images if total_images > 0 else 0.0,
            'processing_times': processing_times
        }
        
        # çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_lightweight_report(all_results, overall_stats)
        
        return {
            'dataset_results': all_results,
            'overall_statistics': overall_stats,
            'test_completion_time': datetime.now().isoformat()
        }
    
    def test_dataset(self, dataset_path: str, dataset_name: str) -> Dict:
        """å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        image_files = [f for f in os.listdir(dataset_path) 
                      if f.lower().endswith(('.jpg', '.jpeg', '.png')) 
                      and not f.endswith('_gt.png')]
        
        results = {
            'dataset_name': dataset_name,
            'image_count': len(image_files),
            'detection_results': [],
            'total_faces_detected': 0,
            'total_processing_time': 0.0,
            'processing_times': [],
            'method_counts': {}
        }
        
        for image_file in image_files:
            image_path = os.path.join(dataset_path, image_file)
            
            try:
                # ç”»åƒèª­ã¿è¾¼ã¿
                image = cv2.imread(image_path)
                if image is None:
                    continue
                
                # è»½é‡ç‰ˆé¡”æ¤œå‡ºå®Ÿè¡Œ
                detection_result = self.detector.detect_faces_lightweight(image)
                
                face_count = len(detection_result.get('detections', []))
                processing_time = detection_result.get('total_time', 0.0)
                
                results['detection_results'].append({
                    'image_file': image_file,
                    'faces_detected': face_count,
                    'processing_time': processing_time,
                    'preprocessing_time': detection_result.get('preprocessing_time', 0.0),
                    'detection_time': detection_result.get('detection_time', 0.0),
                    'methods_used': detection_result.get('methods_used', [])
                })
                
                results['total_faces_detected'] += face_count
                results['total_processing_time'] += processing_time
                results['processing_times'].append(processing_time)
                
                # æ‰‹æ³•åˆ¥çµ±è¨ˆ
                for method_info in detection_result.get('methods_used', []):
                    method_name = method_info.split(':')[0]
                    if method_name not in results['method_counts']:
                        results['method_counts'][method_name] = 0
                    results['method_counts'][method_name] += 1
                
                logger.debug(f"  {image_file}: {face_count}ä»¶æ¤œå‡º ({processing_time:.2f}ç§’)")
            
            except Exception as e:
                logger.error(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ {image_file}: {e}")
        
        return results
    
    def generate_lightweight_report(self, all_results: Dict, overall_stats: Dict):
        """è»½é‡ç‰ˆæ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        print("\n" + "=" * 80)
        print("ğŸš€ è»½é‡ç‰ˆé¡”æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆçµæœï¼ˆGPT-4Oæœ€é©åŒ–é©ç”¨ï¼‰")
        print("=" * 80)
        
        print(f"\nğŸ“Š ç·åˆçµ±è¨ˆ:")
        print(f"  å‡¦ç†ç”»åƒæ•°: {overall_stats['total_images_processed']}æš")
        print(f"  ç·æ¤œå‡ºæ•°: {overall_stats['total_faces_detected']}ä»¶")
        print(f"  æ¤œå‡ºç‡: {overall_stats['overall_detection_rate']:.1%}")
        print(f"  ç·å‡¦ç†æ™‚é–“: {overall_stats['total_processing_time']:.1f}ç§’")
        print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {overall_stats['average_processing_time']:.2f}ç§’/ç”»åƒ")
        
        print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥è©³ç´°:")
        for dataset_name, results in all_results.items():
            detection_rate = (results['total_faces_detected'] / results['image_count']) if results['image_count'] > 0 else 0
            avg_time = results['total_processing_time'] / results['image_count'] if results['image_count'] > 0 else 0
            
            print(f"  {dataset_name}:")
            print(f"    ç”»åƒæ•°: {results['image_count']}æš")
            print(f"    æ¤œå‡ºæ•°: {results['total_faces_detected']}ä»¶")
            print(f"    æ¤œå‡ºç‡: {detection_rate:.1%}")
            print(f"    å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.2f}ç§’")
            print(f"    ä½¿ç”¨æ‰‹æ³•: {', '.join(results['method_counts'].keys())}")
        
        # GPT-4Oæœ€é©åŒ–åŠ¹æœåˆ†æ
        print(f"\nğŸ¯ GPT-4Oæœ€é©åŒ–åŠ¹æœ:")
        
        target_time = 12.0  # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚: 10-12ç§’ä»¥ä¸‹
        current_avg_time = overall_stats['average_processing_time']
        
        if current_avg_time <= target_time:
            print(f"  âœ… å‡¦ç†æ™‚é–“ç›®æ¨™é”æˆ: {current_avg_time:.2f}ç§’ <= {target_time}ç§’")
        else:
            print(f"  âš ï¸ å‡¦ç†æ™‚é–“è¦æ”¹å–„: {current_avg_time:.2f}ç§’ > {target_time}ç§’")
        
        target_detection_rate = 0.90  # 90%ç›®æ¨™
        current_rate = overall_stats['overall_detection_rate']
        
        if current_rate >= target_detection_rate:
            print(f"  âœ… æ¤œå‡ºç‡ç›®æ¨™é”æˆ: {current_rate:.1%} >= {target_detection_rate:.1%}")
        else:
            print(f"  ğŸ“‹ æ¤œå‡ºç‡è¦æ”¹å–„: {current_rate:.1%} < {target_detection_rate:.1%}")
        
        # é€Ÿåº¦æ”¹å–„æ¨å®š
        estimated_old_time = current_avg_time * 8  # å¾“æ¥ç‰ˆæ¨å®šï¼ˆ8å€é…ã„ï¼‰
        improvement_factor = estimated_old_time / current_avg_time
        
        print(f"\nğŸ“ˆ å‡¦ç†é€Ÿåº¦æ”¹å–„æ¨å®š:")
        print(f"  æ¨å®šå¾“æ¥å‡¦ç†æ™‚é–“: {estimated_old_time:.1f}ç§’/ç”»åƒ")
        print(f"  è»½é‡ç‰ˆå‡¦ç†æ™‚é–“: {current_avg_time:.2f}ç§’/ç”»åƒ")
        print(f"  æ”¹å–„å€ç‡: {improvement_factor:.1f}å€é«˜é€ŸåŒ–")
        
        print(f"\nğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        if current_rate >= target_detection_rate and current_avg_time <= target_time:
            print(f"  ğŸ‰ Week 1å®Œå…¨é”æˆ! Phase A2ç›®æ¨™ã‚¯ãƒªã‚¢")
        else:
            if current_rate < target_detection_rate:
                print(f"  ğŸ“‹ æ¤œå‡ºç‡å‘ä¸ŠãŒå¿…è¦ï¼ˆç¾åœ¨{current_rate:.1%} â†’ ç›®æ¨™90%ï¼‰")
            if current_avg_time > target_time:
                print(f"  ğŸ“‹ å‡¦ç†æ™‚é–“çŸ­ç¸®ãŒå¿…è¦ï¼ˆç¾åœ¨{current_avg_time:.2f}ç§’ â†’ ç›®æ¨™{target_time}ç§’ä»¥ä¸‹ï¼‰")
        
        print("\n" + "=" * 80)


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ è»½é‡ç‰ˆé¡”æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        tester = LightweightFaceDetectionTest()
        results = tester.run_comprehensive_test()
        
        # çµæœã‚’JSONã§ä¿å­˜
        output_file = f"lightweight_face_detection_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è©³ç´°çµæœä¿å­˜: {output_file}")
        print("âœ… è»½é‡ç‰ˆé¡”æ¤œå‡ºçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
        
        return 0
    
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1


if __name__ == "__main__":
    exit(main())