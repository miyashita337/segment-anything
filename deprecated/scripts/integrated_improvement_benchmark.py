#!/usr/bin/env python3
"""
çµ±åˆæ”¹å–„ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯
æœ€é©YOLOé–¾å€¤(0.03) + æœ€å„ªç§€SAMæˆ¦ç•¥(BBox Prompt)ã®çµ±åˆåŠ¹æžœæ¸¬å®š
"""

import numpy as np
import cv2
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import torch

from segment_anything import SamPredictor, sam_model_registry

import json
import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from ultralytics import YOLO

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class IntegratedBenchmarkResult:
    """çµ±åˆãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯çµæžœ"""
    image_id: str
    image_path: str
    human_bbox: Tuple[int, int, int, int]
    yolo_bbox: Optional[Tuple[int, int, int, int]]
    final_bbox: Optional[Tuple[int, int, int, int]]
    iou_score: float
    extraction_success: bool
    processing_time: float
    yolo_confidence: float
    sam_confidence: float
    improvement_method: str = "Integrated (YOLO 0.03 + BBox Prompt)"


class IntegratedImprovementBenchmark:
    """çµ±åˆæ”¹å–„ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.labels_file = project_root / "extracted_labels.json"
        self.output_dir = Path("/mnt/c/AItools/lora/train/yado/integrated_benchmark")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.human_labels = self.load_human_labels()
        logger.info(f"äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(self.human_labels)}ä»¶")
        
        # æœ€é©è¨­å®š
        self.optimal_yolo_threshold = 0.03
        self.optimal_sam_strategy = "BBox Prompt"
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.yolo_model = YOLO('yolov8n.pt')
        self.init_sam()
        
    def init_sam(self):
        """SAMåˆæœŸåŒ–"""
        sam_checkpoint = self.project_root / "sam_vit_h_4b8939.pth"
        if not sam_checkpoint.exists():
            raise FileNotFoundError(f"SAM checkpoint not found: {sam_checkpoint}")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        sam = sam_model_registry["vit_h"](checkpoint=str(sam_checkpoint))
        sam.to(device=device)
        self.sam_predictor = SamPredictor(sam)
        
    def load_human_labels(self) -> Dict[str, Dict]:
        """äººé–“ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            with open(self.labels_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            labels_dict = {}
            for item in data:
                filename = item['filename']
                image_id = filename.rsplit('.', 1)[0]
                
                if item.get('red_boxes') and len(item['red_boxes']) > 0:
                    first_box = item['red_boxes'][0]
                    bbox = first_box['bbox']
                    labels_dict[image_id] = {
                        'filename': filename,
                        'bbox': [bbox['x'], bbox['y'], bbox['width'], bbox['height']]
                    }
            
            return labels_dict
            
        except Exception as e:
            logger.error(f"ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def find_image_path(self, image_id: str) -> Optional[Path]:
        """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ¤œç´¢"""
        search_dirs = [
            Path("/mnt/c/AItools/lora/train/yado/org/kana05_cursor"),
            Path("/mnt/c/AItools/lora/train/yado/org/kana07_cursor"),
            Path("/mnt/c/AItools/lora/train/yado/org/kana08_cursor"),
            self.project_root / "test_small"
        ]
        
        extensions = ['.jpg', '.jpeg', '.png']
        
        for dir_path in search_dirs:
            for ext in extensions:
                image_path = dir_path / f"{image_id}{ext}"
                if image_path.exists():
                    return image_path
        
        return None
    
    def calculate_iou(self, bbox1: Tuple[int, int, int, int], 
                     bbox2: Tuple[int, int, int, int]) -> float:
        """IoUè¨ˆç®—"""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        intersection_area = (x_right - x_left) * (y_bottom - y_top)
        union_area = w1 * h1 + w2 * h2 - intersection_area
        
        return intersection_area / max(union_area, 1e-6)
    
    def integrated_extraction(self, image_path: Path) -> Tuple[Optional[Tuple[int, int, int, int]], float, float]:
        """çµ±åˆæ”¹å–„æŠ½å‡ºï¼ˆæœ€é©YOLOé–¾å€¤ + æœ€å„ªç§€SAMæˆ¦ç•¥ï¼‰"""
        try:
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(str(image_path))
            if image is None:
                return None, 0.0, 0.0
            
            # YOLOæ¤œå‡ºï¼ˆæœ€é©é–¾å€¤0.03ï¼‰
            results = self.yolo_model(image, conf=self.optimal_yolo_threshold, verbose=False)
            
            if not results or len(results[0].boxes) == 0:
                return None, 0.0, 0.0
            
            # æœ€å¤§æ¤œå‡ºçµæžœé¸æŠž
            boxes = results[0].boxes.xyxy.cpu().numpy()
            confs = results[0].boxes.conf.cpu().numpy()
            
            if len(boxes) == 0:
                return None, 0.0, 0.0
            
            areas = [(box[2] - box[0]) * (box[3] - box[1]) for box in boxes]
            largest_idx = np.argmax(areas)
            x1, y1, x2, y2 = boxes[largest_idx]
            yolo_confidence = float(confs[largest_idx])
            
            # SAM ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆBBox Promptæˆ¦ç•¥ï¼‰
            self.sam_predictor.set_image(image)
            
            # YOLOãƒœãƒƒã‚¯ã‚¹ã‚’å°‘ã—æ‹¡å¼µ
            w, h = x2 - x1, y2 - y1
            expansion = 0.1  # 10%æ‹¡å¼µ
            expand_w = int(w * expansion)
            expand_h = int(h * expansion)
            
            expanded_bbox = np.array([
                max(0, x1 - expand_w),
                max(0, y1 - expand_h),
                x2 + expand_w,
                y2 + expand_h
            ])
            
            masks, scores, _ = self.sam_predictor.predict(
                box=expanded_bbox[None, :],
                multimask_output=True
            )
            
            if masks is None or len(masks) == 0:
                # SAMå¤±æ•—æ™‚ã¯YOLOãƒœãƒƒã‚¯ã‚¹ã‚’è¿”ã™
                return (int(x1), int(y1), int(x2 - x1), int(y2 - y1)), yolo_confidence, 0.0
            
            # æœ€è‰¯ãƒžã‚¹ã‚¯ã‹ã‚‰å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹è¨ˆç®—
            best_mask = masks[np.argmax(scores)]
            sam_confidence = float(np.max(scores))
            
            y_indices, x_indices = np.where(best_mask > 0)
            
            if len(x_indices) == 0 or len(y_indices) == 0:
                return (int(x1), int(y1), int(x2 - x1), int(y2 - y1)), yolo_confidence, 0.0
            
            x_min = int(np.min(x_indices))
            x_max = int(np.max(x_indices))
            y_min = int(np.min(y_indices))
            y_max = int(np.max(y_indices))
            
            return (x_min, y_min, x_max - x_min, y_max - y_min), yolo_confidence, sam_confidence
            
        except Exception as e:
            logger.error(f"çµ±åˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None, 0.0, 0.0
    
    def run_integrated_benchmark(self) -> List[IntegratedBenchmarkResult]:
        """çµ±åˆãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯å®Ÿè¡Œï¼ˆå…¨101ä»¶ï¼‰"""
        logger.info("çµ±åˆæ”¹å–„ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯é–‹å§‹ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰")
        results = []
        
        total = len(self.human_labels)
        for i, (image_id, label_data) in enumerate(self.human_labels.items(), 1):
            logger.info(f"å‡¦ç†ä¸­ [{i}/{total}]: {image_id}")
            
            start_time = time.time()
            
            # ç”»åƒãƒ‘ã‚¹æ¤œç´¢
            image_path = self.find_image_path(image_id)
            if not image_path:
                results.append(IntegratedBenchmarkResult(
                    image_id=image_id,
                    image_path="",
                    human_bbox=tuple(label_data['bbox']),
                    yolo_bbox=None,
                    final_bbox=None,
                    iou_score=0.0,
                    extraction_success=False,
                    processing_time=0.0,
                    yolo_confidence=0.0,
                    sam_confidence=0.0
                ))
                continue
            
            # çµ±åˆæŠ½å‡ºå®Ÿè¡Œ
            final_bbox, yolo_conf, sam_conf = self.integrated_extraction(image_path)
            processing_time = time.time() - start_time
            
            # çµæžœè©•ä¾¡
            human_bbox = tuple(label_data['bbox'])
            
            if final_bbox is None:
                iou_score = 0.0
                extraction_success = False
            else:
                iou_score = self.calculate_iou(human_bbox, final_bbox)
                extraction_success = iou_score > 0.5  # IoU > 0.5 ã‚’æˆåŠŸã¨ã™ã‚‹
            
            results.append(IntegratedBenchmarkResult(
                image_id=image_id,
                image_path=str(image_path),
                human_bbox=human_bbox,
                yolo_bbox=None,  # ç°¡ç•¥åŒ–ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
                final_bbox=final_bbox,
                iou_score=iou_score,
                extraction_success=extraction_success,
                processing_time=processing_time,
                yolo_confidence=yolo_conf,
                sam_confidence=sam_conf
            ))
            
            # é€²æ—è¡¨ç¤º
            if i % 20 == 0:
                success_count = sum(1 for r in results if r.extraction_success)
                logger.info(f"é€²æ—: {i}/{total} - æˆåŠŸçŽ‡: {success_count/i*100:.1f}%")
        
        return results
    
    def generate_improvement_report(self, results: List[IntegratedBenchmarkResult], 
                                  baseline_success_rate: float = 16.8):
        """æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        # çµ±è¨ˆè¨ˆç®—
        total = len(results)
        success_count = sum(1 for r in results if r.extraction_success)
        success_rate = success_count / total * 100
        
        iou_scores = [r.iou_score for r in results]
        avg_iou = np.mean(iou_scores)
        
        processing_times = [r.processing_time for r in results]
        avg_time = np.mean(processing_times)
        
        # æ”¹å–„åŠ¹æžœè¨ˆç®—
        improvement = success_rate - baseline_success_rate
        improvement_ratio = success_rate / baseline_success_rate if baseline_success_rate > 0 else 0
        
        # ãƒ™ã‚¹ãƒˆ5ãƒ»ãƒ¯ãƒ¼ã‚¹ãƒˆ5
        sorted_results = sorted(results, key=lambda r: r.iou_score, reverse=True)
        best_5 = sorted_results[:5]
        worst_5 = sorted_results[-5:]
        
        # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = f"""# çµ±åˆæ”¹å–„ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**ç·ç”»åƒæ•°**: {total}æžš
**æ”¹å–„æ‰‹æ³•**: {results[0].improvement_method if results else 'N/A'}

---

## ðŸŽ¯ æ”¹å–„åŠ¹æžœã‚µãƒžãƒªãƒ¼

### æˆåŠŸçŽ‡æ¯”è¼ƒ
- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: {baseline_success_rate}% (å¾“æ¥æ‰‹æ³•)
- **æ”¹å–„å¾Œ**: {success_rate:.1f}% â­
- **æ”¹å–„å¹…**: +{improvement:.1f}ãƒã‚¤ãƒ³ãƒˆ
- **æ”¹å–„å€çŽ‡**: {improvement_ratio:.1f}å€

### IoUã‚¹ã‚³ã‚¢
- **å¹³å‡IoU**: {avg_iou:.3f}
- **æœ€é«˜IoU**: {max(iou_scores):.3f}
- **æœ€ä½ŽIoU**: {min(iou_scores):.3f}

### å‡¦ç†æ€§èƒ½
- **å¹³å‡å‡¦ç†æ™‚é–“**: {avg_time:.2f}ç§’/ç”»åƒ

---

## ðŸ† æˆåŠŸä¾‹ãƒ™ã‚¹ãƒˆ5

"""
        
        for i, result in enumerate(best_5, 1):
            report += f"{i}. **{result.image_id}** - IoU: {result.iou_score:.3f} (YOLOä¿¡é ¼åº¦: {result.yolo_confidence:.3f})\n"
        
        report += """
---

## ðŸ’¥ å¤±æ•—ä¾‹ãƒ¯ãƒ¼ã‚¹ãƒˆ5

"""
        
        for i, result in enumerate(worst_5, 1):
            report += f"{i}. **{result.image_id}** - IoU: {result.iou_score:.3f}\n"
        
        # IoUåˆ†å¸ƒåˆ†æž
        high_iou = sum(1 for score in iou_scores if score >= 0.7)
        medium_iou = sum(1 for score in iou_scores if 0.3 <= score < 0.7)
        low_iou = sum(1 for score in iou_scores if score < 0.3)
        
        report += f"""
---

## ðŸ“Š è©³ç´°åˆ†æž

### IoUåˆ†å¸ƒ
- **é«˜ç²¾åº¦ (IoU â‰¥ 0.7)**: {high_iou}ä»¶ ({high_iou/total*100:.1f}%)
- **ä¸­ç²¾åº¦ (0.3 â‰¤ IoU < 0.7)**: {medium_iou}ä»¶ ({medium_iou/total*100:.1f}%)
- **ä½Žç²¾åº¦ (IoU < 0.3)**: {low_iou}ä»¶ ({low_iou/total*100:.1f}%)

### æ”¹å–„æ‰‹æ³•ã®åŠ¹æžœ
1. **YOLOé–¾å€¤æœ€é©åŒ–**: 0.07 â†’ 0.03
   - æ¤œå‡ºçŽ‡å‘ä¸Šã«ã‚ˆã‚ŠåŸºç›¤æ€§èƒ½æ”¹å–„
2. **SAM BBoxãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹åˆ©ç”¨
   - ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦å‘ä¸Š

### æ¬¡ã®æ”¹å–„ç›®æ¨™
- **ç›®æ¨™æˆåŠŸçŽ‡**: 60%+ (ç¾åœ¨{success_rate:.1f}%)
- **å°‚ç”¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ´»ç”¨**: 101ä»¶ã®äººé–“ãƒ©ãƒ™ãƒ«ã§ã®æ•™å¸«ã‚ã‚Šå­¦ç¿’
- **å›°é›£ãƒãƒ¼ã‚ºå¯¾å¿œ**: ç‰¹åˆ¥å‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…

---

## ðŸš€ å®Ÿè£…æŽ¨å¥¨äº‹é …

1. **å³åº§ã«é©ç”¨å¯èƒ½**
   - YOLOä¿¡é ¼åº¦é–¾å€¤ã‚’0.03ã«å¤‰æ›´
   - SAMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã‚’BBoxPromptã«å¤‰æ›´

2. **æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºé–‹ç™º**
   - ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å°‚ç”¨YOLOå­¦ç¿’
   - ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰çµ±åˆæœ€é©åŒ–

*Generated by Integrated Improvement Benchmark System*
"""
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = self.output_dir / f"integrated_improvement_report_{time.strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # çµæžœJSONä¿å­˜
        results_data = [asdict(r) for r in results]
        json_path = self.output_dir / f"integrated_improvement_results_{time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        # æ¯”è¼ƒã‚°ãƒ©ãƒ•ç”Ÿæˆ
        self.generate_improvement_graphs(results, baseline_success_rate)
        
        logger.info(f"çµ±åˆæ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        return report_path, success_rate, improvement
    
    def generate_improvement_graphs(self, results: List[IntegratedBenchmarkResult], 
                                  baseline_rate: float):
        """æ”¹å–„åŠ¹æžœã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
        success_rate = sum(1 for r in results if r.extraction_success) / len(results) * 100
        
        # æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æˆåŠŸçŽ‡æ¯”è¼ƒ
        categories = ['Baseline\n(Original)', 'Improved\n(Integrated)']
        rates = [baseline_rate, success_rate]
        colors = ['red', 'green']
        
        bars = ax1.bar(categories, rates, color=colors, alpha=0.7)
        ax1.set_ylabel('Success Rate (%)')
        ax1.set_title('Extraction Success Rate Comparison')
        ax1.set_ylim(0, 100)
        
        # ãƒãƒ¼ã«æ•°å€¤è¡¨ç¤º
        for bar, rate in zip(bars, rates):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # æ”¹å–„åŠ¹æžœ
        improvement = success_rate - baseline_rate
        ax1.text(0.5, max(rates) * 0.8, f'Improvement:\n+{improvement:.1f} points\n({success_rate/baseline_rate:.1f}x)',
                ha='center', va='center', bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.5),
                fontsize=12, fontweight='bold')
        
        # IoUåˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        iou_scores = [r.iou_score for r in results]
        ax2.hist(iou_scores, bins=20, alpha=0.7, color='blue', edgecolor='black')
        ax2.set_xlabel('IoU Score')
        ax2.set_ylabel('Frequency')
        ax2.set_title('IoU Score Distribution (Improved Method)')
        ax2.axvline(x=0.5, color='red', linestyle='--', label='Success Threshold (0.5)')
        ax2.legend()
        
        plt.tight_layout()
        
        # ã‚°ãƒ©ãƒ•ä¿å­˜
        graph_path = self.output_dir / f"integrated_improvement_graph_{time.strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(graph_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f"æ”¹å–„åŠ¹æžœã‚°ãƒ©ãƒ•ä¿å­˜: {graph_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    project_root = Path("/mnt/c/AItools/segment-anything")
    
    benchmark = IntegratedImprovementBenchmark(project_root)
    
    # çµ±åˆãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯å®Ÿè¡Œï¼ˆå…¨101ä»¶ï¼‰
    results = benchmark.run_integrated_benchmark()
    
    # æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³16.8%ã¨æ¯”è¼ƒï¼‰
    report_path, new_success_rate, improvement = benchmark.generate_improvement_report(
        results, baseline_success_rate=16.8
    )
    
    # çµæžœã‚µãƒžãƒªãƒ¼
    print(f"\nâœ… çµ±åˆæ”¹å–„ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯å®Œäº†")
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æˆåŠŸçŽ‡: 16.8%")
    print(f"æ”¹å–„å¾ŒæˆåŠŸçŽ‡: {new_success_rate:.1f}%")
    print(f"æ”¹å–„åŠ¹æžœ: +{improvement:.1f}ãƒã‚¤ãƒ³ãƒˆ")
    print(f"ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")


if __name__ == "__main__":
    main()