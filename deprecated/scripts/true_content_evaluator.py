#!/usr/bin/env python3
"""
çœŸã®å†…å®¹è©•ä¾¡å™¨
å®Ÿéš›ã®æŠ½å‡ºç”»åƒã¨äººé–“æ„å›³é ˜åŸŸã®æ¯”è¼ƒ
"""

import numpy as np
import cv2

import json
import logging
from pathlib import Path
from typing import Optional, Tuple

# CLIP import with error handling
try:
    import torch

    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TrueContentEvaluator:
    """å®Ÿéš›ã®æŠ½å‡ºç”»åƒã«ã‚ˆã‚‹å†…å®¹è©•ä¾¡"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.results_file = project_root / "lora/train/yado/integrated_benchmark/integrated_improvement_results_20250724_030756.json"
        
        # CLIPåˆæœŸåŒ–
        self.model = None
        self.preprocess = None
        if CLIP_AVAILABLE:
            self.model, self.preprocess = clip.load("ViT-B/32", device="cuda" if torch.cuda.is_available() else "cpu")
            logger.info("CLIP model loaded successfully")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.load_data()
    
    def load_data(self):
        """AIãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœèª­ã¿è¾¼ã¿"""
        with open(self.results_file, 'r', encoding='utf-8') as f:
            self.ai_results = json.load(f)
        logger.info(f"AIçµæœèª­ã¿è¾¼ã¿: {len(self.ai_results)}ä»¶")
    
    def find_actual_extraction(self, image_id: str) -> Optional[Path]:
        """å®Ÿéš›ã®æŠ½å‡ºç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        candidate_paths = [
            self.project_root / f"lora/train/yado/clipped_boundingbox/kana07/{image_id}.jpg",
            self.project_root / f"lora/train/yado/clipped_boundingbox/kana05/{image_id}.jpg", 
            self.project_root / f"lora/train/yado/clipped_boundingbox/kana08/{image_id}.jpg"
        ]
        
        for path in candidate_paths:
            if path.exists():
                return path
        return None
    
    def get_human_intended_crop(self, image_id: str) -> Optional[np.ndarray]:
        """äººé–“ãŒæ„å›³ã—ãŸé ˜åŸŸã®ã‚¯ãƒ­ãƒƒãƒ—å–å¾—"""
        # AIçµæœã‹ã‚‰æƒ…å ±å–å¾—
        ai_result = None
        for result in self.ai_results:
            if result['image_id'] == image_id:
                ai_result = result
                break
        
        if not ai_result:
            return None
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒèª­ã¿è¾¼ã¿
        image_path = Path(ai_result['image_path'])
        if not image_path.exists():
            return None
        
        original_img = cv2.imread(str(image_path))
        if original_img is None:
            return None
        
        # äººé–“ãƒ©ãƒ™ãƒ«é ˜åŸŸã‚’ã‚¯ãƒ­ãƒƒãƒ—
        hx, hy, hw, hh = ai_result['human_bbox']
        human_crop = original_img[hy:hy+hh, hx:hx+hw]
        
        return human_crop
    
    def calculate_clip_similarity(self, image1: np.ndarray, image2: np.ndarray) -> float:
        """CLIPç‰¹å¾´é‡ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—"""
        if not CLIP_AVAILABLE:
            logger.warning("CLIP not available, returning 0.0")
            return 0.0
        
        try:
            # ç”»åƒã‚’PILå½¢å¼ã«å¤‰æ›
            from PIL import Image
            
            img1_pil = Image.fromarray(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))
            img2_pil = Image.fromarray(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))
            
            # CLIPå‰å‡¦ç†
            img1_tensor = self.preprocess(img1_pil).unsqueeze(0)
            img2_tensor = self.preprocess(img2_pil).unsqueeze(0)
            
            # GPUç§»å‹•
            device = next(self.model.parameters()).device
            img1_tensor = img1_tensor.to(device)
            img2_tensor = img2_tensor.to(device)
            
            # ç‰¹å¾´æŠ½å‡º
            with torch.no_grad():
                features1 = self.model.encode_image(img1_tensor)
                features2 = self.model.encode_image(img2_tensor)
                
                # æ­£è¦åŒ–
                features1 = features1 / features1.norm(dim=-1, keepdim=True)
                features2 = features2 / features2.norm(dim=-1, keepdim=True)
                
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                similarity = torch.cosine_similarity(features1, features2, dim=-1)
                return float(similarity.item())
                
        except Exception as e:
            logger.error(f"CLIP similarity calculation failed: {e}")
            return 0.0
    
    def evaluate_true_content(self, image_id: str) -> dict:
        """çœŸã®å†…å®¹è©•ä¾¡å®Ÿè¡Œ"""
        result = {
            'image_id': image_id,
            'actual_extraction_found': False,
            'human_crop_extracted': False,
            'clip_similarity': 0.0,
            'visual_match': False,
            'evaluation_method': 'true_content'
        }
        
        # å®Ÿéš›ã®æŠ½å‡ºç”»åƒå–å¾—
        extracted_path = self.find_actual_extraction(image_id)
        if not extracted_path:
            logger.warning(f"å®Ÿéš›ã®æŠ½å‡ºç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_id}")
            return result
        
        extracted_img = cv2.imread(str(extracted_path))
        if extracted_img is None:
            logger.warning(f"æŠ½å‡ºç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {extracted_path}")
            return result
        
        result['actual_extraction_found'] = True
        
        # äººé–“æ„å›³é ˜åŸŸå–å¾—
        human_crop = self.get_human_intended_crop(image_id)
        if human_crop is None:
            logger.warning(f"äººé–“æ„å›³é ˜åŸŸå–å¾—å¤±æ•—: {image_id}")
            return result
        
        result['human_crop_extracted'] = True
        
        # CLIPé¡ä¼¼åº¦è¨ˆç®—
        if CLIP_AVAILABLE:
            similarity = self.calculate_clip_similarity(extracted_img, human_crop)
            result['clip_similarity'] = similarity
            result['visual_match'] = similarity > 0.7  # é¡ä¼¼åº¦é–¾å€¤
            
            logger.info(f"{image_id}: CLIPé¡ä¼¼åº¦ {similarity:.3f}")
        
        return result
    
    def test_problem_cases(self, cases: list = None) -> dict:
        """å•é¡Œã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
        if cases is None:
            cases = ['kana07_0023', 'kana05_0001', 'kana05_0002']
        
        results = {}
        
        print("ğŸ” çœŸã®å†…å®¹è©•ä¾¡ãƒ†ã‚¹ãƒˆ")
        print("=" * 50)
        
        for case in cases:
            print(f"\nğŸ“Š {case} ã®è©•ä¾¡:")
            
            # å¾“æ¥ã®å ±å‘Šå€¤å–å¾—
            ai_result = None
            for result in self.ai_results:
                if result['image_id'] == case:
                    ai_result = result
                    break
            
            if ai_result:
                print(f"å¾“æ¥IoU: {ai_result['iou_score']:.3f}")
                print(f"å¾“æ¥åˆ¤å®š: {'âœ…æˆåŠŸ' if ai_result['extraction_success'] else 'âŒå¤±æ•—'}")
            
            # çœŸã®å†…å®¹è©•ä¾¡
            true_result = self.evaluate_true_content(case)
            results[case] = true_result
            
            print(f"å®Ÿéš›æŠ½å‡º: {'âœ…' if true_result['actual_extraction_found'] else 'âŒ'}")
            print(f"äººé–“é ˜åŸŸ: {'âœ…' if true_result['human_crop_extracted'] else 'âŒ'}")
            if CLIP_AVAILABLE:
                print(f"CLIPé¡ä¼¼åº¦: {true_result['clip_similarity']:.3f}")
                print(f"çœŸã®åˆ¤å®š: {'âœ…æˆåŠŸ' if true_result['visual_match'] else 'âŒå¤±æ•—'}")
                
                # åˆ¤å®šæ¯”è¼ƒ
                if ai_result and ai_result['extraction_success'] != true_result['visual_match']:
                    change = "æˆåŠŸâ†’å¤±æ•—" if ai_result['extraction_success'] else "å¤±æ•—â†’æˆåŠŸ"
                    print(f"âš ï¸  åˆ¤å®šå¤‰æ›´: {change}")
            
        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    project_root = Path("/mnt/c/AItools")
    evaluator = TrueContentEvaluator(project_root)
    
    if not CLIP_AVAILABLE:
        print("âŒ CLIP ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ä»¥ä¸‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("pip install git+https://github.com/openai/CLIP.git")
        return
    
    # å•é¡Œã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ
    results = evaluator.test_problem_cases(['kana07_0023'])
    
    print(f"\nâœ… çœŸã®å†…å®¹è©•ä¾¡å®Œäº†")
    print(f"å®Ÿéš›ã®æŠ½å‡ºç”»åƒã¨äººé–“æ„å›³é ˜åŸŸã®æ¯”è¼ƒã«ã‚ˆã‚Šã€çœŸã®ä¸€è‡´åº¦ã‚’æ¸¬å®šã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()