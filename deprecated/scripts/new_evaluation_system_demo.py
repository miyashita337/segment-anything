#!/usr/bin/env python3
"""
æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
GPT-4Oè¨­è¨ˆã«ã‚ˆã‚‹åº§æ¨™+å†…å®¹çµ±åˆè©•ä¾¡ã®å®Ÿæ¼”
"""

import numpy as np

import json
import logging
import time
from pathlib import Path

# æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆCLIPä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
try:
    from evaluation import EvaluationConfig, EvaluationOrchestrator
    EVALUATION_AVAILABLE = True
except ImportError as e:
    EVALUATION_AVAILABLE = False
    print(f"âš ï¸  æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
    print("å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install git+https://github.com/openai/CLIP.git")
    print("pip install torch torchvision")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NewEvaluationDemo:
    """æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.results_file = project_root / "lora/train/yado/integrated_benchmark/integrated_improvement_results_20250724_030756.json"
        self.labels_file = project_root / "segment-anything/extracted_labels.json"
        
        # è¨­å®š
        self.config = EvaluationConfig() if EVALUATION_AVAILABLE else None
        if self.config:
            self.config.alpha = 0.6  # IoUé‡è¦–ï¼ˆ60%ï¼‰+ å†…å®¹é¡ä¼¼åº¦40%
            self.config.iou_threshold = 0.3
            self.config.content_threshold = 0.25
            self.config.success_threshold = 0.5
            self.config.use_fp16 = False  # FP16ã‚¨ãƒ©ãƒ¼å›é¿
        
        self.orchestrator = None
        if EVALUATION_AVAILABLE and self.config:
            try:
                self.orchestrator = EvaluationOrchestrator(self.config)
            except Exception as e:
                logger.warning(f"Orchestrator initialization failed: {e}")
                self.orchestrator = None
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.load_data()
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        with open(self.results_file, 'r', encoding='utf-8') as f:
            self.ai_results = json.load(f)
            
        with open(self.labels_file, 'r', encoding='utf-8') as f:
            labels_data = json.load(f)
            
        self.human_labels = {}
        for item in labels_data:
            if item.get('red_boxes'):
                image_id = item['filename'].rsplit('.', 1)[0]
                self.human_labels[image_id] = item
                
        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.ai_results)}ä»¶ã®çµæœ, {len(self.human_labels)}ä»¶ã®äººé–“ãƒ©ãƒ™ãƒ«")
    
    def prepare_evaluation_data(self, image_id: str) -> tuple:
        """è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        # AIçµæœã‚’æ¤œç´¢
        ai_result = None
        for result in self.ai_results:
            if result['image_id'] == image_id:
                ai_result = result
                break
        
        if not ai_result:
            return None, None
        
        # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿æº–å‚™
        predictions = {
            'image_path': ai_result['image_path'],
            'bboxes': [ai_result['final_bbox']] if ai_result['final_bbox'] else []
        }
        
        # Ground truth ãƒ‡ãƒ¼ã‚¿æº–å‚™
        ground_truth = {
            'image_path': ai_result['image_path'],
            'bboxes': [ai_result['human_bbox']]
        }
        
        return predictions, ground_truth
    
    def demo_single_case(self, image_id: str = "kana07_0023"):
        """å˜ä¸€ã‚±ãƒ¼ã‚¹ã®ãƒ‡ãƒ¢"""
        if not EVALUATION_AVAILABLE or not self.orchestrator:
            print("âŒ æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
        
        print(f"\nğŸ” ã€{image_id}ã€‘ã®æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        predictions, ground_truth = self.prepare_evaluation_data(image_id)
        if not predictions or not ground_truth:
            print(f"âŒ {image_id} ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # å¾“æ¥è©•ä¾¡çµæœ
        old_result = None
        for result in self.ai_results:
            if result['image_id'] == image_id:
                old_result = result
                break
        
        print(f"ğŸ“Š å¾“æ¥è©•ä¾¡:")
        print(f"   IoU: {old_result['iou_score']:.3f}")
        print(f"   æˆåŠŸåˆ¤å®š: {'âœ…' if old_result['extraction_success'] else 'âŒ'}")
        
        # æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
        print(f"\nğŸ”„ æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­...")
        try:
            start_time = time.time()
            new_result = self.orchestrator.run_single_image(predictions, ground_truth)
            processing_time = time.time() - start_time
            
            print(f"ğŸ“Š æ–°è©•ä¾¡çµæœ:")
            print(f"   çµ±åˆã‚¹ã‚³ã‚¢: {new_result.get('integrated_score', 0):.3f}")
            print(f"   ç©ºé–“ã‚¹ã‚³ã‚¢: {new_result.get('spatial_score', 0):.3f}")
            print(f"   å†…å®¹ã‚¹ã‚³ã‚¢: {new_result.get('content_score', 0):.3f}")
            print(f"   æˆåŠŸåˆ¤å®š: {'âœ…' if new_result.get('success', False) else 'âŒ'}")
            print(f"   å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
            
            # æ¯”è¼ƒåˆ†æ
            if old_result['extraction_success'] != new_result.get('success', False):
                print(f"\nâš ï¸  åˆ¤å®šå¤‰æ›´:")
                print(f"   å¾“æ¥: {'æˆåŠŸ' if old_result['extraction_success'] else 'å¤±æ•—'}")
                print(f"   æ–°æ–¹å¼: {'æˆåŠŸ' if new_result.get('success', False) else 'å¤±æ•—'}")
                print(f"   åŸå› : å†…å®¹é¡ä¼¼åº¦ãŒ {new_result.get('content_score', 0):.3f} (é–¾å€¤: {self.config.content_threshold})")
        
        except Exception as e:
            print(f"âŒ æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    def demo_batch_comparison(self, sample_size: int = 10):
        """ãƒãƒƒãƒæ¯”è¼ƒãƒ‡ãƒ¢"""
        if not EVALUATION_AVAILABLE or not self.orchestrator:
            print("âŒ æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
        
        print(f"\nğŸ“Š ãƒãƒƒãƒæ¯”è¼ƒãƒ‡ãƒ¢ï¼ˆ{sample_size}ä»¶ã‚µãƒ³ãƒ—ãƒ«ï¼‰")
        
        # ã‚µãƒ³ãƒ—ãƒ«é¸æŠï¼ˆé«˜IoUã‚±ãƒ¼ã‚¹å„ªå…ˆï¼‰
        high_iou_cases = [r for r in self.ai_results if r['extraction_success'] and r['iou_score'] > 0.8]
        sample_cases = high_iou_cases[:sample_size] if len(high_iou_cases) >= sample_size else self.ai_results[:sample_size]
        
        predictions_batch = []
        ground_truths_batch = []
        
        for result in sample_cases:
            pred, gt = self.prepare_evaluation_data(result['image_id'])
            if pred and gt:
                predictions_batch.append(pred)
                ground_truths_batch.append(gt)
        
        if not predictions_batch:
            print("âŒ è©•ä¾¡å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        print(f"ğŸ”„ {len(predictions_batch)}ä»¶ã®ãƒãƒƒãƒè©•ä¾¡å®Ÿè¡Œä¸­...")
        
        try:
            batch_result = self.orchestrator.run_batch(predictions_batch, ground_truths_batch)
            
            # å¾“æ¥è©•ä¾¡ã®çµ±è¨ˆ
            old_success_count = sum(1 for r in sample_cases[:len(predictions_batch)] if r['extraction_success'])
            old_success_rate = old_success_count / len(predictions_batch)
            old_mean_iou = np.mean([r['iou_score'] for r in sample_cases[:len(predictions_batch)]])
            
            print(f"\nğŸ“ˆ æ¯”è¼ƒçµæœ:")
            print(f"å¾“æ¥è©•ä¾¡:")
            print(f"   æˆåŠŸç‡: {old_success_rate:.1%} ({old_success_count}/{len(predictions_batch)})")
            print(f"   å¹³å‡IoU: {old_mean_iou:.3f}")
            
            print(f"æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ :")
            print(f"   æˆåŠŸç‡: {batch_result['success_rate']:.1%} ({batch_result['successful_images']}/{batch_result['total_images']})")
            print(f"   å¹³å‡çµ±åˆã‚¹ã‚³ã‚¢: {batch_result['mean_integrated_score']:.3f}")
            print(f"   å¹³å‡ç©ºé–“ã‚¹ã‚³ã‚¢: {batch_result['mean_spatial_score']:.3f}")
            print(f"   å¹³å‡å†…å®¹ã‚¹ã‚³ã‚¢: {batch_result['mean_content_score']:.3f}")
            print(f"   å¹³å‡å‡¦ç†æ™‚é–“: {batch_result['avg_time_per_image']:.3f}ç§’/ç”»åƒ")
            
            # åˆ¤å®šå¤‰æ›´ã®åˆ†æ
            judgment_changes = 0
            for i, result in enumerate(batch_result['individual_results']):
                old_success = sample_cases[i]['extraction_success']
                new_success = result.get('success', False)
                if old_success != new_success:
                    judgment_changes += 1
            
            print(f"\nğŸ”„ åˆ¤å®šå¤‰æ›´: {judgment_changes}ä»¶ ({judgment_changes/len(predictions_batch):.1%})")
            
        except Exception as e:
            print(f"âŒ ãƒãƒƒãƒè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
    
    def show_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º"""
        print("ğŸ¤– GPT-4Oè¨­è¨ˆã«ã‚ˆã‚‹æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 50)
        print("ç‰¹å¾´:")
        print("â€¢ åº§æ¨™ä¸€è‡´ + å†…å®¹é¡ä¼¼åº¦ã®çµ±åˆè©•ä¾¡")
        print("â€¢ CLIP/DINOv2ã«ã‚ˆã‚‹è¦–è¦šçš„ç‰¹å¾´æŠ½å‡º")
        print("â€¢ ãƒãƒ³ã‚¬ãƒªã‚¢ãƒ³æ³•ã«ã‚ˆã‚‹æœ€é©ãƒãƒƒãƒãƒ³ã‚°")
        print("â€¢ ãƒãƒ«ãƒã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”»åƒå¯¾å¿œ")
        print("â€¢ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ï¼ˆ0.5ç§’/ç”»åƒä»¥å†…ï¼‰")
        
        if EVALUATION_AVAILABLE:
            print(f"\nè¨­å®š:")
            print(f"â€¢ Î± (IoU:å†…å®¹) = {self.config.alpha}:{1-self.config.alpha}")
            print(f"â€¢ IoUé–¾å€¤: {self.config.iou_threshold}")
            print(f"â€¢ å†…å®¹é¡ä¼¼åº¦é–¾å€¤: {self.config.content_threshold}")
            print(f"â€¢ æˆåŠŸåˆ¤å®šé–¾å€¤: {self.config.success_threshold}")
        else:
            print("\nâš ï¸  ä¾å­˜é–¢ä¿‚ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚CLIPã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    project_root = Path("/mnt/c/AItools")
    demo = NewEvaluationDemo(project_root)
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
    demo.show_system_info()
    
    if EVALUATION_AVAILABLE:
        # å˜ä¸€ã‚±ãƒ¼ã‚¹ãƒ‡ãƒ¢ï¼ˆå•é¡Œã®kana07_0023ï¼‰
        demo.demo_single_case("kana07_0023")
        
        # ãƒãƒƒãƒæ¯”è¼ƒãƒ‡ãƒ¢
        demo.demo_batch_comparison(5)  # 5ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«
        
        print(f"\nâœ… æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢å®Œäº†")
        print(f"åº§æ¨™ã®ä¸€è‡´â‰ å†…å®¹ã®ä¸€è‡´ å•é¡Œã‚’è§£æ±ºã™ã‚‹çµ±åˆè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚")
    else:
        print(f"\nâŒ æ–°è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ä»¥ä¸‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print(f"pip install git+https://github.com/openai/CLIP.git")
        print(f"pip install torch torchvision")


if __name__ == "__main__":
    main()