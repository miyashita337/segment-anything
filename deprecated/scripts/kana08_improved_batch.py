#!/usr/bin/env python3
"""
kana08æ”¹å–„ç‰ˆãƒãƒƒãƒæŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è©•ä¾¡çµæœã«åŸºã¥ãæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ çµ±åˆç‰ˆ
"""

import numpy as np
import cv2
import torch

from segment_anything import SamAutomaticMaskGenerator, sam_model_registry

import logging
import sys
import time
from pathlib import Path
from typing import List, Optional, Tuple
from ultralytics import YOLO

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from features.evaluation.utils.non_character_filter import NonCharacterFilter
from features.processing.limb_protection_system import LimbProtectionSystem
# æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from features.processing.preprocessing.boundary_enhancer import BoundaryEnhancer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ImprovedKana08Extractor:
    """kana08æ”¹å–„ç‰ˆãƒãƒƒãƒæŠ½å‡ºå™¨"""
    
    def __init__(self):
        self.input_dir = Path("/mnt/c/AItools/lora/train/yado/org/kana08")
        self.output_dir = Path("/mnt/c/AItools/lora/train/yado/clipped_boundingbox/kana08_rev_merge2")
        
        # æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        logger.info("æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        
        self.boundary_enhancer = BoundaryEnhancer(
            skin_enhancement_factor=1.4,    # è‚Œè‰²å¼·èª¿ã‚’å¼·åŒ–
            edge_enhancement_factor=2.2,    # ã‚¨ãƒƒã‚¸å¼·èª¿ã‚’å¼·åŒ–
            contrast_enhancement=1.3        # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ã‚’ç¶­æŒ
        )
        
        self.non_character_filter = NonCharacterFilter()
        
        self.limb_protector = LimbProtectionSystem(
            enable_pose_estimation=True,
            enable_limb_completion=True,
            protection_margin=12  # å°‘ã—æ§ãˆã‚ã«èª¿æ•´
        )
        
        # SAMãƒ»YOLOãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        logger.info("SAMãƒ»YOLOãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
        
        sam_checkpoint = Path("/mnt/c/AItools/segment-anything/sam_vit_h_4b8939.pth")
        if not sam_checkpoint.exists():
            logger.error(f"SAMãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sam_checkpoint}")
            raise FileNotFoundError(f"SAMãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™: {sam_checkpoint}")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        sam = sam_model_registry["vit_h"](checkpoint=str(sam_checkpoint))
        sam.to(device=device)
        self.sam_generator = SamAutomaticMaskGenerator(sam)
        
        self.yolo_model = YOLO('yolov8n.pt')
        
        # è¨­å®š
        self.confidence_threshold = 0.07
        
        logger.info("æ”¹å–„ç‰ˆæŠ½å‡ºå™¨åˆæœŸåŒ–å®Œäº†")
    
    def process_image(self, image_path: Path) -> Tuple[bool, Optional[str], Optional[dict]]:
        """æ”¹å–„ç‰ˆå˜ä¸€ç”»åƒã®å‡¦ç†"""
        try:
            start_time = time.time()
            
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(str(image_path))
            if image is None:
                return False, "ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—", None
            
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            processing_stats = {"original_size": image_rgb.shape}
            
            # Step 1: å¢ƒç•Œå¼·èª¿å‰å‡¦ç†
            enhanced_image = self.boundary_enhancer.enhance_image_boundaries(image_rgb)
            enhancement_stats = self.boundary_enhancer.get_enhancement_stats(image_rgb, enhanced_image)
            processing_stats["enhancement"] = enhancement_stats
            
            logger.debug(f"å¢ƒç•Œå¼·èª¿: ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæ”¹å–„ {enhancement_stats['contrast_improvement']:.2f}x")
            
            # Step 2: YOLOæ¤œå‡ºï¼ˆæ”¹å–„ç‰ˆå‰å‡¦ç†ç”»åƒä½¿ç”¨ï¼‰
            results = self.yolo_model(cv2.cvtColor(enhanced_image, cv2.COLOR_RGB2BGR), conf=self.confidence_threshold)
            
            if not results or len(results[0].boxes) == 0:
                return False, "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æœªæ¤œå‡º", processing_stats
            
            # Step 3: SAMã§ãƒã‚¹ã‚¯ç”Ÿæˆ
            sam_masks = self.sam_generator.generate(enhanced_image)
            if not sam_masks:
                return False, "SAMãƒã‚¹ã‚¯ç”Ÿæˆå¤±æ•—", processing_stats
            
            # Step 4: YOLOæ¤œå‡ºçµæœã¨çµ±åˆ
            boxes = results[0].boxes.xyxy.cpu().numpy()
            integrated_masks = self._integrate_yolo_sam_masks(sam_masks, boxes, enhanced_image)
            
            if not integrated_masks:
                return False, "çµ±åˆãƒã‚¹ã‚¯ç”Ÿæˆå¤±æ•—", processing_stats
            
            # Step 5: éã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¦ç´ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_masks = self.non_character_filter.filter_non_character_elements(
                integrated_masks, cv2.cvtColor(enhanced_image, cv2.COLOR_RGB2BGR)
            )
            
            processing_stats["filtering"] = {
                "original_candidates": len(integrated_masks),
                "filtered_candidates": len(filtered_masks)
            }
            
            if not filtered_masks:
                return False, "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œãƒã‚¹ã‚¯ãªã—", processing_stats
            
            # Step 6: æœ€é©ãƒã‚¹ã‚¯é¸æŠï¼ˆã‚µã‚¤ã‚ºãƒ»ä½ç½®ãƒ»å“è³ªã®è¤‡åˆè©•ä¾¡ï¼‰
            best_mask_data = self._select_best_mask_improved(filtered_masks, enhanced_image.shape)
            
            if not best_mask_data:
                return False, "æœ€é©ãƒã‚¹ã‚¯é¸æŠå¤±æ•—", processing_stats
            
            best_mask = best_mask_data['segmentation']
            
            # Step 7: æ‰‹è¶³ä¿è­·ã‚·ã‚¹ãƒ†ãƒ é©ç”¨
            protected_mask, limb_analysis = self.limb_protector.protect_limbs_in_mask(
                enhanced_image, best_mask
            )
            
            processing_stats["limb_protection"] = limb_analysis
            
            # Step 8: æœ€çµ‚æŠ½å‡ºå‡¦ç†
            extracted_image = self._extract_character_with_mask(enhanced_image, protected_mask)
            
            if extracted_image is None:
                return False, "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºå¤±æ•—", processing_stats
            
            # Step 9: çµæœä¿å­˜
            output_path = self.output_dir / image_path.name
            cv2.imwrite(str(output_path), cv2.cvtColor(extracted_image, cv2.COLOR_RGB2BGR))
            
            processing_time = time.time() - start_time
            processing_stats["total_time"] = processing_time
            
            quality_info = f"æ”¹å–„ç‰ˆæŠ½å‡º"
            if limb_analysis["protection_applied"]:
                quality_info += f" (æ‰‹è¶³ä¿è­·: {limb_analysis['protection_quality']:.2f})"
            
            return True, quality_info, processing_stats
            
        except Exception as e:
            return False, f"ã‚¨ãƒ©ãƒ¼: {str(e)}", None
    
    def _integrate_yolo_sam_masks(self, sam_masks: List[dict], yolo_boxes: np.ndarray, 
                                image: np.ndarray) -> List[dict]:
        """YOLOæ¤œå‡ºçµæœã¨SAMãƒã‚¹ã‚¯ã®çµ±åˆ"""
        integrated_masks = []
        
        for mask_data in sam_masks:
            mask = mask_data['segmentation']
            
            # ãƒã‚¹ã‚¯ã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹å–å¾—
            y_indices, x_indices = np.where(mask)
            if len(x_indices) == 0:
                continue
            
            mask_x1, mask_y1 = np.min(x_indices), np.min(y_indices)
            mask_x2, mask_y2 = np.max(x_indices), np.max(y_indices)
            
            # YOLOãƒœãƒƒã‚¯ã‚¹ã¨ã®é‡è¤‡ç¢ºèª
            best_overlap = 0.0
            best_yolo_score = 0.0
            
            for box in yolo_boxes:
                yolo_x1, yolo_y1, yolo_x2, yolo_y2 = box
                
                # é‡è¤‡é ˜åŸŸè¨ˆç®—
                overlap_x1 = max(mask_x1, yolo_x1)
                overlap_y1 = max(mask_y1, yolo_y1)
                overlap_x2 = min(mask_x2, yolo_x2)
                overlap_y2 = min(mask_y2, yolo_y2)
                
                if overlap_x2 > overlap_x1 and overlap_y2 > overlap_y1:
                    overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
                    mask_area = (mask_x2 - mask_x1) * (mask_y2 - mask_y1)
                    overlap_ratio = overlap_area / max(mask_area, 1)
                    
                    if overlap_ratio > best_overlap:
                        best_overlap = overlap_ratio
                        best_yolo_score = 0.8  # YOLOä¿¡é ¼åº¦ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            
            # ååˆ†ãªé‡è¤‡ãŒã‚ã‚‹ãƒã‚¹ã‚¯ã®ã¿æ¡ç”¨
            if best_overlap > 0.3:
                mask_data['yolo_overlap'] = best_overlap
                mask_data['yolo_confidence'] = best_yolo_score
                mask_data['bbox'] = (mask_x1, mask_y1, mask_x2 - mask_x1, mask_y2 - mask_y1)
                integrated_masks.append(mask_data)
        
        return integrated_masks
    
    def _select_best_mask_improved(self, masks: List[dict], image_shape: Tuple[int, int, int]) -> Optional[dict]:
        """æ”¹å–„ç‰ˆæœ€é©ãƒã‚¹ã‚¯é¸æŠ"""
        if not masks:
            return None
        
        h, w = image_shape[:2]
        best_mask = None
        best_score = 0.0
        
        for mask_data in masks:
            score = 0.0
            
            # ã‚µã‚¤ã‚ºã‚¹ã‚³ã‚¢
            area = mask_data.get('area', 0)
            area_ratio = area / (h * w)
            if 0.02 <= area_ratio <= 0.4:  # é©åˆ‡ãªã‚µã‚¤ã‚ºç¯„å›²
                score += min(area_ratio / 0.2, 1.0) * 0.3
            
            # ä½ç½®ã‚¹ã‚³ã‚¢ï¼ˆä¸­å¤®å¯„ã‚Šï¼‰
            bbox = mask_data.get('bbox', [0, 0, 0, 0])
            if len(bbox) >= 4:
                center_x = bbox[0] + bbox[2] / 2
                center_y = bbox[1] + bbox[3] / 2
                
                # ç”»åƒä¸­å¤®ã‹ã‚‰ã®è·é›¢
                distance_from_center = np.sqrt(
                    ((center_x - w/2) / w)**2 + 
                    ((center_y - h/2) / h)**2
                )
                score += max(0, 1.0 - distance_from_center) * 0.2
            
            # YOLOé‡è¤‡ã‚¹ã‚³ã‚¢
            yolo_overlap = mask_data.get('yolo_overlap', 0)
            score += yolo_overlap * 0.3
            
            # SAMå“è³ªã‚¹ã‚³ã‚¢
            stability_score = mask_data.get('stability_score', 0.5)
            score += stability_score * 0.2
            
            if score > best_score:
                best_score = score
                best_mask = mask_data
        
        return best_mask
    
    def _extract_character_with_mask(self, image: np.ndarray, mask: np.ndarray) -> Optional[np.ndarray]:
        """ãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã—ãŸã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡º"""
        if len(mask.shape) == 3:
            mask = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY)
        
        # ãƒã‚¹ã‚¯ã‚’3ãƒãƒ£ãƒãƒ«ã«å¤‰æ›
        mask_normalized = mask.astype(np.float32) / 255.0
        mask_3ch = np.stack([mask_normalized] * 3, axis=-1)
        
        # èƒŒæ™¯ã‚’é»’ã«ã—ã¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’æŠ½å‡º
        extracted = image.astype(np.float32) * mask_3ch
        
        # å¢ƒç•Œã®å–å¾—ã¨ã‚¯ãƒ­ãƒƒãƒ—
        y_indices, x_indices = np.where(mask > 0)
        if len(x_indices) == 0:
            return None
        
        x_min, x_max = np.min(x_indices), np.max(x_indices)
        y_min, y_max = np.min(y_indices), np.max(y_indices)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ 
        padding = 10
        x_min = max(0, x_min - padding)
        y_min = max(0, y_min - padding)
        x_max = min(image.shape[1], x_max + padding)
        y_max = min(image.shape[0], y_max + padding)
        
        # ã‚¯ãƒ­ãƒƒãƒ—
        cropped = extracted[y_min:y_max, x_min:x_max]
        
        return cropped.astype(np.uint8)
    
    def run_batch(self):
        """æ”¹å–„ç‰ˆãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ"""
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_dir.mkdir(exist_ok=True)
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
        image_files = sorted(list(self.input_dir.glob("*.jpg")))
        total = len(image_files)
        
        if total == 0:
            logger.error("å‡¦ç†ã™ã‚‹ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        logger.info(f"æ”¹å–„ç‰ˆãƒãƒƒãƒå‡¦ç†é–‹å§‹: {total}æšã®ç”»åƒ")
        logger.info(f"å…¥åŠ›: {self.input_dir}")
        logger.info(f"å‡ºåŠ›: {self.output_dir}")
        
        # å‡¦ç†çµ±è¨ˆ
        success_count = 0
        failed_files = []
        improvement_stats = []
        start_time = time.time()
        
        # å„ç”»åƒã‚’å‡¦ç†
        for i, image_path in enumerate(image_files, 1):
            logger.info(f"[{i}/{total}] å‡¦ç†ä¸­: {image_path.name}")
            
            success, message, stats = self.process_image(image_path)
            
            if success:
                success_count += 1
                logger.info(f"  âœ… æˆåŠŸ - {message}")
                if stats:
                    improvement_stats.append(stats)
            else:
                failed_files.append((image_path.name, message))
                logger.warning(f"  âŒ å¤±æ•— - {message}")
            
            # é€²æ—è¡¨ç¤º
            if i % 5 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / i
                remaining = avg_time * (total - i)
                logger.info(f"é€²æ—: {i}/{total} ({i/total*100:.1f}%) - æ®‹ã‚Šæ™‚é–“: {remaining:.0f}ç§’")
        
        # å‡¦ç†å®Œäº†
        total_time = time.time() - start_time
        logger.info("=" * 50)
        logger.info("æ”¹å–„ç‰ˆãƒãƒƒãƒå‡¦ç†å®Œäº†")
        logger.info(f"ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        logger.info(f"å¹³å‡å‡¦ç†æ™‚é–“: {total_time/total:.1f}ç§’/ç”»åƒ")
        logger.info(f"æˆåŠŸ: {success_count}/{total} ({success_count/total*100:.1f}%)")
        
        # æ”¹å–„çµ±è¨ˆã®è¡¨ç¤º
        if improvement_stats:
            avg_enhancement = np.mean([s["enhancement"]["contrast_improvement"] 
                                     for s in improvement_stats if "enhancement" in s])
            
            filtering_effective = sum(1 for s in improvement_stats 
                                    if "filtering" in s and 
                                    s["filtering"]["filtered_candidates"] < s["filtering"]["original_candidates"])
            
            limb_protection_applied = sum(1 for s in improvement_stats 
                                        if "limb_protection" in s and 
                                        s["limb_protection"]["protection_applied"])
            
            logger.info("ğŸ”§ æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ:")
            logger.info(f"  å¹³å‡ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæ”¹å–„: {avg_enhancement:.2f}x")
            logger.info(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æœ‰åŠ¹: {filtering_effective}/{len(improvement_stats)}ä»¶")
            logger.info(f"  æ‰‹è¶³ä¿è­·é©ç”¨: {limb_protection_applied}/{len(improvement_stats)}ä»¶")
        
        if failed_files:
            logger.info("å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«:")
            for name, reason in failed_files:
                logger.info(f"  - {name}: {reason}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    extractor = ImprovedKana08Extractor()
    extractor.run_batch()


if __name__ == "__main__":
    main()