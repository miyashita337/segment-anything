#!/usr/bin/env python3
"""
Character Extraction Command
Main command for extracting characters from manga images using SAM + YOLO
"""

import os
import sys
import argparse
import time
from pathlib import Path
from typing import Optional, Dict, Any, List

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

import cv2
import numpy as np

from hooks.start import get_sam_model, get_yolo_model, get_performance_monitor
from utils.difficult_pose import (
    DifficultPoseProcessor, 
    detect_difficult_pose, 
    get_difficult_pose_config,
    process_with_retry
)
from utils.preprocessing import preprocess_image_pipeline
from utils.postprocessing import (
    enhance_character_mask, 
    extract_character_from_image, 
    crop_to_content,
    save_character_result,
    calculate_mask_quality_metrics
)
from utils.text_detection import TextDetector

# Phase 4: çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.phase4_integration import Phase4IntegratedExtractor

# Phase 4.1: é¸æŠçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.phase41_integrated_system import Phase41IntegratedSystem
from utils.multi_character_handler import SelectionCriteria


def extract_character_from_path(image_path: str,
                               output_path: Optional[str] = None,
                               enhance_contrast: bool = False,
                               filter_text: bool = True,
                               save_mask: bool = False,
                               save_transparent: bool = False,
                               min_yolo_score: float = 0.1,
                               verbose: bool = True,
                               difficult_pose: bool = False,
                               low_threshold: bool = False,
                               auto_retry: bool = False,
                               high_quality: bool = False,
                               manga_mode: bool = False,
                               effect_removal: bool = False,
                               panel_split: bool = False,
                               # Phase 4: æ–°ã—ã„ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                               enable_phase4: bool = False,
                               enable_mask_inversion_detection: bool = False,
                               enable_adaptive_range: bool = False,
                               enable_quality_prediction: bool = False,
                               # Phase 4.1: é¸æŠçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                               enable_phase41: bool = False,
                               multi_character_criteria: str = "balanced",
                               **kwargs) -> Dict[str, Any]:
    """
    ç”»åƒãƒ‘ã‚¹ã‹ã‚‰ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’æŠ½å‡º (Phase 4å¯¾å¿œç‰ˆ)
    
    Args:
        image_path: å…¥åŠ›ç”»åƒãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ‘ã‚¹ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        enhance_contrast: ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–
        filter_text: ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        save_mask: ãƒã‚¹ã‚¯ã‚’ä¿å­˜
        save_transparent: é€æ˜èƒŒæ™¯ç‰ˆã‚’ä¿å­˜
        min_yolo_score: YOLOæœ€å°ã‚¹ã‚³ã‚¢
        verbose: è©³ç´°å‡ºåŠ›
        difficult_pose: è¤‡é›‘ãƒãƒ¼ã‚ºãƒ¢ãƒ¼ãƒ‰
        low_threshold: ä½é–¾å€¤ãƒ¢ãƒ¼ãƒ‰ï¼ˆYOLO 0.02ï¼‰
        auto_retry: è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ¢ãƒ¼ãƒ‰
        high_quality: é«˜å“è³ªSAMå‡¦ç†
        manga_mode: æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ (Phase 2)
        effect_removal: ã‚¨ãƒ•ã‚§ã‚¯ãƒˆç·šé™¤å»ã‚’æœ‰åŠ¹åŒ– (Phase 2)
        panel_split: ãƒãƒ«ãƒã‚³ãƒåˆ†å‰²ã‚’æœ‰åŠ¹åŒ– (Phase 2)
        enable_phase4: Phase 4çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’æœ‰åŠ¹åŒ–
        enable_mask_inversion_detection: ãƒã‚¹ã‚¯é€†è»¢æ¤œå‡ºãƒ»ä¿®æ­£
        enable_adaptive_range: é©å¿œçš„ç¯„å›²èª¿æ•´
        enable_quality_prediction: å“è³ªäºˆæ¸¬ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        
    Returns:
        æŠ½å‡ºçµæœã®è¾æ›¸
    """
    result = {
        'success': False,
        'input_path': image_path,
        'output_path': None,
        'processing_time': 0.0,
        'mask_quality': {},
        'error': None
    }
    
    start_time = time.time()
    
    # è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ process_with_retry ã‚’ä½¿ç”¨
    if auto_retry:
        if verbose:
            print(f"ğŸ”„ è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ¢ãƒ¼ãƒ‰ã§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºé–‹å§‹: {image_path}")
        
        def extract_function(img_path, **config):
            # å…ƒã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—ï¼ˆãƒªãƒˆãƒ©ã‚¤ç”¨ï¼‰
            return extract_character_from_path(
                img_path, output_path, enhance_contrast, filter_text,
                save_mask, save_transparent, config.get('min_yolo_score', min_yolo_score),
                verbose=False,  # ãƒªãƒˆãƒ©ã‚¤ä¸­ã¯è©³ç´°å‡ºåŠ›ã‚’æŠ‘åˆ¶
                difficult_pose=False, low_threshold=False, auto_retry=False,  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
                high_quality=config.get('enable_enhanced_processing', high_quality),
                manga_mode=config.get('enable_manga_preprocessing', manga_mode),
                effect_removal=config.get('enable_effect_removal', effect_removal),
                panel_split=config.get('enable_panel_split', panel_split),
                **{k: v for k, v in config.items() if k not in [
                    'min_yolo_score', 'enable_enhanced_processing', 'enable_manga_preprocessing',
                    'enable_effect_removal', 'enable_panel_split'
                ]}
            )
        
        return process_with_retry(image_path, extract_function, max_retries=4)
    
    try:
        # Get models
        sam_model = get_sam_model()
        yolo_model = get_yolo_model()
        performance_monitor = get_performance_monitor()
        
        if not sam_model or not yolo_model:
            raise RuntimeError("Models not initialized. Run start hook first.")
        
        # è¤‡é›‘ãƒãƒ¼ã‚ºåˆ¤å®šã¨è¨­å®šèª¿æ•´ (Phase 2å¯¾å¿œç‰ˆ)
        if difficult_pose or low_threshold or manga_mode:
            processor = DifficultPoseProcessor()
            
            if difficult_pose:
                # è¤‡é›‘ãƒãƒ¼ã‚ºãƒ¢ãƒ¼ãƒ‰: è‡ªå‹•åˆ¤å®šã«ã‚ˆã‚‹è¨­å®š
                complexity_info = processor.detect_pose_complexity(image_path)
                recommended_config = processor.get_recommended_config(complexity_info)
                
                if verbose:
                    print(f"ğŸ” ãƒãƒ¼ã‚ºè¤‡é›‘åº¦: {complexity_info['complexity']} (ã‚¹ã‚³ã‚¢: {complexity_info['score']:.1f})")
                    print(f"ğŸ”§ æ¨å¥¨è¨­å®šé©ç”¨: {recommended_config['description']}")
                
                # æ¨å¥¨è¨­å®šã‚’é©ç”¨
                min_yolo_score = min(min_yolo_score, recommended_config['min_yolo_score'])
                if 'sam_points_per_side' in recommended_config:
                    # SAMè¨­å®šã‚’kwargsã«è¿½åŠ 
                    kwargs.update({
                        'sam_points_per_side': recommended_config['sam_points_per_side'],
                        'sam_pred_iou_thresh': recommended_config['sam_pred_iou_thresh'],
                        'sam_stability_score_thresh': recommended_config['sam_stability_score_thresh']
                    })
            
            if low_threshold:
                min_yolo_score = 0.02
                if verbose:
                    print(f"ğŸ”§ ä½é–¾å€¤ãƒ¢ãƒ¼ãƒ‰: YOLOé–¾å€¤ã‚’{min_yolo_score}ã«è¨­å®š")
            
            # Phase 2: æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
            if manga_mode or effect_removal or panel_split:
                if verbose:
                    print(f"ğŸ¨ æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
                    print(f"   ã‚¨ãƒ•ã‚§ã‚¯ãƒˆç·šé™¤å»: {'âœ…' if effect_removal else 'âŒ'}")
                    print(f"   ãƒãƒ«ãƒã‚³ãƒåˆ†å‰²: {'âœ…' if panel_split else 'âŒ'}")
                
                # å‰å‡¦ç†ã‚’é©ç”¨
                processed_image_path = processor.preprocess_for_difficult_pose(
                    image_path,
                    enable_manga_preprocessing=True,
                    enable_effect_removal=effect_removal,
                    enable_panel_split=panel_split
                )
                
                # å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ä½¿ç”¨
                image_path = processed_image_path
        
        if high_quality:
            # é«˜å“è³ªSAMè¨­å®š
            kwargs.update({
                'sam_points_per_side': kwargs.get('sam_points_per_side', 64),
                'sam_pred_iou_thresh': kwargs.get('sam_pred_iou_thresh', 0.88),
                'sam_stability_score_thresh': kwargs.get('sam_stability_score_thresh', 0.92)
            })
            if verbose:
                print(f"ğŸ”§ é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰: SAMå¯†åº¦ {kwargs.get('sam_points_per_side', 64)} ãƒã‚¤ãƒ³ãƒˆ/ã‚µã‚¤ãƒ‰")
        
        if verbose:
            print(f"ğŸ¯ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºé–‹å§‹: {image_path}")
            print(f"ğŸ“Š YOLOé–¾å€¤: {min_yolo_score}")
        
        # Step 1: Image preprocessing
        performance_monitor.start_stage("Image Preprocessing")
        bgr_image, rgb_image, scale = preprocess_image_pipeline(
            image_path, 
            enhance_contrast=enhance_contrast
        )
        
        if rgb_image is None:
            raise ValueError(f"Failed to load image: {image_path}")
        
        performance_monitor.end_stage()
        
        # Phase 4.1: é¸æŠçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å‡¦ç†åˆ†å²
        if enable_phase41:
            if verbose:
                print("ğŸŒŸ Phase 4.1é¸æŠçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­...")
            
            # Phase 4.1çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            try:
                criteria = SelectionCriteria(multi_character_criteria)
            except ValueError:
                criteria = SelectionCriteria.BALANCED
                if verbose:
                    print(f"âš ï¸ ç„¡åŠ¹ãªé¸æŠåŸºæº–: {multi_character_criteria}, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ(balanced)ã‚’ä½¿ç”¨")
            
            phase41_system = Phase41IntegratedSystem(
                sam_model=sam_model,
                yolo_model=yolo_model,
                multi_character_criteria=criteria,
                enable_detailed_logging=verbose
            )
            
            # YOLOæ¤œå‡ºã®å®Ÿè¡Œ
            yolo_detections = yolo_model.detect_persons(bgr_image)
            if not yolo_detections or len(yolo_detections) == 0:
                if verbose:
                    print(f"âš ï¸ YOLOæ¤œå‡ºãªã— (é–¾å€¤: {min_yolo_score}), ä½é–¾å€¤ã§å†è©¦è¡Œ...")
                # ä½é–¾å€¤ã§å†è©¦è¡Œ
                yolo_detections = yolo_model.detect_persons(bgr_image, score_threshold=0.02)
                if not yolo_detections:
                    raise ValueError(f"YOLOæ¤œå‡ºçµæœãªã—")
            
            # YOLOçµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_detections = []
            for det in yolo_detections:
                if det['score'] >= min_yolo_score:
                    formatted_detections.append({
                        "bbox": det['bbox'],
                        "confidence": det['score']
                    })
            
            if verbose:
                print(f"ğŸ“Š YOLOæ¤œå‡º: {len(formatted_detections)}å€‹ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼")
            
            # Phase 4.1çµ±åˆå‡¦ç†
            phase41_result = phase41_system.extract_character(bgr_image, formatted_detections)
            
            if not phase41_result.success:
                raise RuntimeError(f"Phase 4.1å‡¦ç†å¤±æ•—: {phase41_result.error_message}")
            
            # çµæœã®å¤‰æ›
            enhanced_mask = phase41_result.final_mask
            character_image = extract_character_from_image(
                bgr_image, 
                enhanced_mask,
                background_color=(0, 0, 0)
            )
            
            # Phase 4.1å°‚ç”¨çµæœæ§‹ç¯‰
            result = {
                'success': True,
                'image_path': image_path,
                'output_path': output_path,
                'mask': enhanced_mask,
                'character_image': character_image,
                'bbox': phase41_result.final_bbox,
                'phase41_result': phase41_result,
                'processing_engine': phase41_result.selected_engine.value,
                'quality_score': phase41_result.quality_score,
                'yolo_detections': phase41_result.yolo_detections,
                'final_character_count': phase41_result.final_character_count,
                'complexity_level': phase41_result.complexity_analysis.level.value if phase41_result.complexity_analysis else "unknown",
                'multi_character_selection': phase41_result.multi_character_analysis.success if phase41_result.multi_character_analysis else False,
                'adjustments_made': phase41_result.adjustments_made,
                'processing_time': phase41_result.processing_time,
                'warnings': phase41_result.warnings or []
            }
            
            if verbose:
                print(f"âœ… Phase 4.1å‡¦ç†å®Œäº†:")
                print(f"   é¸æŠã‚¨ãƒ³ã‚¸ãƒ³: {result['processing_engine']}")
                print(f"   å“è³ªã‚¹ã‚³ã‚¢: {result['quality_score']:.3f}")
                print(f"   è¤‡é›‘åº¦: {result['complexity_level']}")
                if result['multi_character_selection']:
                    print(f"   è¤‡æ•°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é¸æŠ: æˆåŠŸ")
                if result['adjustments_made']:
                    print(f"   é©ç”¨ã•ã‚ŒãŸèª¿æ•´: {', '.join(result['adjustments_made'])}")
                if result['warnings']:
                    print(f"   è­¦å‘Š: {'; '.join(result['warnings'])}")
        
        # Phase 4: çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å‡¦ç†åˆ†å²
        elif enable_phase4 or enable_mask_inversion_detection or enable_adaptive_range or enable_quality_prediction:
            if verbose:
                print("ğŸš€ Phase 4çµ±åˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­...")
            
            # Phase 4çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            phase4_extractor = Phase4IntegratedExtractor(
                enable_mask_inversion_detection=enable_mask_inversion_detection or enable_phase4,
                enable_adaptive_range=enable_adaptive_range or enable_phase4,
                enable_quality_prediction=enable_quality_prediction or enable_phase4,
                max_iterations=3
            )
            
            # YOLOæ¤œå‡ºã®å®Ÿè¡Œï¼ˆPhase 4ã§å¿…è¦ï¼‰
            yolo_detections = yolo_model.detect_persons(bgr_image)
            if not yolo_detections or len(yolo_detections) == 0:
                raise ValueError(f"No YOLO detections found (min score: {min_yolo_score})")
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®æ¤œå‡ºã‚’ä½¿ç”¨
            best_detection = max(yolo_detections, key=lambda x: x.get('confidence', 0))
            yolo_bbox = (
                int(best_detection['bbox'][0]),
                int(best_detection['bbox'][1]), 
                int(best_detection['bbox'][2]),
                int(best_detection['bbox'][3])
            )
            yolo_confidence = best_detection.get('confidence', 0.0)
            
            if verbose:
                print(f"ğŸ¯ YOLOæ¤œå‡º: bbox={yolo_bbox}, confidence={yolo_confidence:.3f}")
            
            # Phase 4ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
            phase4_params = {
                'min_yolo_score': min_yolo_score,
                'high_quality': high_quality,
                'manga_mode': manga_mode,
                'effect_removal': effect_removal,
                'expansion_factor': kwargs.get('expansion_factor', 1.1),
                'difficult_pose': difficult_pose,
                'low_threshold': low_threshold,
                'auto_retry': auto_retry
            }
            
            # Phase 4çµ±åˆå‡¦ç†å®Ÿè¡Œ
            phase4_result = phase4_extractor.extract_with_phase4_enhancements(
                rgb_image, yolo_bbox, yolo_confidence, sam_model, phase4_params
            )
            
            if phase4_result.success and phase4_result.final_mask is not None:
                if verbose:
                    print(f"âœ… Phase 4å‡¦ç†æˆåŠŸ")
                    print(f"   å“è³ªã‚¹ã‚³ã‚¢: {phase4_result.quality_metrics.confidence_score:.3f}")
                    print(f"   å®Ÿè¡Œèª¿æ•´: {phase4_result.adjustments_made}")
                    print(f"   å‡¦ç†æ™‚é–“: {phase4_result.processing_stats['processing_time']:.3f}ç§’")
                
                # Phase 4çµæœã‚’ä½¿ç”¨ï¼ˆé©åˆ‡ãªå½¢å¼ã«å¤‰æ›ï¼‰
                raw_mask = phase4_result.final_mask
                if raw_mask.dtype == bool:
                    enhanced_mask = raw_mask.astype(np.uint8) * 255
                else:
                    enhanced_mask = raw_mask
                result['phase4_stats'] = phase4_result.processing_stats
                result['phase4_adjustments'] = phase4_result.adjustments_made
                result['mask_quality'] = {
                    'coverage_ratio': phase4_result.quality_metrics.confidence_score,
                    'compactness': phase4_result.quality_metrics.edge_consistency,
                    'phase4_enabled': True
                }
                
                # Phase 4å‡¦ç†å¾Œã¯é€šå¸¸ã®å¾Œå‡¦ç†ã¸ã‚¸ãƒ£ãƒ³ãƒ—
                performance_monitor.start_stage("Character Extraction")
                character_image = extract_character_from_image(
                    bgr_image, 
                    enhanced_mask,
                    background_color=(0, 0, 0)
                )
                
                # Crop to content
                cropped_character, cropped_mask, crop_bbox = crop_to_content(
                    character_image,
                    enhanced_mask,
                    padding=10
                )
                
                performance_monitor.end_stage()
                
                # Step 7: Save results
                performance_monitor.start_stage("Saving Results")
                
                # Generate output path if not provided
                if output_path is None:
                    input_path = Path(image_path)
                    output_dir = input_path.parent / "character_output"
                    output_dir.mkdir(exist_ok=True)
                    output_path = output_dir / input_path.stem
                
                # Save results
                save_success = save_character_result(
                    cropped_character,
                    cropped_mask,
                    str(output_path),
                    save_mask=save_mask,
                    save_transparent=save_transparent
                )
                
                if not save_success:
                    raise RuntimeError("Failed to save results")
                
                result['output_path'] = str(output_path)
                performance_monitor.end_stage()
                
                # Success
                result['success'] = True
                result['processing_time'] = time.time() - start_time
                
                if verbose:
                    print(f"âœ… Phase 4ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºå®Œäº†: {result['processing_time']:.2f}ç§’")
                    print(f"   å‡ºåŠ›: {result['output_path']}")
                
                return result
            else:
                if verbose:
                    print(f"âš ï¸ Phase 4å‡¦ç†å¤±æ•—ã€å¾“æ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # Step 2: SAM mask generation (å¾“æ¥å‡¦ç†)
        performance_monitor.start_stage("SAM Mask Generation")
        
        # é«˜å“è³ª/è¤‡é›‘ãƒãƒ¼ã‚ºãƒ¢ãƒ¼ãƒ‰ã§SAMè¨­å®šã‚’å‹•çš„ã«é©ç”¨
        if any(key.startswith('sam_') for key in kwargs.keys()) or high_quality:
            # SAMGeneratorã‚’ä¸€æ™‚çš„ã«å†æ§‹ç¯‰
            try:
                from segment_anything import SamAutomaticMaskGenerator
                
                sam_params = {
                    'model': sam_model.sam,
                    'points_per_side': kwargs.get('sam_points_per_side', 32),
                    'pred_iou_thresh': kwargs.get('sam_pred_iou_thresh', 0.8),
                    'stability_score_thresh': kwargs.get('sam_stability_score_thresh', 0.85),
                    'crop_n_layers': 1,
                    'crop_n_points_downscale_factor': 2,
                    'min_mask_region_area': 100,
                }
                
                if verbose:
                    print(f"ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ SAMè¨­å®šé©ç”¨:")
                    print(f"   ãƒã‚¤ãƒ³ãƒˆå¯†åº¦: {sam_params['points_per_side']}")
                    print(f"   IoUé–¾å€¤: {sam_params['pred_iou_thresh']}")
                    print(f"   å®‰å®šæ€§é–¾å€¤: {sam_params['stability_score_thresh']}")
                
                # ä¸€æ™‚çš„ãªãƒã‚¹ã‚¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã§å‡¦ç†
                temp_generator = SamAutomaticMaskGenerator(**sam_params)
                all_masks = temp_generator.generate(rgb_image)
                
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ ã‚«ã‚¹ã‚¿ãƒ SAMè¨­å®šå¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ç¶™ç¶š: {e}")
                all_masks = sam_model.generate_masks(rgb_image)
        else:
            all_masks = sam_model.generate_masks(rgb_image)
        
        if not all_masks:
            raise ValueError("No masks generated by SAM")
        
        character_masks = sam_model.filter_character_masks(all_masks)
        
        if verbose:
            print(f"ğŸ“Š ç”Ÿæˆãƒã‚¹ã‚¯: {len(all_masks)} â†’ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å€™è£œ: {len(character_masks)}")
        
        performance_monitor.end_stage()
        
        # Step 3: YOLO scoring
        performance_monitor.start_stage("YOLO Scoring")
        scored_masks = yolo_model.score_masks_with_detections(character_masks, bgr_image)
        
        best_mask = yolo_model.get_best_character_mask(
            scored_masks, 
            bgr_image, 
            min_yolo_score=min_yolo_score
        )
        
        if best_mask is None:
            raise ValueError(f"No good character masks found (min YOLO score: {min_yolo_score})")
        
        if verbose:
            print(f"ğŸ¯ æœ€é©ãƒã‚¹ã‚¯é¸æŠ: YOLO score={best_mask['yolo_score']:.3f}, "
                  f"combined score={best_mask['combined_score']:.3f}")
        
        performance_monitor.end_stage()
        
        # Step 4: Text filtering (optional)
        if filter_text:
            performance_monitor.start_stage("Text Filtering")
            text_detector = TextDetector(use_easyocr=True)
            
            text_density = text_detector.calculate_text_density_score(
                bgr_image, 
                best_mask['bbox']
            )
            
            if text_density > 0.5:
                if verbose:
                    print(f"âš ï¸ é«˜ãƒ†ã‚­ã‚¹ãƒˆå¯†åº¦æ¤œå‡º: {text_density:.3f} - å‡¦ç†ç¶šè¡Œ")
            
            # Add text density to result
            best_mask['text_density'] = text_density
            performance_monitor.end_stage()
        
        # Step 5: Mask refinement
        performance_monitor.start_stage("Mask Refinement")
        raw_mask = sam_model.mask_to_binary(best_mask)
        
        # è¤‡é›‘ãƒãƒ¼ã‚ºç”¨ã®å¼·åŒ–ãƒã‚¹ã‚¯å‡¦ç†ã‚’é©ç”¨
        if difficult_pose or low_threshold or high_quality:
            if verbose:
                print("ğŸ”§ è¤‡é›‘ãƒãƒ¼ã‚ºç”¨ãƒã‚¹ã‚¯å¼·åŒ–å‡¦ç†ã‚’é©ç”¨")
            
            # DifficultPoseProcessorã‚’ä½¿ç”¨ã—ãŸå¼·åŒ–å‡¦ç†
            if 'processor' not in locals():
                processor = DifficultPoseProcessor()
            
            enhanced_mask = processor.enhance_mask_for_complex_pose(raw_mask, bgr_image)
        else:
            enhanced_mask = enhance_character_mask(
                raw_mask,
                remove_small_area=100,
                smooth_kernel=3,
                fill_holes=True
            )
        
        # Calculate mask quality metrics
        quality_metrics = calculate_mask_quality_metrics(enhanced_mask)
        result['mask_quality'] = quality_metrics
        
        if verbose:
            print(f"ğŸ“ ãƒã‚¹ã‚¯å“è³ª: coverage={quality_metrics['coverage_ratio']:.3f}, "
                  f"compactness={quality_metrics['compactness']:.3f}")
        
        performance_monitor.end_stage()
        
        # Step 6: Character extraction
        performance_monitor.start_stage("Character Extraction")
        character_image = extract_character_from_image(
            bgr_image, 
            enhanced_mask,
            background_color=(0, 0, 0)  # Black background
        )
        
        # Crop to content
        cropped_character, cropped_mask, crop_bbox = crop_to_content(
            character_image,
            enhanced_mask,
            padding=10
        )
        
        performance_monitor.end_stage()
        
        # Step 7: Save results
        performance_monitor.start_stage("Saving Results")
        
        # Generate output path if not provided
        if output_path is None:
            input_path = Path(image_path)
            output_dir = input_path.parent / "character_output"
            output_dir.mkdir(exist_ok=True)
            output_path = output_dir / input_path.stem
        
        # Save results
        save_success = save_character_result(
            cropped_character,
            cropped_mask,
            str(output_path),
            save_mask=save_mask,
            save_transparent=save_transparent
        )
        
        if not save_success:
            raise RuntimeError("Failed to save results")
        
        result['output_path'] = str(output_path)
        performance_monitor.end_stage()
        
        # Success
        result['success'] = True
        result['processing_time'] = time.time() - start_time
        
        if verbose:
            print(f"âœ… ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºå®Œäº†: {result['processing_time']:.2f}ç§’")
            print(f"   å‡ºåŠ›: {result['output_path']}")
        
        return result
        
    except Exception as e:
        result['error'] = str(e)
        result['processing_time'] = time.time() - start_time
        
        if verbose:
            print(f"âŒ æŠ½å‡ºå¤±æ•—: {e}")
        
        return result


def batch_extract_characters(input_dir: str,
                           output_dir: str,
                           **extract_kwargs) -> Dict[str, Any]:
    """
    ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ç”»åƒã«å¯¾ã—ã¦ãƒãƒƒãƒå‡¦ç†
    
    Args:
        input_dir: å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        **extract_kwargs: extract_character_from_path ã®å¼•æ•°
        
    Returns:
        ãƒãƒƒãƒå‡¦ç†çµæœ
    """
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    
    if not input_path.exists():
        return {'success': False, 'error': f'Input directory not found: {input_dir}'}
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    image_files = []
    
    for ext in image_extensions:
        image_files.extend(input_path.glob(f'*{ext}'))
        image_files.extend(input_path.glob(f'*{ext.upper()}'))
    
    if not image_files:
        return {'success': False, 'error': f'No image files found in {input_dir}'}
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
    results = []
    successful = 0
    
    print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(image_files)} ç”»åƒ")
    
    for i, image_file in enumerate(image_files, 1):
        print(f"\nğŸ“ å‡¦ç†ä¸­ [{i}/{len(image_files)}]: {image_file.name}")
        
        # å‡ºåŠ›ãƒ‘ã‚¹ç”Ÿæˆ
        output_file = output_path / image_file.stem
        
        # æŠ½å‡ºå®Ÿè¡Œ
        # verboseã¯ãƒãƒƒãƒå‡¦ç†ã§ã¯æŠ‘åˆ¶
        batch_kwargs = extract_kwargs.copy()
        batch_kwargs['verbose'] = False
        result = extract_character_from_path(
            str(image_file),
            output_path=str(output_file),
            **batch_kwargs
        )
        
        result['filename'] = image_file.name
        results.append(result)
        
        if result['success']:
            successful += 1
            print(f"âœ… æˆåŠŸ: {image_file.name}")
        else:
            print(f"âŒ å¤±æ•—: {image_file.name} - {result['error']}")
    
    # çµæœã‚µãƒãƒª
    batch_result = {
        'success': True,
        'total_files': len(image_files),
        'successful': successful,
        'failed': len(image_files) - successful,
        'success_rate': successful / len(image_files),
        'results': results
    }
    
    print(f"\nğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†:")
    print(f"   æˆåŠŸ: {successful}/{len(image_files)} ({batch_result['success_rate']:.1%})")
    
    # Pushoveré€šçŸ¥é€ä¿¡
    try:
        from utils.notification import send_batch_notification
        print("\nğŸ“± é€šçŸ¥é€ä¿¡ä¸­...")
        notification_sent = send_batch_notification(
            successful=successful,
            total=len(image_files),
            failed=len(image_files) - successful,
            total_time=batch_result['total_time']
        )
        
        if notification_sent:
            print("âœ… Pushoveré€šçŸ¥é€ä¿¡å®Œäº†")
        else:
            print("âš ï¸ Pushoveré€šçŸ¥é€ä¿¡å¤±æ•—ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—")
    except ImportError:
        print("âš ï¸ é€šçŸ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except Exception as e:
        print(f"âš ï¸ é€šçŸ¥é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
    
    return batch_result


def main():
    """Main function for command line interface"""
    parser = argparse.ArgumentParser(description="Character Extraction using SAM + YOLO")
    
    parser.add_argument('input', help='Input image path or directory')
    parser.add_argument('-o', '--output', help='Output path (auto-generated if not specified)')
    parser.add_argument('--batch', action='store_true', help='Batch processing mode')
    parser.add_argument('--enhance-contrast', action='store_true', help='Enhance image contrast')
    parser.add_argument('--filter-text', action='store_true', default=True, help='Filter text regions')
    parser.add_argument('--save-mask', action='store_true', default=False, help='Save mask files')
    parser.add_argument('--save-transparent', action='store_true', default=False, help='Save transparent background')
    parser.add_argument('--min-yolo-score', type=float, default=0.1, help='Minimum YOLO score threshold')
    parser.add_argument('--verbose', action='store_true', default=True, help='Verbose output')
    
    # è¤‡é›‘ãƒãƒ¼ã‚ºãƒ»ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯æ§‹å›³å¯¾å¿œã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--difficult-pose', action='store_true', help='Enable difficult pose processing mode')
    parser.add_argument('--low-threshold', action='store_true', help='Use low threshold settings (YOLO score 0.02)')
    parser.add_argument('--auto-retry', action='store_true', help='Enable automatic retry with progressive settings')
    parser.add_argument('--high-quality', action='store_true', help='Enable high-quality SAM processing')
    
    # Phase 2: æ¼«ç”»å‰å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--manga-mode', action='store_true', help='Enable manga-specific preprocessing (Phase 2)')
    parser.add_argument('--effect-removal', action='store_true', help='Enable effect line removal (Phase 2)')
    parser.add_argument('--panel-split', action='store_true', help='Enable multi-panel splitting (Phase 2)')
    
    # Phase 4: çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--phase4', action='store_true', help='Enable Phase 4 integrated system (all enhancements)')
    parser.add_argument('--mask-inversion-detection', action='store_true', help='Enable mask inversion detection/correction')
    parser.add_argument('--adaptive-range', action='store_true', help='Enable adaptive extraction range adjustment')
    parser.add_argument('--quality-prediction', action='store_true', help='Enable quality prediction and feedback')
    
    # Phase 4.1: é¸æŠçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ 
    parser.add_argument('--phase41', action='store_true', help='Enable Phase 4.1 selective hybrid system (best of 0.0.3 and 0.0.4)')
    parser.add_argument('--multi-character-criteria', choices=['balanced', 'size_priority', 'fullbody_priority', 'central_priority', 'confidence_priority'],
                       default='balanced', help='Multi-character selection criteria for Phase 4.1')
    
    args = parser.parse_args()
    
    # Extract common arguments (Phase 4å¯¾å¿œç‰ˆ)
    extract_args = {
        'enhance_contrast': args.enhance_contrast,
        'filter_text': args.filter_text,
        'save_mask': args.save_mask,
        'save_transparent': args.save_transparent,
        'min_yolo_score': args.min_yolo_score,
        'verbose': args.verbose,
        'difficult_pose': args.difficult_pose,
        'low_threshold': args.low_threshold,
        'auto_retry': args.auto_retry,
        'high_quality': args.high_quality,
        'manga_mode': args.manga_mode,
        'effect_removal': args.effect_removal,
        'panel_split': args.panel_split,
        # Phase 4ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        'enable_phase4': args.phase4,
        'enable_mask_inversion_detection': args.mask_inversion_detection,
        'enable_adaptive_range': args.adaptive_range,
        'enable_quality_prediction': args.quality_prediction,
        # Phase 4.1ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        'enable_phase41': args.phase41 if hasattr(args, 'phase41') else False,
        'multi_character_criteria': args.multi_character_criteria if hasattr(args, 'multi_character_criteria') else "balanced"
    }
    
    # è¤‡é›‘ãƒãƒ¼ã‚ºãƒ¢ãƒ¼ãƒ‰ç”¨ã®è¨­å®šèª¿æ•´
    if args.low_threshold:
        extract_args['min_yolo_score'] = 0.02
        print("ğŸ”§ ä½é–¾å€¤ãƒ¢ãƒ¼ãƒ‰: YOLOé–¾å€¤ã‚’0.02ã«è¨­å®š")
    
    if args.high_quality:
        print("ğŸ”§ é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰: SAMé«˜å¯†åº¦å‡¦ç†ã‚’æœ‰åŠ¹åŒ–")
    
    # Phase 2: æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
    if args.manga_mode or args.effect_removal or args.panel_split:
        print("ğŸ¨ Phase 2: æ¼«ç”»å‰å‡¦ç†ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
        if args.effect_removal:
            print("   ğŸ“ ã‚¨ãƒ•ã‚§ã‚¯ãƒˆç·šé™¤å»: æœ‰åŠ¹")
        if args.panel_split:
            print("   ğŸ“Š ãƒãƒ«ãƒã‚³ãƒåˆ†å‰²: æœ‰åŠ¹")
    
    # Phase 4.1: é¸æŠçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®š
    if args.phase41:
        print("ğŸŒŸ Phase 4.1: é¸æŠçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ æœ‰åŠ¹")
        print(f"   ğŸ¯ è¤‡æ•°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é¸æŠåŸºæº–: {args.multi_character_criteria}")
        print("   âœ¨ 0.0.3ã¨0.0.4ã®ã„ã„ã¨ã“ã©ã‚Šå‡¦ç†")
    
    # Phase 4: çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®š
    elif args.phase4 or args.mask_inversion_detection or args.adaptive_range or args.quality_prediction:
        print("ğŸš€ Phase 4: çµ±åˆã‚·ã‚¹ãƒ†ãƒ æœ‰åŠ¹")
        if args.phase4:
            print("   ğŸ”§ ãƒ•ãƒ«çµ±åˆãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹")
        if args.mask_inversion_detection:
            print("   ğŸ”„ ãƒã‚¹ã‚¯é€†è»¢æ¤œå‡º: æœ‰åŠ¹")
        if args.adaptive_range:
            print("   ğŸ“ é©å¿œçš„ç¯„å›²èª¿æ•´: æœ‰åŠ¹")
        if args.quality_prediction:
            print("   ğŸ¯ å“è³ªäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ : æœ‰åŠ¹")
    
    if args.batch:
        # Batch processing
        output_dir = args.output or f"{args.input}_character_output"
        result = batch_extract_characters(args.input, output_dir, **extract_args)
    else:
        # Single file processing
        result = extract_character_from_path(args.input, args.output, **extract_args)
    
    # Exit with appropriate code
    sys.exit(0 if result['success'] else 1)


if __name__ == "__main__":
    main()